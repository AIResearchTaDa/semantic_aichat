import os
import asyncio
import time
import logging
import hashlib
import httpx
import json
import re
from collections import OrderedDict
from typing import List, Optional, Dict, Tuple, Any, Set
from contextlib import asynccontextmanager
from dataclasses import dataclass

from fastapi import FastAPI, HTTPException, Depends, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse, StreamingResponse, Response
from elasticsearch import AsyncElasticsearch
from dotenv import load_dotenv
from pydantic import BaseModel, Field, field_validator
from pydantic_settings import BaseSettings, SettingsConfigDict
from tenacity import (
    retry, stop_after_attempt, wait_exponential,
    retry_if_exception_type
)

# –Ü–º–ø–æ—Ä—Ç –º–æ–¥—É–ª—è –¥–ª—è –ª–æ–≥—É–≤–∞–Ω–Ω—è –ø–æ—à—É–∫—É
from search_logger import SearchLogger

# Logging configuration
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger("search-backend")
load_dotenv()

# Settings
class Settings(BaseSettings):
    # ES / Embeddings
    elastic_url: str = Field(default="http://elasticsearch:9200", env="ELASTIC_URL")
    elastic_user: str = Field(default="elastic", env="ELASTIC_USER")
    elastic_password: str = Field(default="elastic", env="ELASTIC_PASSWORD")
    embedding_api_url: str = Field(default="http://10.2.0.171:9001/api/embeddings", env="EMBEDDING_API_URL")
    ollama_model_name: str = Field(default="dengcao/Qwen3-Embedding-8B:Q8_0", env="OLLAMA_MODEL_NAME")
    index_name: str = Field(default="products_qwen3_8b", env="INDEX_NAME")
    vector_dimension: int = Field(default=4096, env="VECTOR_DIMENSION")
    embed_cache_size: int = Field(default=2000, env="EMBED_CACHE_SIZE")
    request_timeout: int = Field(default=30, env="REQUEST_TIMEOUT")
    max_retries: int = Field(default=3, env="MAX_RETRIES")
    cache_ttl_seconds: int = Field(default=3600, env="CACHE_TTL_SECONDS")
    knn_num_candidates: int = Field(default=500, env="KNN_NUM_CANDIDATES")
    hybrid_alpha: float = Field(default=0.7, env="HYBRID_ALPHA")
    hybrid_fusion: str = Field(default="weighted", env="HYBRID_FUSION")
    bm25_min_score: float = Field(default=5.0, env="BM25_MIN_SCORE")
    vector_field_name: str = Field(default="description_vector", env="VECTOR_FIELD_NAME")

    # GPT
    openai_api_key: str = Field(default="", env="OPENAI_API_KEY")
    gpt_model: str = Field(default="gpt-4o-mini", env="GPT_MODEL")
    enable_gpt_chat: bool = Field(default=True, env="ENABLE_GPT_CHAT")
    gpt_temperature: float = Field(default=0.3, env="GPT_TEMPERATURE")
    gpt_analyze_timeout_seconds: float = Field(default=10.0, env="GPT_ANALYZE_TIMEOUT_SECONDS")

    # Tokens
    gpt_max_tokens_analyze: int = Field(default=1500, env="GPT_MAX_TOKENS_ANALYZE")
    gpt_max_tokens_reco: int = Field(default=2000, env="GPT_MAX_TOKENS_RECO")
    gpt_reco_timeout_seconds: float = Field(default=30.0, env="GPT_RECO_TIMEOUT_SECONDS")  # –û–∫—Ä–µ–º–∏–π timeout –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π

    # Recommendations
    reco_detailed_count: int = Field(default=3, env="RECO_DETAILED_COUNT")
    grounded_recommendations: bool = Field(default=True, env="GROUNDED_RECOMMENDATIONS")

    # History
    search_history_ttl_days: int = Field(default=7, env="SEARCH_HISTORY_TTL_DAYS")
    max_search_history: int = Field(default=20, env="MAX_SEARCH_HISTORY")
    max_chat_display_items: int = Field(default=100, env="MAX_CHAT_DISPLAY_ITEMS")

    # Lazy loading settings
    initial_products_batch: int = Field(default=20, env="INITIAL_PRODUCTS_BATCH")
    load_more_batch_size: int = Field(default=20, env="LOAD_MORE_BATCH_SIZE")
    search_results_ttl_seconds: int = Field(default=3600, env="SEARCH_RESULTS_TTL_SECONDS")
    
    # Embedding concurrency settings
    embedding_max_concurrent: int = Field(default=2, env="EMBEDDING_MAX_CONCURRENT")  # –ú–∞–∫—Å –ø–∞—Ä–∞–ª–µ–ª—å–Ω–∏—Ö embedding –∑–∞–ø–∏—Ç—ñ–≤
    
    # Chat search relevance settings
    chat_search_score_threshold_ratio: float = Field(default=0.4, env="CHAT_SEARCH_SCORE_THRESHOLD_RATIO")  # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π –≤—ñ–¥—Å–æ—Ç–æ–∫ –≤—ñ–¥ max_score
    chat_search_min_score_absolute: float = Field(default=0.3, env="CHAT_SEARCH_MIN_SCORE_ABSOLUTE")  # –ê–±—Å–æ–ª—é—Ç–Ω–∏–π –º—ñ–Ω—ñ–º—É–º
    chat_search_subquery_weight_decay: float = Field(default=0.85, env="CHAT_SEARCH_SUBQUERY_WEIGHT_DECAY")  # –ó–º–µ–Ω—à–µ–Ω–Ω—è –≤–∞–≥–∏ –¥–ª—è –Ω–∞—Å—Ç—É–ø–Ω–∏—Ö –ø—ñ–¥–∑–∞–ø–∏—Ç—ñ–≤
    chat_search_max_k_per_subquery: int = Field(default=20, env="CHAT_SEARCH_MAX_K_PER_SUBQUERY")  # –ú–∞–∫—Å–∏–º—É–º —Ç–æ–≤–∞—Ä—ñ–≤ –Ω–∞ –ø—ñ–¥–∑–∞–ø–∏—Ç

    # TA-DA external API proxy
    ta_da_api_base_url: str = Field(default="https://api.ta-da.net.ua/v1.2/mobile", env="TA_DA_API_BASE_URL")
    ta_da_api_token: str = Field(default="", env="TA_DA_API_TOKEN")
    ta_da_default_shop_id: str = Field(default="8", env="TA_DA_DEFAULT_SHOP_ID")
    ta_da_default_language: str = Field(default="ua", env="TA_DA_DEFAULT_LANGUAGE")

    @field_validator("request_timeout")
    @classmethod
    def _validate_timeout(cls, v: int) -> int:
        if v < 5 or v > 300:
            raise ValueError("Request timeout must be between 5 and 300 seconds")
        return v

    model_config = SettingsConfigDict(
        env_file=".env",
        case_sensitive=False,
        extra="ignore"
    )

settings = Settings()

# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ª–æ–≥–µ—Ä–∞ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –ø–æ—à—É–∫—É
search_logger = SearchLogger(logs_dir="search_logs")

# Dependency Injection container
@dataclass
class Dependencies:
    es_client: Optional[AsyncElasticsearch] = None
    http_client: Optional[httpx.AsyncClient] = None
    embedding_cache: Optional["TTLCache"] = None
    gpt_service: Optional["GPTService"] = None
    context_manager: Optional["SearchContextManager"] = None

dependencies = Dependencies()

# TTL Cache
class TTLCache:
    def __init__(self, capacity: int = 1000, ttl_seconds: int = 3600):
        self.capacity = capacity
        self.ttl_seconds = ttl_seconds
        self.cache: OrderedDict = OrderedDict()
        self.timestamps: Dict[str, float] = {}
        self._lock = asyncio.Lock()

    async def get(self, key: str) -> Optional[Any]:
        async with self._lock:
            if key not in self.cache:
                return None
            if time.time() - self.timestamps.get(key, 0) > self.ttl_seconds:
                self.cache.pop(key, None)
                self.timestamps.pop(key, None)
                return None
            self.cache.move_to_end(key)
            return self.cache[key]

    async def put(self, key: str, value: Any) -> None:
        async with self._lock:
            now = time.time()
            self._cleanup_expired_sync()
            if key in self.cache:
                self.cache.move_to_end(key)
            self.cache[key] = value
            self.timestamps[key] = now
            if len(self.cache) > self.capacity:
                oldest_key, _ = self.cache.popitem(last=False)
                self.timestamps.pop(oldest_key, None)

    def _cleanup_expired_sync(self) -> int:
        now = time.time()
        expired_keys = [k for k, t in self.timestamps.items() if now - t > self.ttl_seconds]
        for k in expired_keys:
            self.cache.pop(k, None)
            self.timestamps.pop(k, None)
        return len(expired_keys)

    async def cleanup_expired(self) -> int:
        async with self._lock:
            return self._cleanup_expired_sync()

    async def clear(self) -> None:
        async with self._lock:
            self.cache.clear()
            self.timestamps.clear()

    def __len__(self) -> int:
        return len(self.cache)

# Pydantic Models
class SearchRequest(BaseModel):
    query: str = Field(min_length=2, max_length=500)
    k: int = Field(default=50, ge=1, le=500)
    min_score: float = Field(default=0.1, ge=0.0, le=1.0)
    mode: str = Field(default="bm25", description="knn | hybrid | bm25")

class SearchResult(BaseModel):
    id: str
    score: float
    title_ua: Optional[str] = None
    title_ru: Optional[str] = None
    description_ua: Optional[str] = None
    description_ru: Optional[str] = None
    sku: Optional[str] = None
    good_code: Optional[str] = None
    uktzed: Optional[str] = None
    measurement_unit_ua: Optional[str] = None
    vat: Optional[str] = None
    discounted: Optional[bool] = None
    height: Optional[float] = None
    width: Optional[float] = None
    length: Optional[float] = None
    weight: Optional[float] = None
    availability: bool = True
    highlight: Optional[Dict[str, List[str]]] = None

    @classmethod
    def from_hit(cls, hit: Dict[str, Any]) -> "SearchResult":
        doc_id, src = hit.get("_id", ""), hit.get("_source", {}) or {}
        return cls(
            id=doc_id,
            score=float(hit.get("_score", 0.0)),
            title_ua=src.get("title_ua"),
            title_ru=src.get("title_ru"),
            description_ua=src.get("description_ua"),
            description_ru=src.get("description_ru"),
            sku=src.get("sku"),
            good_code=src.get("good_code"),
            uktzed=src.get("uktzed"),
            measurement_unit_ua=src.get("measurement_unit_ua"),
            vat=src.get("vat"),
            discounted=src.get("discounted"),
            height=src.get("height"),
            width=src.get("width"),
            length=src.get("length"),
            weight=src.get("weight"),
            availability=src.get("availability", True),
            highlight=hit.get("highlight")
        )

def _build_human_reason(query: str, sr: "SearchResult") -> str:
    """Simplified reason builder without complex categorization."""
    q = (query or "").lower()
    parts: List[str] = ["–í—ñ–¥–ø–æ–≤—ñ–¥–∞—î –≤–∞—à–æ–º—É –∑–∞–ø–∏—Ç—É"]
    
    if any(t in q for t in ["—Ö–ª–æ–ø", "–º–∞–ª—å—á", "—é–Ω–∞–∫", "–ø–∞—Ä—É–±"]):
        parts.append("–ø—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è —Ö–ª–æ–ø—á–∏–∫–∞")
    if any(t in q for t in ["–¥—ñ–≤—á", "–¥–µ–≤–æ—á", "girl"]):
        parts.append("–ø—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è –¥—ñ–≤—á–∏–Ω–∫–∏")
    if any(t in q for t in ["–¥–∏—Ç", "—Ä–µ–±–µ–Ω", "–¥–∏—Ç—è—á", "kid", "child"]):
        parts.append("–¥–∏—Ç—è—á–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è")
    
    return ", ".join(parts)

class SearchResponse(BaseModel):
    results: List[SearchResult]
    total_found: int
    search_time_ms: float
    mode: str

class HealthResponse(BaseModel):
    status: str
    elasticsearch: str
    index: str
    documents_count: int
    cache_size: int
    uptime_seconds: float

class StatsResponse(BaseModel):
    index: str
    documents_count: int
    index_size_bytes: int
    health: str
    embedding_cache_size: int
    embedding_model: str
    uptime_seconds: float

class TadaFindRequest(BaseModel):
    shop_id: str = Field(default_factory=lambda: settings.ta_da_default_shop_id)
    good_code: str
    user_language: Optional[str] = Field(default=None, description="ua|ru|...")

class SearchHistoryItem(BaseModel):
    query: str
    keywords: List[str]
    timestamp: float
    results_count: int

class ChatSearchRequest(BaseModel):
    query: str = Field(min_length=1, max_length=500)
    search_history: List[SearchHistoryItem] = Field(default_factory=list)
    session_id: str
    k: int = Field(default=50, ge=1, le=200)
    dialog_context: Optional[Dict[str, Any]] = None
    selected_category: Optional[str] = Field(default=None, description="Selected category code or name")

class LoadMoreRequest(BaseModel):
    session_id: str
    offset: int = Field(ge=0)
    limit: int = Field(default=20, ge=1, le=50)

class QueryAnalysis(BaseModel):
    original_query: str
    expanded_query: str
    keywords: List[str]
    context_used: bool
    intent: str
    atomic_queries: List[str] = Field(default_factory=list)
    es_dsl: Optional[Dict[str, Any]] = Field(default=None)
    filters: Optional[Dict[str, Any]] = Field(default=None)
    semantic_subqueries: List[str] = Field(default_factory=list)
    intent_description: Optional[str] = Field(default=None)

class ProductRecommendation(BaseModel):
    product_id: str
    relevance_score: float
    reason: str
    title: Optional[str] = None
    bucket: Optional[str] = Field(default=None, description="must_have | good_to_have | budget")

class ChatSearchResponse(BaseModel):
    query_analysis: QueryAnalysis
    results: List[SearchResult]
    recommendations: List[ProductRecommendation]
    search_time_ms: float
    context_used: bool
    assistant_message: Optional[str] = None
    dialog_state: Optional[str] = None
    dialog_context: Optional[Dict[str, Any]] = None
    needs_user_input: bool = True
    actions: Optional[List[Dict[str, Any]]] = Field(default=None)
    stage_timings_ms: Optional[Dict[str, float]] = Field(default=None)

# Category schema - —Ä–æ–∑—à–∏—Ä–µ–Ω–∞ –≤–µ—Ä—Å—ñ—è –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∞—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç—É TA-DA
CATEGORY_SCHEMA: Dict[str, Dict[str, Any]] = {
    # –û–î–Ø–ì (Clothes)
    "clothes_tshirts": {"label": "–§—É—Ç–±–æ–ª–∫–∏", "keywords": ["—Ñ—É—Ç–±–æ–ª–∫", "t-shirt", "tee", "–º–∞–π–∫"]},
    "clothes_shirts": {"label": "–°–æ—Ä–æ—á–∫–∏", "keywords": ["—Å–æ—Ä–æ—á", "—Ä—É–±–∞—à"]},
    "clothes_pants": {"label": "–®—Ç–∞–Ω–∏", "keywords": ["—à—Ç–∞–Ω", "–±—Ä—é–∫", "–¥–∂–∏–Ω—Å", "–¥–∂–∏–Ω—Å–∏"]},
    "clothes_shorts": {"label": "–®–æ—Ä—Ç–∏", "keywords": ["—à–æ—Ä—Ç"]},
    "clothes_dresses": {"label": "–°—É–∫–Ω—ñ", "keywords": ["—Å—É–∫–Ω", "–ø–ª–∞—Ç—Ç"]},
    "clothes_skirts": {"label": "–°–ø—ñ–¥–Ω–∏—Ü—ñ", "keywords": ["—Å–ø—ñ–¥–Ω–∏—Ü", "—é–±–∫"]},
    "clothes_sweaters": {"label": "–°–≤–µ—Ç—Ä–∏/–•—É–¥—ñ", "keywords": ["—Å–≤–µ—Ç—Ä", "–∫–æ—Ñ—Ç", "—Ç–æ–ª—Å—Ç–æ–≤–∫", "—Ö—É–¥—ñ"]},
    "clothes_outerwear": {"label": "–ö—É—Ä—Ç–∫–∏/–ü–∞–ª—å—Ç–∞", "keywords": ["–∫—É—Ä—Ç–∫", "–ø–∞–ª—å—Ç", "–∂–∏–ª–µ—Ç"]},
    "clothes_underwear": {"label": "–ë—ñ–ª–∏–∑–Ω–∞/–®–∫–∞—Ä–ø–µ—Ç–∫–∏", "keywords": ["–±—ñ–ª–∏–∑–Ω", "–Ω–∏–∂–Ω", "—Ç—Ä—É—Å–∏", "—à–∫–∞—Ä–ø", "–Ω–æ—Å–∫", "–∫–æ–ª–≥–æ—Ç"]},
    "clothes_sleepwear": {"label": "–ü—ñ–∂–∞–º–∏/–î–æ–º–∞—à–Ω—ñ–π –æ–¥—è–≥", "keywords": ["–ø—ñ–∂–∞–º", "–¥–æ–º–∞—à", "—Ö–∞–ª–∞—Ç"]},
    "clothes_accessories": {"label": "–ê–∫—Å–µ—Å—É–∞—Ä–∏ –¥–ª—è –æ–¥—è–≥—É", "keywords": ["—à–∞–ø–∫", "—à–∞—Ä—Ñ", "—Ä—É–∫–∞–≤–∏—Ü", "–ø–µ—Ä—á–∞—Ç", "—Ä–µ–º—ñ–Ω", "–ø–æ—è—Å", "–∫—Ä–∞–≤–∞—Ç–∫"]},
    "clothes_shoes": {"label": "–í–∑—É—Ç—Ç—è", "keywords": ["–≤–∑—É—Ç—Ç", "–æ–±—É–≤", "—Ç–∞–ø–æ—á–∫", "—á–µ—Ä–µ–≤–∏–∫", "–±–æ—Å–æ–Ω—ñ–∂–∫", "–∫—Ä–æ—Å—ñ–≤–∫"]},
    
    # –Ü–ì–†–ê–®–ö–ò –¢–ê –Ü–ì–†–ò (Toys & Games)
    "toys_water": {"label": "–î–ª—è –ø–ª–∞–≤–∞–Ω–Ω—è —Ç–∞ –≤–æ–¥–∏", "keywords": ["–∫—Ä—É–≥ –Ω–∞–¥—É–≤", "–Ω–∞—Ä—É–∫–∞–≤–Ω–∏–∫", "–∂–∏–ª–µ—Ç –Ω–∞–¥—É–≤", "–±–∞—Å–µ–π–Ω", "–≤–æ–¥–Ω–∏–π", "–ø–ª–∞–≤–∞–Ω", "–º–∞—Ç—Ä–∞—Ü –Ω–∞–¥—É–≤"]},
    "toys_general": {"label": "–Ü–≥—Ä–∞—à–∫–∏", "keywords": ["—ñ–≥—Ä–∞—à", "–∏–≥—Ä—É—à", "–ª—è–ª—å–∫", "–∫—É–∫–ª", "–∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä", "–º–∞—à–∏–Ω–∫", "–º'—è—á", "–º—è—á", "–ø–ª—é—à", "–ø—ñ—Å—Ç–æ–ª–µ—Ç"]},
    "toys_educational": {"label": "–†–æ–∑–≤–∏–≤–∞—é—á—ñ —ñ–≥—Ä–∞—à–∫–∏", "keywords": ["—Ä–æ–∑–≤–∏–≤–∞", "–Ω–∞–≤—á–∞–ª", "—Ä–æ–∑—É–º–Ω", "–ª–æ–≥—ñ—á–Ω", "–ø–∞–∑–∑–ª –¥–∏—Ç—è—á"]},
    "games_board": {"label": "–ù–∞—Å—Ç—ñ–ª—å–Ω—ñ —ñ–≥—Ä–∏", "keywords": ["–Ω–∞—Å—Ç—ñ–ª—å–Ω", "–Ω–∞—Å—Ç–æ–ª—å–Ω", "–ø–∞–∑–ª", "–ª–æ—Ç–æ", "–¥–æ–º—ñ–Ω–æ", "–º–æ–Ω–æ–ø–æ–ª", "–º–æ–∑–∞—ó–∫"]},
    "toys_outdoor": {"label": "–î–ª—è –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –≤—ñ–¥–ø–æ—á–∏–Ω–∫—É", "keywords": ["—Å–∫–µ–π—Ç–±–æ—Ä–¥", "—Å–∞–º–æ–∫–∞—Ç", "–≤–µ–ª–æ—Å–∏–ø–µ–¥", "–º'—è—á —Å–ø–æ—Ä—Ç", "—Ä–∞–∫–µ—Ç–∫", "–±–∞–¥–º—ñ–Ω—Ç–æ–Ω"]},
    
    # –ö–£–•–ù–Ø –¢–ê –ü–û–°–£–î (Kitchen & Tableware)
    "house_kitchen_cookware": {"label": "–ö—É—Ö–æ–Ω–Ω–∏–π –ø–æ—Å—É–¥", "keywords": ["–∫–∞—Å—Ç—Ä", "—Å–∫–æ–≤–æ—Ä", "–∫–∞—Å—Ç—Ä—É–ª", "–∫–∞–∑–∞–Ω", "—Å–æ—Ç–µ–π–Ω", "–∂–∞—Ä–æ–≤–Ω"]},
    "house_kitchen_tableware": {"label": "–ü–æ—Å—É–¥ –¥–ª—è —Å–µ—Ä–≤—ñ—Ä—É–≤–∞–Ω–Ω—è", "keywords": ["—Ç–∞—Ä—ñ–ª", "—Ç–∞—Ä–µ–ª", "—á–∞—à–∫", "–∫–µ–ª–∏—Ö", "—Å–∫–ª—è–Ω–∫", "—Å—Ç–∞–∫–∞–Ω", "–±–ª—é–¥", "—Å–∞–ª–∞—Ç–Ω–∏—Ü"]},
    "house_kitchen_cutlery": {"label": "–°—Ç–æ–ª–æ–≤—ñ –ø—Ä–∏–±–æ—Ä–∏", "keywords": ["–≤–∏–ª–∫", "–ª–æ–∂–∫", "–Ω—ñ–∂ —Å—Ç–æ–ª–æ–≤", "–ø—Ä–∏–±–æ—Ä"]},
    "house_kitchen_tools": {"label": "–ö—É—Ö–æ–Ω–Ω—ñ –∞–∫—Å–µ—Å—É–∞—Ä–∏", "keywords": ["–¥–æ—à–∫", "–Ω—ñ–∂ –∫—É—Ö–æ–Ω", "—Ç–µ—Ä–∫", "–≤—ñ–¥–∫—Ä–∏–≤–∞—á", "–∫–æ–Ω—Å–µ—Ä–≤–Ω", "–ª–æ–ø–∞—Ç–∫", "—à—É–º—ñ–≤–∫", "–¥—É—Ä—à–ª–∞–≥"]},
    "house_kitchen_storage": {"label": "–Ñ–º–Ω–æ—Å—Ç—ñ –¥–ª—è –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è", "keywords": ["–∫–æ–Ω—Ç–µ–π–Ω–µ—Ä", "—î–º–∫—ñ—Å—Ç", "–±–∞–Ω–∫", "–ø–ª—è—à–∫", "—Ç–µ—Ä–º–æ—Å", "–ª–∞–Ω—á"]},
    "house_kitchen_textiles": {"label": "–ö—É—Ö–æ–Ω–Ω–∏–π —Ç–µ–∫—Å—Ç–∏–ª—å", "keywords": ["—Ä—É—à–Ω–∏–∫ –∫—É—Ö–æ–Ω", "–ø—Ä–∏—Ö–≤–∞—Ç–∫", "—Ñ–∞—Ä—Ç—É—Ö", "—Å–µ—Ä–≤–µ—Ç–∫"]},
    
    # –ü–†–ò–ë–ò–†–ê–ù–ù–Ø –¢–ê –ì–û–°–ü–û–î–ê–†–°–¨–ö–Ü –¢–û–í–ê–†–ò (Cleaning & Household)
    "house_cleaning_tools": {"label": "–Ü–Ω–≤–µ–Ω—Ç–∞—Ä –¥–ª—è –ø—Ä–∏–±–∏—Ä–∞–Ω–Ω—è", "keywords": ["—à–≤–∞–±—Ä", "—â—ñ—Ç–∫", "–≤—ñ–Ω–∏–∫", "—Å–æ–≤–æ–∫", "–≤—ñ–¥—Ä–æ", "–≥–∞–Ω—á—ñ—Ä", "–≥—É–±–∫"]},
    "house_cleaning_chemicals": {"label": "–ü–æ–±—É—Ç–æ–≤–∞ —Ö—ñ–º—ñ—è", "keywords": ["—á–∏—Å—Ç", "–º–∏—é—á", "–ø–æ—Ä–æ—à–æ–∫", "–≥–µ–ª—å –¥–ª—è –º–∏—Ç—Ç—è", "–∑–∞—Å—ñ–±", "—Å–ø—Ä–µ–π", "–≤—ñ–¥–±—ñ–ª—é–≤–∞—á", "–∫–æ–Ω–¥–∏—Ü—ñ–æ–Ω–µ—Ä –¥–ª—è –±—ñ–ª–∏–∑–Ω–∏"]},
    "house_cleaning_bathroom": {"label": "–î–ª—è –≤–∞–Ω–Ω–æ—ó –∫—ñ–º–Ω–∞—Ç–∏", "keywords": ["—Ç—É–∞–ª–µ—Ç", "–≤–∞–Ω–Ω", "–¥—É—à", "–π–æ—Ä–∂–∏–∫", "—Ç—Ä–∏–º–∞—á", "–¥–æ–∑–∞—Ç–æ—Ä", "–∫–∏–ª–∏–º–æ–∫ –≤–∞–Ω–Ω"]},
    "house_laundry": {"label": "–î–ª—è –ø—Ä–∞–Ω–Ω—è", "keywords": ["–ø—Ä–∞–Ω–Ω—è", "—Å—É—à–∞—Ä–∫", "–ø—Ä–∏—â—ñ–ø–∫", "–º–æ—Ç—É–∑–∫", "–ø—Ä–∞—Å—É–≤–∞–ª", "–≥–ª–∞–¥–∏–ª—å–Ω"]},
    
    # –ö–û–°–ú–ï–¢–ò–ö–ê –¢–ê –ì–Ü–ì–Ü–Ñ–ù–ê (Cosmetics & Hygiene)
    "cosmetics_skincare": {"label": "–î–æ–≥–ª—è–¥ –∑–∞ —à–∫—ñ—Ä–æ—é", "keywords": ["–∫—Ä–µ–º", "–ª–æ—Å—å–æ–Ω", "—Ç–æ–Ω—ñ–∫", "–º–∞—Å–∫ –∫–æ—Å–º–µ—Ç–∏—á", "—Å–∫—Ä–∞–±", "–ø—ñ–ª—ñ–Ω–≥", "—Å–∏—Ä–æ–≤–∞—Ç–∫"]},
    "cosmetics_suncare": {"label": "–°–æ–Ω—Ü–µ–∑–∞—Ö–∏—Å–Ω—ñ –∑–∞—Å–æ–±–∏", "keywords": ["—Å–æ–Ω—Ü–µ–∑–∞—Ö–∏—Å–Ω", "spf", "–∑–∞—Ö–∏—Å—Ç –≤—ñ–¥ —Å–æ–Ω—Ü—è", "–∫—Ä–µ–º –¥–ª—è –∑–∞—Å–º–∞–≥"]},
    "cosmetics_body": {"label": "–î–æ–≥–ª—è–¥ –∑–∞ —Ç—ñ–ª–æ–º", "keywords": ["–≥–µ–ª—å –¥–ª—è –¥—É—à", "–º–∏–ª–æ", "—à–∞–º–ø—É–Ω", "–±–∞–ª—å–∑–∞–º –¥–ª—è –≤–æ–ª–æ—Å—Å—è", "–¥–µ–∑–æ–¥–æ—Ä–∞–Ω", "–∞–Ω—Ç–∏–ø–µ—Ä—Å–ø—ñ—Ä–∞–Ω—Ç"]},
    "cosmetics_oral": {"label": "–ì—ñ–≥—ñ—î–Ω–∞ –ø–æ—Ä–æ–∂–Ω–∏–Ω–∏ —Ä–æ—Ç–∞", "keywords": ["–∑—É–±–Ω –ø–∞—Å—Ç", "—â—ñ—Ç–∫ –∑—É–±–Ω", "–Ω–∏—Ç–∫ –∑—É–±–Ω", "–æ–ø–æ–ª—ñ—Å–∫—É–≤–∞—á"]},
    "cosmetics_firstaid": {"label": "–ê–ø—Ç–µ—á–∫–∞", "keywords": ["–ø–∞–Ω—Ç–µ–Ω", "–±–∏–Ω—Ç", "–ø–ª–∞—Å—Ç–∏—Ä", "–≤–∞—Ç–∞", "–ø–µ—Ä–µ–∫–∏—Å", "–∑–µ–ª–µ–Ω", "–π–æ–¥"]},
    
    # –ö–ê–ù–¶–ï–õ–Ø–†–Ü–Ø (Stationery)
    "stationery_notebooks": {"label": "–ó–æ—à–∏—Ç–∏ —Ç–∞ –±–ª–æ–∫–Ω–æ—Ç–∏", "keywords": ["–∑–æ—à–∏—Ç", "—Ç–µ—Ç—Ä–∞–¥", "–±–ª–æ–∫–Ω–æ—Ç", "—â–æ–¥–µ–Ω–Ω–∏–∫", "–∑–∞–ø–∏—Å–Ω"]},
    "stationery_paper": {"label": "–ü–∞–ø—ñ—Ä", "keywords": ["–ø–∞–ø—ñ—Ä", "–±—É–º–∞–≥", "–∞—Ä–∫—É—à", "–∫–∞—Ä—Ç–æ–Ω", "–∫–æ–ª—å–æ—Ä–æ–≤ –ø–∞–ø—ñ—Ä"]},
    "stationery_writing": {"label": "–†—É—á–∫–∏ —Ç–∞ –æ–ª—ñ–≤—Ü—ñ", "keywords": ["—Ä—É—á–∫", "–æ–ª—ñ–≤—Ü", "–∫–∞—Ä–∞–Ω–¥–∞—à", "–º–∞—Ä–∫–µ—Ä", "—Ñ–ª–æ–º–∞—Å—Ç–µ—Ä", "—Ç–µ–∫—Å—Ç–æ–≤–∏–¥—ñ–ª—é–≤–∞—á"]},
    "stationery_cases": {"label": "–ü–µ–Ω–∞–ª–∏ —Ç–∞ –ø–∞–ø–∫–∏", "keywords": ["–ø–µ–Ω–∞–ª", "–ø–∞–ø–∫", "—Ñ–∞–π–ª", "—Å–∫–æ—Ä–æ—Å–∑—à–∏–≤–∞—á", "–ø–æ—Ä—Ç—Ñ–µ–ª—å"]},
    "stationery_art": {"label": "–¢–æ–≤–∞—Ä–∏ –¥–ª—è —Ç–≤–æ—Ä—á–æ—Å—Ç—ñ", "keywords": ["—Ñ–∞—Ä–±", "–∞–∫–≤–∞—Ä–µ–ª", "–ø–µ–Ω–∑–ª", "–ø–ª–∞—Å—Ç–∏–ª—ñ–Ω", "–∫–ª–µ–π", "–Ω–æ–∂–∏—Ü—ñ", "—Ü–∏—Ä–∫—É–ª—å", "–ª—ñ–Ω—ñ–π–∫"]},
    "stationery_office": {"label": "–û—Ñ—ñ—Å–Ω—ñ —Ç–æ–≤–∞—Ä–∏", "keywords": ["—Å—Ç–µ–ø–ª–µ—Ä", "—Å–∫–æ–±–∫", "–∫–Ω–æ–ø–∫", "—Å–∫—Ä—ñ–ø–∫", "—Å—Ç—ñ–∫–µ—Ä", "–∫–ª–µ–π–∫ —Å—Ç—Ä—ñ—á–∫", "–∫–æ—Ä–µ–∫—Ç–æ—Ä"]},
    
    # –¢–û–í–ê–†–ò –î–õ–Ø –î–û–ú–£ (Home & Living)
    "home_decor": {"label": "–î–µ–∫–æ—Ä —Ç–∞ –ø—Ä–∏–∫—Ä–∞—Å–∏", "keywords": ["—Å–≤—ñ—á–∫", "—Ä–∞–º–∫", "–≤–∞–∑", "—Å—Ç–∞—Ç—É–µ—Ç–∫", "–¥–µ–∫–æ—Ä", "–ø—Ä–∏–∫—Ä–∞—Å"]},
    "home_textiles": {"label": "–î–æ–º–∞—à–Ω—ñ–π —Ç–µ–∫—Å—Ç–∏–ª—å", "keywords": ["–ø–æ—Å—Ç—ñ–ª—å–Ω", "–ø—Ä–æ—Å—Ç–∏—Ä–∞–¥–ª", "–ø–æ–¥—É—à–∫", "–∫–æ–≤–¥—Ä", "–ø–ª–µ–¥", "–ø–æ–∫—Ä–∏–≤–∞–ª"]},
    "home_storage": {"label": "–û—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ—è —Ç–∞ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è", "keywords": ["–∫–æ—Ä–æ–±–∫", "–∫–æ—Ä–∑–∏–Ω", "–æ—Ä–≥–∞–Ω–∞–π–∑–µ—Ä", "–ø–æ–ª–∏—Ü", "–≤—ñ—à–∞–ª–∫", "–ø—ñ–¥—Å—Ç–∞–≤–∫"]},
    "home_lighting": {"label": "–û—Å–≤—ñ—Ç–ª–µ–Ω–Ω—è", "keywords": ["–ª–∞–º–ø–æ—á–∫", "–ª—ñ—Ö—Ç–∞—Ä", "—Å–≤—ñ—Ç–∏–ª—å–Ω–∏–∫", "—Ç–æ—Ä—à–µ—Ä", "–±—Ä–∞", "—Å–≤—ñ—Ç–ª–æ–¥—ñ–æ–¥"]},
    "home_electronics": {"label": "–ü–æ–±—É—Ç–æ–≤–∞ –µ–ª–µ–∫—Ç—Ä–æ–Ω—ñ–∫–∞", "keywords": ["–≤–µ–Ω—Ç–∏–ª—è—Ç–æ—Ä", "–æ–±—ñ–≥—Ä—ñ–≤–∞—á", "–≥–æ–¥–∏–Ω–Ω–∏–∫", "–±—É–¥–∏–ª—å–Ω–∏–∫", "–≤–∞–≥–∏", "—Ç–µ—Ä–º–æ–º–µ—Ç—Ä"]},
    "home_garden": {"label": "–î–ª—è —Å–∞–¥—É —Ç–∞ –≥–æ—Ä–æ–¥—É", "keywords": ["–≥–æ—Ä—â–∏–∫", "–ª—ñ–π–∫", "—Å–∞–¥–æ–≤", "–≥–æ—Ä–æ–¥–Ω", "—ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç —Å–∞–¥–æ–≤", "–Ω–∞—Å—ñ–Ω–Ω", "–¥–æ–±—Ä–∏–≤"]},
    
    # –°–ï–ó–û–ù–ù–Ü –¢–û–í–ê–†–ò (Seasonal)
    "seasonal_summer": {"label": "–õ—ñ—Ç–Ω—ñ —Ç–æ–≤–∞—Ä–∏", "keywords": ["–ø–ª—è–∂–Ω", "–∫—É–ø–∞–ª—å–Ω–∏–∫", "–æ–∫—É–ª—è—Ä–∏ —Å–æ–Ω—Ü–µ–∑–∞—Ö–∏—Å–Ω", "–≤–µ–Ω—Ç–∏–ª—è—Ç–æ—Ä", "–∫–æ–Ω–¥–∏—Ü—ñ–æ–Ω–µ—Ä"]},
    "seasonal_winter": {"label": "–ó–∏–º–æ–≤—ñ —Ç–æ–≤–∞—Ä–∏", "keywords": ["—Å–∞–Ω–∫", "–ª–æ–ø–∞—Ç –¥–ª—è —Å–Ω—ñ–≥—É", "–∞–Ω—Ç–∏–æ–∂–µ–ª–µ–¥–Ω", "–æ–±—ñ–≥—Ä—ñ–≤–∞—á"]},
    "seasonal_holiday": {"label": "–°–≤—è—Ç–∫–æ–≤—ñ —Ç–æ–≤–∞—Ä–∏", "keywords": ["–≥—ñ—Ä–ª—è–Ω–¥", "—è–ª–∏–Ω–∫", "—ñ–≥—Ä–∞—à–∫ —è–ª–∏–Ω–∫", "–º—ñ—à—É—Ä", "—Å–µ—Ä–ø–∞–Ω—Ç–∏–Ω", "–ø–æ–≤—ñ—Ç—Ä –∫—É–ª—å–∫"]},
    "seasonal_bbq": {"label": "–î–ª—è –ø—ñ–∫–Ω—ñ–∫–∞ —Ç–∞ –±–∞—Ä–±–µ–∫—é", "keywords": ["–º–∞–Ω–≥–∞–ª", "—à–∞–º–ø—É—Ä", "—Ä–µ—à—ñ—Ç–∫", "–ø–æ—Å—É–¥ –æ–¥–Ω–æ—Ä–∞–∑", "—Ç–µ—Ä–º–æ—Å—É–º–∫", "–ø–æ–∫—Ä–∏–≤–∞–ª –¥–ª—è –ø—ñ–∫–Ω—ñ–∫"]},
    
    # –ó–ê–•–ò–°–¢ –í–Ü–î –ö–û–ú–ê–• (Insect Protection)
    "home_insects": {"label": "–í—ñ–¥ –∫–æ–º–∞—Ö", "keywords": ["–∫–æ–º–∞—Ä", "–º—É—Ö", "—Ç–∞—Ä–∞–∫–∞–Ω", "–∞–µ—Ä–æ–∑–æ–ª", "—Ä–µ–ø–µ–ª–µ–Ω—Ç", "—Ñ—É–º—ñ–≥–∞—Ç–æ—Ä", "—Å—ñ—Ç–∫ –º–æ—Å–∫—ñ—Ç–Ω", "—Å—Ç—Ä—ñ—á–∫ –∫–ª–µ–π–∫ –≤—ñ–¥", "—Å–ø—ñ—Ä–∞–ª –≤—ñ–¥ –∫–æ–º–∞—Ä", "–ø–∞—Å—Ç–∫"]},
    
    # –ê–í–¢–û–¢–û–í–ê–†–ò (Auto)
    "auto_accessories": {"label": "–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä–∏", "keywords": ["–∞–≤—Ç–æ–º–æ–±—ñ–ª", "–º–∞—à–∏–Ω –∞–≤—Ç–æ", "–∫–∏–ª–∏–º –∞–≤—Ç–æ", "–æ—Å–≤—ñ–∂—É–≤–∞—á –∞–≤—Ç–æ", "—Ç—Ä–∏–º–∞—á –∞–≤—Ç–æ", "—á–æ—Ö–ª –∞–≤—Ç–æ"]},
    
    # –ü–ï–¢ –¢–û–í–ê–†–ò (Pet)
    "pet_supplies": {"label": "–î–ª—è —Ç–≤–∞—Ä–∏–Ω", "keywords": ["—Å–æ–±–∞–∫", "–∫—ñ—à", "–∫–æ—Ä–º", "–º–∏—Å–æ—á–∫", "—ñ–≥—Ä–∞—à–∫ –¥–ª—è —Ç–≤–∞—Ä–∏–Ω", "–Ω–∞—à–∏–π–Ω–∏–∫", "–ø–æ–≤–æ–¥–æ–∫"]},
}

def _assign_category_code(sr: "SearchResult") -> Optional[str]:
    text = " ".join(filter(None, [sr.title_ua, sr.title_ru, sr.description_ua, sr.description_ru])).lower()
    best_code, best_hits = None, 0
    for code, data in CATEGORY_SCHEMA.items():
        hits = sum(1 for kw in data["keywords"] if kw in text)
        if hits > best_hits:
            best_code, best_hits = code, hits
    return best_code

def _allowed_category_codes_for_query(query: str) -> Optional[Set[str]]:
    q = (query or "").lower()
    clothes_tokens = [
        "–æ–¥—è–≥", "–æ–¥–µ–∂", "—Ñ—É—Ç–±–æ–ª–∫", "—Å–æ—Ä–æ—á", "—à—Ç–∞–Ω", "–±—Ä—é–∫", "—à–æ—Ä—Ç", "—Å—É–∫–Ω", "–ø–ª–∞—Ç—Ç",
        "—Å–ø—ñ–¥–Ω–∏—Ü", "—é–±–∫", "–∫–æ—Ñ—Ç", "—Å–≤–µ—Ç—Ä", "—Ç–æ–ª—Å—Ç–æ–≤–∫", "—Ö—É–¥—ñ", "–∫—É—Ä—Ç–∫", "–ø–∞–ª—å—Ç",
        "–∂–∏–ª–µ—Ç", "–±—ñ–ª–∏–∑–Ω", "–Ω–∏–∂–Ω", "—Ç—Ä—É—Å–∏", "–ø—ñ–∂–∞–º", "–∫–æ–º–±—ñ–Ω–µ–∑"
    ]
    shoes_tokens = ["–≤–∑—É—Ç", "–æ–±—É–≤", "–∫–∞–ø—Ü", "—á–æ–±—ñ—Ç", "—á–µ—Ä–µ–≤–∏–∫", "—à–ª—å–æ–ø–∞–Ω—Ü", "–∫—Ä–æ—Å—ñ–≤–∫", "—Ç—É—Ñ–ª", "—Å–∞–Ω–¥–∞–ª", "—Ç–∞–ø–æ—á–∫"]
    accessories_tokens = ["—à–∫–∞—Ä–ø", "–Ω–æ—Å–∫", "–∫–æ–ª–≥–æ—Ç", "–ø–∞–Ω—á–æ—Ö", "—à–∞–ø–∫", "—à–∞—Ä—Ñ", "—Ä—É–∫–∞–≤–∏—Ü", "–ø–µ—Ä—á–∞—Ç"]
    toys_tokens = ["—ñ–≥—Ä–∞—à", "–∏–≥—Ä—É—à", "–ª—è–ª—å–∫", "–∫—É–∫–ª", "–∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä", "–º'—è—á", "–º—è—á", "–ø–ª—é—à", "–≤–æ–¥–Ω–∏–π –ø—ñ—Å—Ç–æ–ª–µ—Ç", "–Ω–∞—Ä—É–∫–∞–≤–Ω–∏–∫", "–±–∞—Å–µ–π–Ω"]
    house_tokens = ["–ø–æ—Å—É–¥", "–∫–∞—Å—Ç—Ä", "—Å–∫–æ–≤–æ—Ä", "—Ç–∞—Ä—ñ–ª", "—á–∞—à–∫", "–∫–µ–ª–∏—Ö", "–∫—É—Ö–æ–Ω", "–≥–∞–Ω—á—ñ—Ä", "—à–≤–∞–±—Ä", "—Å–ø—Ä–µ–π", "–º–∏—é—á"]
    fishing_tokens = ["—Ä–∏–±–∞–ª", "—Ä—ã–±–∞–ª", "–≤—É–¥–∏–ª", "—É–¥–æ—á–∫", "—Å–ø—ñ–Ω—ñ–Ω–≥", "—Å–ø–∏–Ω–Ω–∏–Ω–≥", "–ª–µ—Å–∫–∞", "–≤–æ–ª–æ—Å—ñ–Ω—å", "–≥–∞—á–æ–∫", "–∫—Ä—é—á–æ–∫", "–Ω–∞–∂–∏–≤–∫", "–ø—Ä–∏–º–∞–Ω–∫", "–∫–∞—Ç—É—à–∫", "—Ä–∏–±–∞–ª—å—Å—å–∫"]
    garden_tokens = ["–Ω–∞—Å—ñ–Ω–Ω", "—Å–µ–º–µ–Ω", "—Å–∞–¥", "–≥–æ—Ä–æ–¥", "—Ä–æ—Å–ª–∏–Ω", "—Ä–∞—Å—Ç–µ–Ω", "–∫–≤—ñ—Ç", "—Ü–≤–µ—Ç", "—Ä–æ–∑—Å–∞–¥"]
    stationery_tokens = ["–∑–æ—à–∏—Ç", "—Ç–µ—Ç—Ä–∞–¥", "—Ä—É—á–∫", "–æ–ª—ñ–≤—Ü", "–∫–∞—Ä–∞–Ω–¥–∞—à", "–ø–µ–Ω–∞–ª", "–∫–∞–Ω—Ü–µ–ª", "—Ñ–ª–æ–º–∞—Å—Ç–µ—Ä", "–º–∞—Ä–∫–µ—Ä", "—Ñ–∞—Ä–±", "–∫—Ä–∞—Å–∫", "–ø–∞–ø—ñ—Ä", "–±—É–º–∞–≥", "–∞–ª—å–±–æ–º", "—â–æ–¥–µ–Ω–Ω–∏–∫"]
    cosmetics_tokens = ["–∑—É–±–Ω", "–ø–∞—Å—Ç", "—à–∞–º–ø—É–Ω", "–º–∏–ª–æ", "–≥–µ–ª—å", "–∫—Ä–µ–º", "–∫–æ—Å–º–µ—Ç–∏–∫", "–¥–æ–≥–ª—è–¥", "–≥—ñ–≥—ñ—î–Ω"]
    pets_tokens = ["–∫–æ—Ç—ñ–≤", "–∫—ñ—à–æ–∫", "—Å–æ–±–∞–∫", "—Ç–≤–∞—Ä–∏–Ω", "–∫–æ—Ä–º –¥–ª—è", "–Ω–∞–º–∏—Å—Ç–æ –¥–ª—è", "—ñ–≥—Ä–∞—à–∫–∞ –¥–ª—è –∫–æ—Ç–∞", "–º–∏—Å–∫–∞ –¥–ª—è"]

    is_clothes = any(t in q for t in clothes_tokens)
    is_shoes = any(t in q for t in shoes_tokens)
    is_accessories = any(t in q for t in accessories_tokens)
    is_toys = any(t in q for t in toys_tokens)
    is_house = any(t in q for t in house_tokens)
    is_fishing = any(t in q for t in fishing_tokens)
    is_garden = any(t in q for t in garden_tokens)
    is_stationery = any(t in q for t in stationery_tokens)
    is_cosmetics = any(t in q for t in cosmetics_tokens)
    is_pets = any(t in q for t in pets_tokens)

    # –†–∏–±–æ–ª–æ–≤–ª—è
    if is_fishing:
        return {"fishing"}
    
    # –°–∞–¥—ñ–≤–Ω–∏—Ü—Ç–≤–æ
    if is_garden:
        return {"garden"}
    
    # –ö–∞–Ω—Ü–µ–ª—è—Ä—ñ—è
    if is_stationery:
        return {"stationery"}
    
    # –ö–æ—Å–º–µ—Ç–∏–∫–∞
    if is_cosmetics:
        return {"cosmetics"}
    
    # –¢–æ–≤–∞—Ä–∏ –¥–ª—è —Ç–≤–∞—Ä–∏–Ω
    if is_pets:
        return {"pets"}

    # –û–¥—è–≥ (–±–µ–∑ –≤–∑—É—Ç—Ç—è —Ç–∞ –∞–∫—Å–µ—Å—É–∞—Ä—ñ–≤)
    if is_clothes and not is_shoes and not is_toys and not is_house:
        return {code for code in CATEGORY_SCHEMA.keys() if code.startswith("clothes_") and code != "clothes_shoes" and code != "clothes_accessories"}
    
    # –í–∑—É—Ç—Ç—è –æ–∫—Ä–µ–º–æ
    if is_shoes and not is_toys and not is_house:
        return {code for code in CATEGORY_SCHEMA.keys() if code == "clothes_shoes" or code.startswith("shoes_")}
    
    # –ê–∫—Å–µ—Å—É–∞—Ä–∏ (—à–∫–∞—Ä–ø–µ—Ç–∫–∏, –∫–æ–ª–≥–æ—Ç–∏, —à–∞–ø–∫–∏)
    if is_accessories and not is_shoes and not is_toys and not is_house:
        return {code for code in CATEGORY_SCHEMA.keys() if code == "clothes_accessories" or code.startswith("accessories_") or "sock" in code or "tights" in code}
    
    # –Ü–≥—Ä–∞—à–∫–∏
    if is_toys and not is_clothes and not is_shoes:
        return {code for code in CATEGORY_SCHEMA.keys() if code.startswith("toys_") or code.startswith("games_")}
    
    # –ì–æ—Å–ø–æ–¥–∞—Ä—Å—å–∫—ñ —Ç–æ–≤–∞—Ä–∏
    if is_house and not (is_clothes or is_toys or is_shoes):
        return {code for code in CATEGORY_SCHEMA.keys() if code.startswith("house_")}
    
    return None

def _aggregate_categories(products: List["SearchResult"]) -> Tuple[Dict[str, List[SearchResult]], List[Tuple[str, int]]]:
    buckets: Dict[str, List[SearchResult]] = {}
    for p in products:
        code = _assign_category_code(p)
        if not code:
            continue
        buckets.setdefault(code, []).append(p)
    counts = sorted(((c, len(v)) for c, v in buckets.items()), key=lambda x: x[1], reverse=True)
    return buckets, counts

# Utilities
def _validate_query_basic(query: str) -> Tuple[bool, Optional[str]]:
    """
    –ë–∞–∑–æ–≤–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è –∑–∞–ø–∏—Ç—É.
    Returns: (is_valid, error_message)
    """
    if not query or not query.strip():
        return False, "–ó–∞–ø–∏—Ç –Ω–µ –º–æ–∂–µ –±—É—Ç–∏ –ø–æ—Ä–æ–∂–Ω—ñ–º"
    
    query = query.strip()
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –¥–æ–≤–∂–∏–Ω–∏
    if len(query) < 2:
        return False, "–ó–∞–ø–∏—Ç –∑–∞–Ω–∞–¥—Ç–æ –∫–æ—Ä–æ—Ç–∫–∏–π. –ù–∞–ø–∏—à—ñ—Ç—å —Ö–æ—á–∞ –± 2 —Å–∏–º–≤–æ–ª–∏."
    
    if len(query) > 500:
        return False, "–ó–∞–ø–∏—Ç –∑–∞–Ω–∞–¥—Ç–æ –¥–æ–≤–≥–∏–π. –ú–∞–∫—Å–∏–º—É–º 500 —Å–∏–º–≤–æ–ª—ñ–≤."
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ —Ç—ñ–ª—å–∫–∏ —Ü–∏—Ñ—Ä–∏ –∞–±–æ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–∏
    if re.match(r'^[\d\s\W]+$', query) and not re.search(r'[a-zA-Z–∞-—è–ê-–Ø—ñ—ó—î“ë–Ü–á–Ñ“ê]', query):
        return False, "–ë—É–¥—å –ª–∞—Å–∫–∞, –Ω–∞–ø–∏—à—ñ—Ç—å —Ç–µ–∫—Å—Ç–æ–≤–∏–π –∑–∞–ø–∏—Ç."
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ —Å–ø–∞–º (–ø–æ–≤—Ç–æ—Ä—é–≤–∞–Ω—ñ —Å–∏–º–≤–æ–ª–∏)
    if re.search(r'(.)\1{7,}', query):  # 8+ –æ–¥–Ω–∞–∫–æ–≤–∏—Ö —Å–∏–º–≤–æ–ª—ñ–≤ –ø—ñ–¥—Ä—è–¥
        return False, "–ë—É–¥—å –ª–∞—Å–∫–∞, –Ω–∞–ø–∏—à—ñ—Ç—å –∫–æ—Ä–µ–∫—Ç–Ω–∏–π –∑–∞–ø–∏—Ç."
    
    return True, None


# –ü–æ–≤–Ω–∏–π —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π –º–∞–≥–∞–∑–∏–Ω—É TA-DA! (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è –∫–∞—Ç–∞–ª–æ–≥—É)
FULL_CATALOG_CATEGORIES = [
    "–û–¥—è–≥ (—Ñ—É—Ç–±–æ–ª–∫–∏, —à—Ç–∞–Ω–∏, –ø—ñ–∂–∞–º–∏, —Å—É–∫–Ω—ñ)",
    "–í–∑—É—Ç—Ç—è (–∫–∞–ø—Ü—ñ, —à–ª—å–æ–ø–∞–Ω—Ü—ñ, —á–æ–±–æ—Ç–∏)",
    "–ö–æ–ª–≥–æ—Ç–∏ —Ç–∞ —à–∫–∞—Ä–ø–µ—Ç–∫–∏",
    "–ü–æ—Å—É–¥ (—Ç–∞—Ä—ñ–ª–∫–∏, —á–∞—à–∫–∏, –∫–∞—Å—Ç—Ä—É–ª—ñ, —Å–∫–æ–≤–æ—Ä—ñ–¥–∫–∏)",
    "–ö—É—Ö–æ–Ω–Ω–µ –ø—Ä–∏–ª–∞–¥–¥—è",
    "–ì–æ—Å–ø–æ–¥–∞—Ä—Å—å–∫—ñ —Ç–æ–≤–∞—Ä–∏ (–º–∏—é—á—ñ –∑–∞—Å–æ–±–∏, –≥—É–±–∫–∏, —Å–µ—Ä–≤–µ—Ç–∫–∏)",
    "–ö–æ—Å–º–µ—Ç–∏–∫–∞ —Ç–∞ –≥—ñ–≥—ñ—î–Ω–∞",
    "–î–∏—Ç—è—á—ñ —ñ–≥—Ä–∞—à–∫–∏",
    "–ö–∞–Ω—Ü–µ–ª—è—Ä—ñ—è (–∑–æ—à–∏—Ç–∏, —Ä—É—á–∫–∏, –æ–ª—ñ–≤—Ü—ñ)",
    "–¢–µ–∫—Å—Ç–∏–ª—å (—Ä—É—à–Ω–∏–∫–∏, –ø–æ—Å—Ç—ñ–ª—å–Ω–∞ –±—ñ–ª–∏–∑–Ω–∞)",
    "–î–µ–∫–æ—Ä –¥–ª—è –¥–æ–º—É",
    "–†–∏–±–∞–ª—å—Å—å–∫—ñ —Ç–æ–≤–∞—Ä–∏",
    "–ù–∞—Å—ñ–Ω–Ω—è –¥–ª—è —Å–∞–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞",
    "–¢–æ–≤–∞—Ä–∏ –¥–ª—è —Ç–≤–∞—Ä–∏–Ω"
]



def _extract_json_safely(text: str) -> Dict[str, Any]:
    """Extracts JSON from model response, ignoring code blocks and other symbols."""
    if not text:
        return {}
    match = re.search(r"```(?:json)?\s*(\{.*?\})\s*```", text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(1))
        except json.JSONDecodeError:
            pass

    stack, start_index = [], -1
    best_json = {}
    max_len = 0
    for i, char in enumerate(text):
        if char == '{':
            if not stack:
                start_index = i
            stack.append('{')
        elif char == '}':
            if stack:
                stack.pop()
                if not stack and start_index != -1:
                    substring = text[start_index: i + 1]
                    try:
                        parsed = json.loads(substring)
                        if len(substring) > max_len:
                            best_json, max_len = parsed, len(substring)
                    except json.JSONDecodeError:
                        continue
    if best_json:
        return best_json

    try:
        return json.loads(text)
    except json.JSONDecodeError:
        logger.warning(f"Failed to extract JSON from text: {text[:500]}...")
        return {}

# Services
class EmbeddingService:
    def __init__(self, http_client: httpx.AsyncClient, cache: TTLCache):
        self.http_client = http_client
        self.cache = cache

    @staticmethod
    def _hash_text(text: str) -> str:
        return hashlib.md5(text.encode("utf-8")).hexdigest()

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=10),
        retry=retry_if_exception_type((httpx.RequestError, httpx.TimeoutException))
    )
    async def _call_ollama_api(self, text: str) -> Optional[List[float]]:
        """Compatibility with different embedding APIs."""
        payload_candidates = [
            {"model": settings.ollama_model_name, "prompt": text},
            {"model": settings.ollama_model_name, "input": text},
            {"model": settings.ollama_model_name, "input": [text]},
        ]
        last_exc = None
        for payload in payload_candidates:
            try:
                r = await self.http_client.post(
                    settings.embedding_api_url,
                    json=payload,
                    timeout=settings.request_timeout
                )
                r.raise_for_status()
                data = r.json()
                emb = None
                if isinstance(data, dict):
                    if isinstance(data.get("embedding"), list):
                        emb = data["embedding"]
                    elif isinstance(data.get("embeddings"), list):
                        emb = data["embeddings"]
                    elif isinstance(data.get("data"), list) and data["data"]:
                        maybe = data["data"][0]
                        if isinstance(maybe, dict) and isinstance(maybe.get("embedding"), list):
                            emb = maybe["embedding"]
                if isinstance(emb, list) and (settings.vector_dimension <= 0 or len(emb) == settings.vector_dimension):
                    return emb
                if emb is not None and isinstance(emb, list):
                    logger.warning("Embedding dimension mismatch; accepting as is.")
                    return emb
            except Exception as e:
                last_exc = e
                logger.warning(f"Embedding payload variant failed: {e}")
                continue
        if last_exc:
            logger.error(f"Failed to call embedding API: {last_exc}")
        return None

    async def generate_embedding(self, text: str) -> Optional[List[float]]:
        text = (text or "").strip()
        if not text:
            return None
        key = self._hash_text(text)
        cached = await self.cache.get(key)
        if cached is not None:
            logger.info("Embedding cache hit")
            return cached
        try:
            t0 = time.time()
            emb = await asyncio.wait_for(self._call_ollama_api(text), timeout=15.0)  # –ó–±—ñ–ª—å—à–µ–Ω–æ –¥–æ 15s
            if emb:
                await self.cache.put(key, emb)
                logger.info(f"Embedding generated in {time.time()-t0:.2f}s")
                return emb
        except asyncio.TimeoutError:
            logger.warning(f"‚è±Ô∏è Embedding timeout –ø—ñ—Å–ª—è 15s –¥–ª—è: '{text[:50]}...'")
        except Exception as e:
            logger.error(f"Embedding generation error: {e}")
        return None

    async def generate_embeddings_parallel(self, texts: List[str], max_concurrent: int = 2) -> List[Optional[List[float]]]:
        """
        Parallel embedding generation with concurrency limit.
        
        Args:
            texts: List of texts to generate embeddings for
            max_concurrent: Maximum number of concurrent embedding requests (default: 2)
                           –û–±–º–µ–∂—É—î–º–æ –¥–æ 2 —â–æ–± –Ω–µ –ø–µ—Ä–µ–≤–∞–Ω—Ç–∞–∂—É–≤–∞—Ç–∏ –ø–æ–≤—ñ–ª—å–Ω–∏–π embedding API
        """
        if not texts:
            return []
        
        # –°–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–±–º–µ–∂–µ–Ω–Ω—è –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –æ–¥–Ω–æ—á–∞—Å–Ω–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def generate_with_semaphore(text: str) -> Optional[List[float]]:
            async with semaphore:
                return await self.generate_embedding(text)
        
        tasks = [generate_with_semaphore(text) for text in texts]
        embeddings = await asyncio.gather(*tasks, return_exceptions=True)
        
        result = []
        success_count = 0
        for i, emb in enumerate(embeddings):
            if isinstance(emb, Exception):
                logger.error(f"‚ùå Embedding error for '{texts[i][:60]}...': {emb}")
                result.append(None)
            elif emb is None:
                logger.warning(f"‚ö†Ô∏è Embedding returned None for '{texts[i][:60]}...'")
                result.append(None)
            else:
                logger.debug(f"‚úÖ Embedding OK for '{texts[i][:60]}...' (dim={len(emb)})")
                result.append(emb)
                success_count += 1
        
        logger.info(f"üìä Parallel embedding (max {max_concurrent} concurrent): {success_count}/{len(texts)} successful")
        
        return result

class ElasticsearchService:
    def __init__(self, es_client: AsyncElasticsearch):
        self.es_client = es_client

    async def semantic_search(self, query_vector: List[float], k: int = 10) -> List[Dict]:
        """
        Semantic search using vector field configured in settings.
        
        Currently indexed vector field:
        - description_vector: embeddings from description text
        
        NOTE: The description_vector field contains embeddings of product descriptions.
        This means semantic search works best for conceptual queries about product features,
        while BM25 is better for exact product names and codes.
        Together in hybrid search, they complement each other.
        """
        try:
            search_params = {
                "index": settings.index_name,
                "size": k,
                "knn": {
                    "field": settings.vector_field_name,
                    "query_vector": query_vector,
                    "k": k,
                    "num_candidates": min(settings.knn_num_candidates, max(100, k * 20))
                }
            }
            res = await self.es_client.search(**search_params)
            hits = res.get("hits", {}).get("hits", [])
            
            # Fallback to description_vector if configured field doesn't work
            if not hits and settings.vector_field_name != "description_vector":
                logger.warning(f"No results from {settings.vector_field_name}, trying description_vector fallback")
                search_params["knn"]["field"] = "description_vector"
                res = await self.es_client.search(**search_params)
                hits = res.get("hits", {}).get("hits", [])
            
            return hits
        except Exception as e:
            logger.error(f"Semantic search error with {settings.vector_field_name}: {e}")
            # Try fallback to description_vector
            if settings.vector_field_name != "description_vector":
                try:
                    logger.info("Attempting fallback to description_vector")
                    search_params = {
                        "index": settings.index_name,
                        "size": k,
                        "knn": {
                            "field": "description_vector",
                            "query_vector": query_vector,
                            "k": k,
                            "num_candidates": min(settings.knn_num_candidates, max(100, k * 20))
                        }
                    }
                    res = await self.es_client.search(**search_params)
                    return res.get("hits", {}).get("hits", [])
                except Exception as fallback_error:
                    logger.error(f"Semantic search fallback also failed: {fallback_error}")
            return []

    async def multi_semantic_search(
        self,
        query_vectors: List[Tuple[str, List[float]]],
        k_per_query: int = 20
    ) -> Dict[str, List[Dict]]:
        """Parallel semantic search for multiple vectors."""
        if not query_vectors:
            return {}
        
        tasks = []
        subquery_names = []
        
        for subquery, vector in query_vectors:
            if vector is not None:
                tasks.append(self.semantic_search(vector, k_per_query))
                subquery_names.append(subquery)
        
        if not tasks:
            logger.warning("multi_semantic_search: no valid vectors")
            return {}
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        output = {}
        for i, (subquery, result) in enumerate(zip(subquery_names, results)):
            if not isinstance(result, Exception):
                output[subquery] = result
                logger.info(f"'{subquery}': found {len(result)} products")
            else:
                logger.warning(f"Search error for '{subquery}': {result}")
                output[subquery] = []
        
        logger.info(f"Parallel search: {len(output)}/{len(query_vectors)} successful")
        
        return output

    async def bm25_search(self, query_text: str, k: int = 10) -> List[Dict]:
        try:
            res = await self.es_client.search(
                index=settings.index_name,
                min_score=float(settings.bm25_min_score),
                query={
                    "bool": {
                        "should": [
                            {"multi_match": {"query": query_text, "fields": ["title_ua^6", "title_ru^6"], "type": "phrase", "boost": 5.0}},
                            {"multi_match": {"query": query_text, "fields": ["title_ua^5", "title_ru^5"], "type": "best_fields", "fuzziness": "AUTO", "boost": 4.0}},
                            {"multi_match": {"query": query_text, "fields": ["description_ua^2", "description_ru^2"], "type": "best_fields", "fuzziness": "AUTO", "boost": 2.0}},
                            {"multi_match": {"query": query_text, "fields": ["sku^3", "good_code^2", "uktzed^1"], "type": "best_fields", "boost": 3.0}}
                        ],
                        "minimum_should_match": 1
                    }
                },
                size=k
            )
            return res.get("hits", {}).get("hits", [])
        except Exception as e:
            logger.error(f"BM25 search error: {e}")
            return []

    async def dsl_search(self, dsl: Dict[str, Any], k: int = 50) -> List[Dict]:
        try:
            dsl_local = dict(dsl or {})
            size = dsl_local.pop("size", k)
            res = await self.es_client.search(index=settings.index_name, size=size, **dsl_local)
            return res.get("hits", {}).get("hits", [])
        except Exception as e:
            logger.error(f"DSL search error: {e}")
            return []

    async def hybrid_search(
        self, 
        query_vector: Optional[List[float]], 
        query_text_semantic: str,
        query_text_bm25: str,
        k: int = 10
    ) -> List[Dict]:
        """
        Hybrid search combining semantic (vector) and lexical (BM25) search.
        
        This provides the best of both worlds:
        - Semantic search: finds conceptually similar products (synonyms, related items)
        - BM25 search: finds exact keyword matches (product names, codes)
        
        The results are merged using weighted combination (configurable via HYBRID_ALPHA).
        
        Args:
            query_vector: Embedding vector for semantic search
            query_text_semantic: Text query for semantic context (currently unused)
            query_text_bm25: Text query for BM25 lexical search
            k: Number of results to return
            
        Returns:
            List of merged search results, sorted by combined score
        """
        try:
            if not query_vector:
                logger.error("hybrid_search: no embedding vector provided")
                raise ValueError("Query vector is required for hybrid search")
            
            # Run both searches in parallel for better performance
            # Get 2x candidates to improve merge quality
            candidates = max(k * 2, 50)
            
            sem_task = asyncio.create_task(self.semantic_search(query_vector, candidates))
            bm_task = asyncio.create_task(self.bm25_search(query_text_bm25, candidates))
            
            sem, bm = await asyncio.gather(sem_task, bm_task)
            
            logger.info(f"Hybrid search: semantic={len(sem)}, BM25={len(bm)} candidates")
            
            # Merge and return top k results
            return self._merge(sem, bm, k)
            
        except Exception as e:
            logger.error(f"Hybrid search error: {e}")
            raise

    def _merge(self, sem: List[Dict], bm: List[Dict], k: int) -> List[Dict]:
        if settings.hybrid_fusion.lower() == "rrf":
            return self._rrf_merge(sem, bm, k)
        return self._weighted_merge(sem, bm, k)

    def _weighted_merge(self, sem: List[Dict], bm: List[Dict], k: int) -> List[Dict]:
        """
        Weighted merge of semantic and BM25 results.
        
        Alpha (HYBRID_ALPHA) controls the balance:
        - 0.7 = 70% semantic + 30% BM25 (good for conceptual search)
        - 0.5 = 50% semantic + 50% BM25 (balanced)
        - 0.3 = 30% semantic + 70% BM25 (good for exact matches)
        
        Scores are normalized by max score in each result set before combining.
        """
        alpha = settings.hybrid_alpha
        beta = 1.0 - alpha
        
        # Normalize scores
        sem_scores = [h.get("_score", 0.0) for h in sem]
        bm_scores = [h.get("_score", 0.0) for h in bm]
        max_sem = max(sem_scores) if sem_scores else 1.0
        max_bm = max(bm_scores) if bm_scores else 1.0
        
        combined: Dict[str, float] = {}
        pool: Dict[str, Dict] = {}
        
        # Add semantic results
        for h in sem:
            _id = h["_id"]
            pool[_id] = h
            normalized_score = h.get("_score", 0.0) / max_sem
            combined[_id] = combined.get(_id, 0.0) + alpha * normalized_score
        
        # Add BM25 results
        for h in bm:
            _id = h["_id"]
            pool[_id] = pool.get(_id) or h
            normalized_score = h.get("_score", 0.0) / max_bm
            combined[_id] = combined.get(_id, 0.0) + beta * normalized_score
        
        # Sort by combined score
        ordered = sorted(combined.items(), key=lambda x: x[1], reverse=True)
        
        out = []
        for _id, sc in ordered[:k]:
            hit = pool[_id]
            hit["_score"] = sc
            out.append(hit)
        
        logger.info(f"Hybrid merge: {len(sem)} semantic + {len(bm)} BM25 ‚Üí {len(out)} results (Œ±={alpha:.2f})")
        return out

    def _rrf_merge(self, sem: List[Dict], bm: List[Dict], k: int, c: int = 30) -> List[Dict]:
        scores: Dict[str, float] = {}
        pool: Dict[str, Dict] = {}
        for r, h in enumerate(sem):
            pool[h["_id"]] = h
            scores[h["_id"]] = scores.get(h["_id"], 0.0) + 1.0 / (c + r + 1)
        for r, h in enumerate(bm):
            pool[h["_id"]] = pool.get(h["_id"]) or h
            scores[h["_id"]] = scores.get(h["_id"], 0.0) + 1.0 / (c + r + 1)
        ordered = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        out = []
        for _id, sc in ordered[:k]:
            hit = pool[_id]
            hit["_score"] = sc
            out.append(hit)
        logger.info(f"Hybrid merge(RRF c={c}) -> {len(out)}")
        return out

    async def get_index_stats(self) -> Dict[str, Any]:
        try:
            stats_task = self.es_client.indices.stats(index=settings.index_name)
            health_task = self.es_client.cluster.health(index=settings.index_name)
            stats, health = await asyncio.gather(stats_task, health_task)
            idx = stats.get("indices", {}).get(settings.index_name, {})
            total = (idx.get("total") or {})
            docs = ((total.get("docs") or {}).get("count")) or 0
            size = ((total.get("store") or {}).get("size_in_bytes")) or 0
            status = health.get("status", "unknown")
            return {
                "documents_count": int(docs),
                "index_size_bytes": int(size),
                "health": status
            }
        except Exception as e:
            logger.error(f"Index stats error: {e}")
            return {"documents_count": 0, "index_size_bytes": 0, "health": "unknown"}

CONTEXTUAL_SYNONYMS = {
    "–æ–¥—è–≥": ["–∫—É—Ä—Ç–∫–∞", "—à—Ç–∞–Ω–∏", "—Å–≤—ñ—Ç–µ—Ä", "—Ñ—É—Ç–±–æ–ª–∫–∞", "—Å—É–∫–Ω—è", "–¥–∂–∏–Ω—Å–∏"],
    "—Ö–ª–æ–ø—á–∏–∫": ["–¥–∏—Ç—è—á–∏–π", "–¥–ª—è —Ö–ª–æ–ø—Ü—è"],
    "–¥—ñ–≤—á–∏–Ω–∫–∞": ["–¥–∏—Ç—è—á–∏–π", "–¥–ª—è –¥—ñ–≤—á–∏–Ω–∏"],
    "–¥–∏—Ç–∏–Ω–∞": ["–¥–∏—Ç—è—á–∏–π"],
    "—à—É—Ä—É–ø–∏": ["–∫—Ä—ñ–ø–ª–µ–Ω–Ω—è", "–±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–æ", "–≥–≤–∏–Ω—Ç–∏", "—Å–∞–º–æ—Ä—ñ–∑–∏"],
    "–º–µ–±–ª—ñ": ["–ª—ñ–∂–∫–æ", "—Ç—É–º–±–∞", "–∫–æ–º–æ–¥", "—à–∞—Ñ–∞", "—Å—Ç—ñ–ª", "—Å—Ç—ñ–ª–µ—Ü—å"],
    "–∫—É—Ö–Ω—è": ["–ø–æ—Å—É–¥", "—Å–∫–æ–≤–æ—Ä—ñ–¥–∫–∞", "–∫–∞—Å—Ç—Ä—É–ª—è", "—Ç–∞—Ä—ñ–ª–∫–∏", "—á–∞—à–∫–∏"],
    "–ø—Ä–∏–±–∏—Ä–∞–Ω–Ω—è": ["–ø–æ–±—É—Ç–æ–≤–∞ —Ö—ñ–º—ñ—è", "–≥–∞–Ω—á—ñ—Ä–∫–∞", "–º–∏—é—á–∏–π –∑–∞—Å—ñ–±", "—à–≤–∞–±—Ä–∞"]
}

# –î–µ—Ç–∞–ª—å–Ω–∞ —Å—Ö–µ–º–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π –¥–ª—è GPT (–Ω–∞ –±–∞–∑—ñ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∞—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç—É 38,420 —Ç–æ–≤–∞—Ä—ñ–≤)
CATEGORY_SCHEMA = {
    "accessories": {
        "label": "–ê–∫—Å–µ—Å—É–∞—Ä–∏",
        "keywords": ["—à–∫–∞—Ä–ø–µ—Ç–∫–∏", "–∫–æ–ª–≥–æ—Ç–∏", "–≥–æ–ª—å—Ñ–∏", "–ø–∞–Ω—á–æ—Ö–∏", "—Å–ª—ñ–¥–∏", "—à–∞–ø–∫–∞", "—à–∞—Ä—Ñ", "—Ä—É–∫–∞–≤–∏—Ü—ñ", "—Ä–µ–º—ñ–Ω—å", "—Å—É–º–∫–∞", "–≥–∞–º–∞–Ω–µ—Ü—å", "—Ä—é–∫–∑–∞–∫", "–ø–∞—Ä–∞—Å–æ–ª—å–∫–∞", "–æ–∫—É–ª—è—Ä–∏", "–∑–∞–∫–æ–ª–∫–∞", "—Ä–µ–∑–∏–Ω–∫–∞ –¥–ª—è –≤–æ–ª–æ—Å—Å—è"]
    },
    "toys": {
        "label": "–Ü–≥—Ä–∞—à–∫–∏",
        "keywords": ["—ñ–≥—Ä–∞—à–∫–∞", "–ª—è–ª—å–∫–∞", "–º–∞—à–∏–Ω–∫–∞", "–∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä", "–ø–∞–∑–ª", "–º'—è–∫–∞ —ñ–≥—Ä–∞—à–∫–∞", "–∞–≤—Ç–æ–º–∞—Ç", "–ø—ñ—Å—Ç–æ–ª–µ—Ç", "—Ç—Ä–∞–∫—Ç–æ—Ä", "–¥–∏–Ω–æ–∑–∞–≤—Ä"]
    },
    "clothing": {
        "label": "–û–¥—è–≥",
        "keywords": ["—Ñ—É—Ç–±–æ–ª–∫–∞", "—à—Ç–∞–Ω–∏", "—à–æ—Ä—Ç–∏", "—Å—É–∫–Ω—è", "–∫–æ—Å—Ç—é–º", "–∫–æ—Ñ—Ç–∞", "—Å–≤—ñ—Ç—à–æ—Ç", "—Ö–∞–ª–∞—Ç", "–ø—ñ–∂–∞–º–∞", "–∂–∏–ª–µ—Ç", "–∫—É—Ä—Ç–∫–∞", "—Ö—É–¥—ñ", "–ª–æ–Ω–≥—Å–ª—ñ–≤", "–¥–∂–∏–Ω—Å–∏", "–≤–æ–¥–æ–ª–∞–∑–∫–∞"]
    },
    "stationery": {
        "label": "–ö–∞–Ω—Ü–µ–ª—è—Ä—ñ—è",
        "keywords": ["–∑–æ—à–∏—Ç", "–±–ª–æ–∫–Ω–æ—Ç", "–æ–ª—ñ–≤–µ—Ü—å", "—Ä—É—á–∫–∞", "–º–∞—Ä–∫–µ—Ä", "—Ñ–∞—Ä–±–∏", "–ø–µ–Ω–∑–ª–∏–∫", "–∞–∫–≤–∞—Ä–µ–ª—å", "—Å—Ç—Ä—É–≥–∞—á–∫–∞", "–ø–∞–ø–∫–∞", "–ø–µ–Ω–∞–ª", "—â–æ–¥–µ–Ω–Ω–∏–∫", "–ø–∞–ø—ñ—Ä", "–∫–ª–µ–π"]
    },
    "household": {
        "label": "–ì–æ—Å–ø–æ–¥–∞—Ä—Å—å–∫—ñ —Ç–æ–≤–∞—Ä–∏",
        "keywords": ["–≤—ñ–¥—Ä–æ", "–º–∏—Å–∫–∞", "—Ç–∞–∑", "–≥—É–±–∫–∞", "—â—ñ—Ç–∫–∞", "—à–≤–∞–±—Ä–∞", "–≥–∞–Ω—á—ñ—Ä–∫–∞", "–º–∏–ª–æ", "–∑–∞—Å—ñ–±", "–ø—Ä–∞–Ω–Ω—è", "–º–∏—Ç—Ç—è", "—á–∏—â–µ–Ω–Ω—è", "—Å–µ—Ä–≤–µ—Ç–∫–∞"]
    },
    "tableware": {
        "label": "–ö—É—Ö–æ–Ω–Ω–∏–π –ø–æ—Å—É–¥",
        "keywords": ["–∫–∞—Å—Ç—Ä—É–ª—è", "—Å–∫–æ–≤–æ—Ä–æ–¥–∞", "–ª–æ–∂–∫–∞", "–≤–∏–¥–µ–ª–∫–∞", "–Ω—ñ–∂", "—Ç–∞—Ä—ñ–ª–∫–∞", "—á–∞—à–∫–∞", "—Å—Ç–∞–∫–∞–Ω", "–∫–µ–ª–∏—Ö", "–±–ª—é–¥–æ", "—Å–∞–ª–∞—Ç–Ω–∏–∫", "—Ñ–æ—Ä–º–∞ –¥–ª—è –≤–∏–ø—ñ–∫–∞–Ω–Ω—è"]
    },
    "garden": {
        "label": "–î–ª—è —Å–∞–¥—É —ñ –≥–æ—Ä–æ–¥—É",
        "keywords": ["–Ω–∞—Å—ñ–Ω–Ω—è", "–¥–æ–±—Ä–∏–≤–æ", "–≥—Ä—É–Ω—Ç", "–≥–æ—Ä—â–æ–∫", "–ª–æ–ø–∞—Ç–∞", "–≥—Ä–∞–±–ª—ñ", "—ñ–Ω—Å–µ–∫—Ç–∏—Ü–∏–¥", "—Ñ—É–Ω–≥—ñ—Ü–∏–¥", "—à–ª–∞–Ω–≥", "—Å—É–±—Å—Ç—Ä–∞—Ç"]
    },
    "cosmetics": {
        "label": "–ö–æ—Å–º–µ—Ç–∏–∫–∞ —ñ –≥—ñ–≥—ñ—î–Ω–∞",
        "keywords": ["—à–∞–º–ø—É–Ω—å", "–±–∞–ª—å–∑–∞–º", "–∫—Ä–µ–º", "–≥–µ–ª—å –¥–ª—è –¥—É—à—É", "–∑—É–±–Ω–∞ –ø–∞—Å—Ç–∞", "–∑—É–±–Ω–∞ —â—ñ—Ç–∫–∞", "–¥–µ–∑–æ–¥–æ—Ä–∞–Ω—Ç", "—Ñ–∞—Ä–±–∞ –¥–ª—è –≤–æ–ª–æ—Å—Å—è", "—Ç—É—à", "–ø–æ–º–∞–¥–∞"]
    },
    "footwear": {
        "label": "–í–∑—É—Ç—Ç—è",
        "keywords": ["—á–æ–±–æ—Ç–∏", "—á–µ—Ä–µ–≤–∏–∫–∏", "–∫—Ä–æ—Å—ñ–≤–∫–∏", "–∫–µ–¥–∏", "—Ç–∞–ø–∫–∏", "—Ç–∞–ø–æ—á–∫–∏", "—à–ª—å–æ–ø–∞–Ω—Ü—ñ", "–∫–∞–ø—Ü—ñ", "—Ç—É—Ñ–ª—ñ", "–±–æ—Å–æ–Ω—ñ–∂–∫–∏"]
    },
    "electrical": {
        "label": "–ï–ª–µ–∫—Ç—Ä–æ—Ç–æ–≤–∞—Ä–∏",
        "keywords": ["–ª–∞–º–ø–∞", "–ª—ñ—Ö—Ç–∞—Ä", "–ø–æ–¥–æ–≤–∂—É–≤–∞—á", "—Ä–æ–∑–µ—Ç–∫–∞", "–≤–∏–º–∏–∫–∞—á", "–±–∞—Ç–∞—Ä–µ–π–∫–∞", "–∑–∞—Ä—è–¥–Ω–∏–π", "–∫–∞–±–µ–ª—å", "–Ω–∞–≤—É—à–Ω–∏–∫–∏"]
    },
    "festive": {
        "label": "–°–≤—è—Ç–∫–æ–≤—ñ —Ç–æ–≤–∞—Ä–∏",
        "keywords": ["—Å–≤—ñ—á–∫–∞", "–ª–∏—Å—Ç—ñ–≤–∫–∞", "–∫–æ—Ä–æ–±–∫–∞ –ø–æ–¥–∞—Ä—É–Ω–∫–æ–≤–∞", "–≤–∞–ª–µ–Ω—Ç–∏–Ω–∫–∞", "–≥—ñ—Ä–ª—è–Ω–¥–∞", "–∫—É–ª—å–∫–∞", "–∑–Ω–∞—á–æ–∫", "–º–∞–≥–Ω—ñ—Ç", "–æ–¥–Ω–æ—Ä–∞–∑–æ–≤–∏–π –ø–æ—Å—É–¥"]
    },
    "containers": {
        "label": "–ö–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏ —ñ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è",
        "keywords": ["–∫–æ–Ω—Ç–µ–π–Ω–µ—Ä", "–æ—Ä–≥–∞–Ω–∞–π–∑–µ—Ä", "–∫–æ—Ä–æ–±–∫–∞", "—î–º–Ω—ñ—Å—Ç—å", "–ª–æ—Ç–æ–∫"]
    },
    "food": {
        "label": "–ü—Ä–æ–¥—É–∫—Ç–∏ —Ö–∞—Ä—á—É–≤–∞–Ω–Ω—è",
        "keywords": ["–ø–µ—á–∏–≤–æ", "—Ü—É–∫–µ—Ä–∫–∏", "—à–æ–∫–æ–ª–∞–¥", "—á—ñ–ø—Å–∏", "–Ω–∞–ø—ñ–π", "–∫–∞–≤–∞", "–º–æ—Ä–æ–∑–∏–≤–æ", "–ø–æ–Ω—á–∏–∫", "—Ç—ñ—Å—Ç–µ—á–∫–æ", "—Ö–ª—ñ–±", "—Å–æ—É—Å"]
    },
    "textiles": {
        "label": "–î–æ–º–∞—à–Ω—ñ–π —Ç–µ–∫—Å—Ç–∏–ª—å",
        "keywords": ["–∫–æ–≤–¥—Ä–∞", "–ø–æ–¥—É—à–∫–∞", "—Ä—É—à–Ω–∏–∫", "—Å–∫–∞—Ç–µ—Ä—Ç–∏–Ω–∞", "—Å–µ—Ä–≤–µ—Ç–∫–∞", "–∫–∏–ª–∏–º", "—à—Ç–æ—Ä–∞", "–ø–æ—Å—Ç—ñ–ª—å–Ω–∞ –±—ñ–ª–∏–∑–Ω–∞"]
    },
    "pets": {
        "label": "–¢–æ–≤–∞—Ä–∏ –¥–ª—è —Ç–≤–∞—Ä–∏–Ω",
        "keywords": ["–∫–æ—Ä–º", "–ª–∞—Å–æ—â—ñ", "–Ω–∞—à–∏–π–Ω–∏–∫", "–ø–æ–≤—ñ–¥–µ—Ü—å", "–ª–µ–∂–∞–∫", "–≥–æ–¥—ñ–≤–Ω–∏—Ü—è", "–ø—Ä–æ—Ç–∏–ø–∞—Ä–∞–∑–∏—Ç–Ω–∏–π", "–¥–ª—è –∫–æ—Ç—ñ–≤", "–¥–ª—è —Å–æ–±–∞–∫"]
    },
    "fishing": {
        "label": "–†–∏–±–æ–ª–æ–≤–ª—è",
        "keywords": ["–≤—É–¥–∏–ª–∏—â–µ", "–ª–µ—Å–∞", "–≥–∞—á–æ–∫", "–∫–æ—Ç—É—à–∫–∞", "–≤–æ–±–ª–µ—Ä", "–ø—Ä–∏–º–∞–Ω–∫–∞", "–ø–æ–ø–ª–∞–≤–µ—Ü—å", "–ø—Ä–∏–∫–æ—Ä–º–∫–∞", "–±–æ–π–ª"]
    },
    "creativity": {
        "label": "–¢–≤–æ—Ä—á—ñ—Å—Ç—å —ñ —Ö–æ–±—ñ",
        "keywords": ["—Ä–æ–∑–º–∞–ª—å–æ–≤–∫–∞", "–∫–∞—Ä—Ç–∏–Ω–∞ –∑–∞ –Ω–æ–º–µ—Ä–∞–º–∏", "–∞–ª–º–∞–∑–Ω–∞ –º–æ–∑–∞—ó–∫–∞", "—Ñ–æ–∞–º—ñ—Ä–∞–Ω", "—Ñ–µ—Ç—Ä", "–Ω–∞–±—ñ—Ä –¥–ª—è —Ç–≤–æ—Ä—á–æ—Å—Ç—ñ", "–±–∞—Ä–µ–ª—å—î—Ñ"]
    },
    "sports": {
        "label": "–°–ø–æ—Ä—Ç —ñ —Ñ—ñ—Ç–Ω–µ—Å",
        "keywords": ["–º'—è—á", "–µ—Å–ø–∞–Ω–¥–µ—Ä", "–≥–∞–Ω—Ç–µ–ª—ñ", "—Å–∫–∞–∫–∞–ª–∫–∞", "—Ç—Ä–µ–Ω–∞–∂–µ—Ä", "–≤–µ–ª–æ—Å–∏–ø–µ–¥", "—Å–∞–º–æ–∫–∞—Ç", "–±–æ–∫—Å–µ—Ä—Å—å–∫–∏–π"]
    },
    "auto": {
        "label": "–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä–∏",
        "keywords": ["–∞—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –∞–≤—Ç–æ–º–æ–±—ñ–ª—å–Ω–∏–π", "—â—ñ—Ç–∫–∞ —Å–∫–ª–æ–æ—á–∏—Å–Ω–∏–∫–∞", "—Ç—Ä–∏–º–∞—á", "—Ç—Ä–æ—Å", "–∞–≤—Ç–æ—Ö—ñ–º—ñ—è"]
    }
}

# GPT Service
class GPTService:
    def __init__(self, http_client: httpx.AsyncClient):
        self.http_client = http_client
        self.base_url = "https://api.openai.com/v1"

    @retry(
        stop=stop_after_attempt(settings.max_retries),
        wait=wait_exponential(multiplier=1, min=1, max=8),
        retry=retry_if_exception_type((httpx.RequestError, httpx.TimeoutException))
    )
    async def _chat(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        r = await self.http_client.post(
            f"{self.base_url}/chat/completions",
            headers={
                "Authorization": f"Bearer {settings.openai_api_key}",
                "Content-Type": "application/json"
            },
            json=payload,
            timeout=settings.request_timeout
        )
        if r.status_code != 200:
            logger.error(f"HTTP error from OpenAI: {r.status_code}, text: {r.text[:200]}")
        r.raise_for_status()
        return r.json()

    async def unified_chat_assistant(
        self, 
        query: str, 
        search_history: List[SearchHistoryItem],
        dialog_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        üéØ –£–ù–Ü–í–ï–†–°–ê–õ–¨–ù–ò–ô GPT –ê–°–ò–°–¢–ï–ù–¢ - –∑–∞–º—ñ–Ω—é—î –≤—Å—ñ –æ–∫—Ä–µ–º—ñ —Ñ—É–Ω–∫—Ü—ñ—ó.
        
        –û–±—Ä–æ–±–ª—è—î –≤—Å—ñ —Ç–∏–ø–∏ –∑–∞–ø–∏—Ç—ñ–≤:
        - –ü—Ä–∏–≤—ñ—Ç–∞–Ω–Ω—è/–ø—Ä–æ—â–∞–Ω–Ω—è
        - –ù–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ –∑–∞–ø–∏—Ç–∏
        - –£—Ç–æ—á–Ω—é—é—á—ñ –ø–∏—Ç–∞–Ω–Ω—è
        - –ü–æ—à—É–∫ —Ç–æ–≤–∞—Ä—ñ–≤
        
        Returns: {
            "action": str,  # "greeting", "invalid", "clarification", "product_search"
            "confidence": float,
            "assistant_message": str,  # –ü–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—É
            "semantic_subqueries": List[str],  # –ü—ñ–¥–∑–∞–ø–∏—Ç–∏ –¥–ª—è –ø–æ—à—É–∫—É (—è–∫—â–æ product_search)
            "categories": Optional[List[str]],  # –ö–∞—Ç–µ–≥–æ—Ä—ñ—ó –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–Ω—è (—è–∫—â–æ clarification)
            "needs_user_input": bool  # –ß–∏ –ø–æ—Ç—Ä—ñ–±–µ–Ω –ø–æ–¥–∞–ª—å—à–∏–π input –≤—ñ–¥ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
        }
        """
        if not settings.enable_gpt_chat:
            logger.error("‚ùå GPT –≤–∏–º–∫–Ω–µ–Ω–æ (ENABLE_GPT_CHAT=False)")
            raise ValueError("GPT chat assistant is disabled. Please enable ENABLE_GPT_CHAT.")
        
        if not settings.openai_api_key:
            logger.error("‚ùå OpenAI API key –Ω–µ –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–∏–π")
            raise ValueError("OpenAI API key is not configured. Please set OPENAI_API_KEY.")
        
        # –§–æ—Ä–º—É—î–º–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç —ñ—Å—Ç–æ—Ä—ñ—ó
        context = ""
        if search_history:
            recent_history = search_history[-3:]
            context_lines = [f"- –ö–æ—Ä–∏—Å—Ç—É–≤–∞—á —à—É–∫–∞–≤: '{it.query}'" for it in recent_history]
            context = "**–Ü—Å—Ç–æ—Ä—ñ—è –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö –ø–æ—à—É–∫—ñ–≤:**\n" + "\n".join(context_lines)
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –±—É–ª–æ –≤–∂–µ —É—Ç–æ—á–Ω–µ–Ω–Ω—è
        already_clarified = dialog_context and dialog_context.get("clarification_asked", False)
        clarification_note = ""
        if already_clarified:
            suggested_cats = dialog_context.get("categories_suggested", [])
            clarification_note = f"""
‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è **–ö–†–ò–¢–ò–ß–ù–û –í–ê–ñ–õ–ò–í–û** ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è

–ö–æ—Ä–∏—Å—Ç—É–≤–∞—á –í–ñ–ï –û–¢–†–ò–ú–ê–í —É—Ç–æ—á–Ω—é—é—á–µ –ø–∏—Ç–∞–Ω–Ω—è –ø—Ä–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó: {suggested_cats}

–¢–µ–ø–µ—Ä –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á –í–Ü–î–ü–û–í–Ü–î–ê–Ñ –Ω–∞ —Ü–µ –ø–∏—Ç–∞–Ω–Ω—è!

‚ùå –ù–ï –ü–ò–¢–ê–ô –ë–Ü–õ–¨–®–ï –£–¢–û–ß–ù–ï–ù–¨!
‚úÖ –û–ë–û–í'–Ø–ó–ö–û–í–û action: "product_search"
‚úÖ –í–∏–∫–æ—Ä–∏—Å—Ç–∞–π –≤—ñ–¥–ø–æ–≤—ñ–¥—å –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è semantic_subqueries

–ü—Ä–∏–∫–ª–∞–¥–∏:
- –ü–∏—Ç–∞–ª–∏ "–Ø–∫—ñ —Ç–æ–≤–∞—Ä–∏ –¥–ª—è –¥–æ–º—É?" ‚Üí –í—ñ–¥–ø–æ–≤—ñ–¥—å "–ø–æ–∫–∞–∂–∏ —É—Å—ñ" ‚Üí —à—É–∫–∞–π ["—Ç–æ–≤–∞—Ä–∏ –¥–ª—è –¥–æ–º—É", "–¥–æ–º–∞—à–Ω—ñ —Ç–æ–≤–∞—Ä–∏", "–¥–ª—è –¥–æ–º—É"]
- –ü–∏—Ç–∞–ª–∏ "–Ø–∫—ñ —ñ–≥—Ä–∞—à–∫–∏?" ‚Üí –í—ñ–¥–ø–æ–≤—ñ–¥—å "–±—É–¥—å-—è–∫—ñ" ‚Üí —à—É–∫–∞–π ["—ñ–≥—Ä–∞—à–∫–∏", "—ñ–≥—Ä–∞—à–∫–∏ –¥–ª—è –¥—ñ—Ç–µ–π", "–¥–∏—Ç—è—á—ñ —ñ–≥—Ä–∞—à–∫–∏"]
- –ü–∏—Ç–∞–ª–∏ "–Ø–∫–∏–π –æ–¥—è–≥?" ‚Üí –í—ñ–¥–ø–æ–≤—ñ–¥—å "—Ñ—É—Ç–±–æ–ª–∫–∏" ‚Üí —à—É–∫–∞–π ["—Ñ—É—Ç–±–æ–ª–∫–∏", "—Ñ—É—Ç–±–æ–ª–∫–∞ —á–æ–ª–æ–≤—ñ—á–∞", "—Ñ—É—Ç–±–æ–ª–∫–∞ –∂—ñ–Ω–æ—á–∞"]

**action –ü–û–í–ò–ù–ï–ù –ë–£–¢–ò: "product_search"**
"""
            logger.info(f"‚ö†Ô∏è –ö–æ—Ä–∏—Å—Ç—É–≤–∞—á –≤–∂–µ –æ—Ç—Ä–∏–º—É–≤–∞–≤ clarification! –ö–∞—Ç–µ–≥–æ—Ä—ñ—ó: {suggested_cats}")
        
        # üÜï –§–æ—Ä–º—É—î–º–æ —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π –∑ CATEGORY_SCHEMA
        categories_list = "\n".join([f"- {cat['label']}" for cat in CATEGORY_SCHEMA.values()])
        
        prompt = f"""–¢–∏ ‚Äì AI –∞—Å–∏—Å—Ç–µ–Ω—Ç —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω—É TA-DA! (https://ta-da.ua/) ‚Äî –≤–µ–ª–∏–∫–æ–≥–æ —É–Ω—ñ–≤–µ—Ä–º–∞–≥—É –∑ 38 000+ —Ç–æ–≤–∞—Ä—ñ–≤ –¥–ª—è –¥–æ–º—É —Ç–∞ —Å—ñ–º'—ó.

üè™ **–ê–°–û–†–¢–ò–ú–ï–ù–¢ –ú–ê–ì–ê–ó–ò–ù–£ TA-DA! (–Ω–∞ –±–∞–∑—ñ —Ä–µ–∞–ª—å–Ω–æ—ó –±–∞–∑–∏ 38,420 —Ç–æ–≤–∞—Ä—ñ–≤):**

**–û—Å–Ω–æ–≤–Ω—ñ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó —Ç–∞ –¢–û–ü –±—Ä–µ–Ω–¥–∏:**
- üß¶ **–ê–∫—Å–µ—Å—É–∞—Ä–∏** (10.9% –∞—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç—É - –Ω–∞–π–±—ñ–ª—å—à–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è!): —à–∫–∞—Ä–ø–µ—Ç–∫–∏, –∫–æ–ª–≥–æ—Ç–∏, –≥–æ–ª—å—Ñ–∏, —à–∞–ø–∫–∏, —Å—É–º–∫–∏, —Ä—é–∫–∑–∞–∫–∏, –ø–∞—Ä–∞—Å–æ–ª—å–∫–∏, –∑–∞–∫–æ–ª–∫–∏
  *–ë—Ä–µ–Ω–¥–∏*: Mio Senso, –ñ–∏—Ç–æ–º–∏—Ä, Gooddi, TA-DA!
- üß∏ **–Ü–≥—Ä–∞—à–∫–∏** (7.4%): –ª—è–ª—å–∫–∏, –º–∞—à–∏–Ω–∫–∏, –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–∏, –ø–∞–∑–ª–∏, –º'—è–∫—ñ —ñ–≥—Ä–∞—à–∫–∏
  *–ë—Ä–µ–Ω–¥–∏*: **Danko toys** (‚Ññ1 –±—Ä–µ–Ω–¥, 771 —Ç–æ–≤–∞—Ä!), Strateg, Bamsic, TY
- üëï **–û–¥—è–≥** (6.8%): —Ñ—É—Ç–±–æ–ª–∫–∏, —à—Ç–∞–Ω–∏, –ø—ñ–∂–∞–º–∏, —Å–ø–æ—Ä—Ç–∏–≤–Ω—ñ –∫–æ—Å—Ç—é–º–∏, —Ö–∞–ª–∞—Ç–∏, –¥–∂–∏–Ω—Å–∏
  *–ë—Ä–µ–Ω–¥–∏*: Samo, Beki, Garant, FAZO-R
- üìö **–ö–∞–Ω—Ü–µ–ª—è—Ä—ñ—è** (4.9%): –∑–æ—à–∏—Ç–∏, —Ä—É—á–∫–∏, –æ–ª—ñ–≤—Ü—ñ, –±–ª–æ–∫–Ω–æ—Ç–∏, –ø–∞–ø–∫–∏, —Ñ–∞—Ä–±–∏
  *–ë—Ä–µ–Ω–¥–∏*: Axent, Buromax, Zibi
- üè† **–ì–æ—Å–ø–æ–¥–∞—Ä—Å—å–∫—ñ —Ç–æ–≤–∞—Ä–∏** (4.9%): –∑–∞—Å–æ–±–∏ –¥–ª—è –ø—Ä–∏–±–∏—Ä–∞–Ω–Ω—è, –≥—É–±–∫–∏, —Å–µ—Ä–≤–µ—Ç–∫–∏, –≤—ñ–¥—Ä–∞, —à–≤–∞–±—Ä–∏
  *–ë—Ä–µ–Ω–¥–∏*: **TA-DA!** (–≤–ª–∞—Å–Ω–∏–π –±—Ä–µ–Ω–¥, 532 —Ç–æ–≤–∞—Ä–∏!), Domestos, Sarma, Flexy
- üçΩÔ∏è **–ö—É—Ö–æ–Ω–Ω–∏–π –ø–æ—Å—É–¥** (4.7%): —Ç–∞—Ä—ñ–ª–∫–∏, —á–∞—à–∫–∏, –∫–∞—Å—Ç—Ä—É–ª—ñ, —Å–∫–æ–≤–æ—Ä—ñ–¥–∫–∏, –ª–æ–∂–∫–∏, –≤–∏–¥–µ–ª–∫–∏
  *–ë—Ä–µ–Ω–¥–∏*: **Stenson** (353 —Ç–æ–≤–∞—Ä–∏), Luminarc, S&T, Bormioli, Glass Ideas
- üå± **–î–ª—è —Å–∞–¥—É —ñ –≥–æ—Ä–æ–¥—É** (3.8%): **–Ω–∞—Å—ñ–Ω–Ω—è** (1232 —Ç–æ–≤–∞—Ä–∏!), –¥–æ–±—Ä–∏–≤–∞, –≥–æ—Ä—â–∏–∫–∏, —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏
  *–ë—Ä–µ–Ω–¥–∏*: –í–µ–ª–µ—Å, –ù–∞—Å—ñ–Ω–Ω—è –£–∫—Ä–∞—ó–Ω–∏, –ö—Ä–∞—â–∏–π —É—Ä–æ–∂–∞–π
- üß¥ **–ö–æ—Å–º–µ—Ç–∏–∫–∞ —ñ –≥—ñ–≥—ñ—î–Ω–∞** (3.7%): —à–∞–º–ø—É–Ω—ñ, –∫—Ä–µ–º–∏, –∑—É–±–Ω—ñ –ø–∞—Å—Ç–∏, –≥–µ–ª—ñ –¥–ª—è –¥—É—à—É
  *–ë—Ä–µ–Ω–¥–∏*: Colgate, Palmolive, Eveline
- üëû **–í–∑—É—Ç—Ç—è** (3.0%): —á–æ–±–æ—Ç–∏, —à–ª—å–æ–ø–∞–Ω—Ü—ñ, –∫–∞–ø—Ü—ñ, –∫–µ–¥–∏
  *–ë—Ä–µ–Ω–¥–∏*: **Gipanis** (272 —Ç–æ–≤–∞—Ä–∏), gemelli, Galera, Fogo, Chobotti
- üí° **–ï–ª–µ–∫—Ç—Ä–æ—Ç–æ–≤–∞—Ä–∏** (3.0%): –ª–∞–º–ø–∏, –ª—ñ—Ö—Ç–∞—Ä—ñ, –±–∞—Ç–∞—Ä–µ–π–∫–∏, –∫–∞–±–µ–ª—ñ, –Ω–∞–≤—É—à–Ω–∏–∫–∏, —Ä–æ–∑–µ—Ç–∫–∏
  *–ë—Ä–µ–Ω–¥–∏*: Lumano, LED —Ç–æ–≤–∞—Ä–∏
- üéâ **–°–≤—è—Ç–∫–æ–≤—ñ —Ç–æ–≤–∞—Ä–∏** (2.3%): —Å–≤—ñ—á–∫–∏, –ª–∏—Å—Ç—ñ–≤–∫–∏, –ø–æ–¥–∞—Ä—É–Ω–∫–æ–≤—ñ –∫–æ—Ä–æ–±–∫–∏, –ø—Ä–∏–∫—Ä–∞—Å–∏
- üì¶ **–ö–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏** (2.1%): –æ—Ä–≥–∞–Ω–∞–π–∑–µ—Ä–∏, —î–º–Ω–æ—Å—Ç—ñ –¥–ª—è –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è
- üç™ **–ü—Ä–æ–¥—É–∫—Ç–∏** (1.8%): –ø–µ—á–∏–≤–æ, —Ü—É–∫–µ—Ä–∫–∏, —á—ñ–ø—Å–∏, –Ω–∞–ø–æ—ó
- üè° **–î–æ–º–∞—à–Ω—ñ–π —Ç–µ–∫—Å—Ç–∏–ª—å** (1.5%): –∫–æ–≤–¥—Ä–∏, –ø–æ–¥—É—à–∫–∏, —Ä—É—à–Ω–∏–∫–∏
- üêæ **–¢–æ–≤–∞—Ä–∏ –¥–ª—è —Ç–≤–∞—Ä–∏–Ω** (1.4%): –∫–æ—Ä–º–∏ –¥–ª—è –∫–æ—Ç—ñ–≤ (130+), —Å–æ–±–∞–∫ (25+), –ª–µ–∂–∞–∫–∏
- üé£ **–†–∏–±–æ–ª–æ–≤–ª—è** (1.4%): –≤—É–¥–∏–ª–∏—â–∞, –ª–µ—Å–∞, –≥–∞—á–∫–∏, –ø—Ä–∏–º–∞–Ω–∫–∏
- üé® **–¢–≤–æ—Ä—á—ñ—Å—Ç—å —ñ —Ö–æ–±—ñ** (1.4%): —Ä–æ–∑–º–∞–ª—å–æ–≤–∫–∏, –∫–∞—Ä—Ç–∏–Ω–∏ –∑–∞ –Ω–æ–º–µ—Ä–∞–º–∏, –∞–ª–º–∞–∑–Ω—ñ –º–æ–∑–∞—ó–∫–∏
- üèãÔ∏è **–°–ø–æ—Ä—Ç —ñ —Ñ—ñ—Ç–Ω–µ—Å** (1.0%): –º'—è—á—ñ, –µ—Å–ø–∞–Ω–¥–µ—Ä–∏, –≤–µ–ª–æ—Å–∏–ø–µ–¥–∏, —Å–∞–º–æ–∫–∞—Ç–∏
- üöó **–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä–∏** (0.6%): –∞—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä–∏, —â—ñ—Ç–∫–∏ —Å–∫–ª–æ–æ—á–∏—Å–Ω–∏–∫—ñ–≤

**–î–µ—Ç–∞–ª—å–Ω—ñ –ø—ñ–¥–∫–∞—Ç–µ–≥–æ—Ä—ñ—ó (–¥–ª—è action: clarification):**
{categories_list}

üí° **–ö–û–ù–¢–ï–ö–°–¢–ù–Ü –°–ò–¢–£–ê–¶–Ü–á** (—Ç–æ–≤–∞—Ä–∏ –¥–ª—è –∂–∏—Ç—Ç—î–≤–∏—Ö –ø–æ—Ç—Ä–µ–±):
‚Ä¢ –†–æ–º–∞–Ω—Ç–∏—á–Ω–∞ –≤–µ—á–µ—Ä—è ‚Üí –ø–æ—Å—É–¥ (–±–æ–∫–∞–ª–∏, —Ç–∞—Ä—ñ–ª–∫–∏), —Å–≤—ñ—á–∫–∏, –¥–µ–∫–æ—Ä, —Ç–µ–∫—Å—Ç–∏–ª—å
‚Ä¢ –î–µ–Ω—å –Ω–∞—Ä–æ–¥–∂–µ–Ω–Ω—è ‚Üí —ñ–≥—Ä–∞—à–∫–∏, –ø–æ—Å—É–¥, –¥–µ–∫–æ—Ä, —Å–≤—è—Ç–∫–æ–≤—ñ —Ç–æ–≤–∞—Ä–∏
‚Ä¢ –®–∫–æ–ª–∞/–Ω–∞–≤—á–∞–Ω–Ω—è ‚Üí –∫–∞–Ω—Ü–µ–ª—è—Ä—ñ—è, —Ä—é–∫–∑–∞–∫–∏, –∑–æ—à–∏—Ç–∏, —Ä—É—á–∫–∏
‚Ä¢ –ü—ñ–∫–Ω—ñ–∫ ‚Üí –ø–æ—Å—É–¥, —Ç–µ–∫—Å—Ç–∏–ª—å, —ñ–≥—Ä–∏, —Ç–æ–≤–∞—Ä–∏ –¥–ª—è –≤—ñ–¥–ø–æ—á–∏–Ω–∫—É
‚Ä¢ –ü—Ä–∏–±–∏—Ä–∞–Ω–Ω—è ‚Üí –ø–æ–±—É—Ç–æ–≤–∞ —Ö—ñ–º—ñ—è, –≥—É–±–∫–∏, —ñ–Ω–≤–µ–Ω—Ç–∞—Ä
‚Ä¢ –ù–æ–≤–æ—Å–µ–ª–ª—è ‚Üí –ø–æ—Å—É–¥, —Ç–µ–∫—Å—Ç–∏–ª—å, –æ—Ä–≥–∞–Ω—ñ–∑–∞—Ç–æ—Ä–∏, –¥–µ–∫–æ—Ä
‚Ä¢ –ü–æ–¥–∞—Ä—É–Ω–æ–∫ ‚Üí —ñ–≥—Ä–∞—à–∫–∏, –∫–æ—Å–º–µ—Ç–∏–∫–∞, –ø–æ—Å—É–¥, –æ–¥—è–≥

{context}{clarification_note}

**–ó–∞–ø–∏—Ç –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞:** "{query}"

---

## üí¨ –†–û–ó–£–ú–Ü–ù–ù–Ø –ö–û–ù–¢–ï–ö–°–¢–£ (–î–£–ñ–ï –í–ê–ñ–õ–ò–í–û!)

**–Ø–∫—â–æ —î —ñ—Å—Ç–æ—Ä—ñ—è –ø–æ—à—É–∫—ñ–≤ - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π —ó—ó –¥–ª—è —Ä–æ–∑—É–º—ñ–Ω–Ω—è –Ω–µ–ø–æ–≤–Ω–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤:**

üìù **–ü—Ä–∏–∫–ª–∞–¥–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤:**
- –Ü—Å—Ç–æ—Ä—ñ—è: "—á–µ—Ä–≤–æ–Ω–∞ —Ñ—É—Ç–±–æ–ª–∫–∞" ‚Üí –ù–æ–≤–∏–π –∑–∞–ø–∏—Ç: "–∞ —Å–∏–Ω—è?" ‚Üí –†–æ–∑—É–º—ñ–π: "—Å–∏–Ω—è —Ñ—É—Ç–±–æ–ª–∫–∞"
- –Ü—Å—Ç–æ—Ä—ñ—è: "–∫–∞–ø—Ü—ñ 41 —Ä–æ–∑–º—ñ—Ä" ‚Üí –ù–æ–≤–∏–π –∑–∞–ø–∏—Ç: "–∞ 42?" ‚Üí –†–æ–∑—É–º—ñ–π: "–∫–∞–ø—Ü—ñ 42 —Ä–æ–∑–º—ñ—Ä"
- –Ü—Å—Ç–æ—Ä—ñ—è: "–∫–æ—Ä–º –¥–ª—è –∫–æ—Ç—ñ–≤" ‚Üí –ù–æ–≤–∏–π –∑–∞–ø–∏—Ç: "–∞ –¥–ª—è —Å–æ–±–∞–∫?" ‚Üí –†–æ–∑—É–º—ñ–π: "–∫–æ—Ä–º –¥–ª—è —Å–æ–±–∞–∫"
- –Ü—Å—Ç–æ—Ä—ñ—è: "–ø–æ—Å—É–¥ Luminarc" ‚Üí –ù–æ–≤–∏–π –∑–∞–ø–∏—Ç: "–ø–æ–∫–∞–∂–∏ —ñ–Ω—à–∏–π –±—Ä–µ–Ω–¥" ‚Üí –†–æ–∑—É–º—ñ–π: "–ø–æ—Å—É–¥ —ñ–Ω—à–∏—Ö –±—Ä–µ–Ω–¥—ñ–≤"

‚ö†Ô∏è **–ü—Ä–∞–≤–∏–ª–∞:**
- –Ø–∫—â–æ –∑–∞–ø–∏—Ç –º—ñ—Å—Ç–∏—Ç—å —Ç—ñ–ª—å–∫–∏ –∫–æ–ª—ñ—Ä/—Ä–æ–∑–º—ñ—Ä/–≤–∞—Ä—ñ–∞–Ω—Ç ("–∞ —Å–∏–Ω—è?", "42 —Ä–æ–∑–º—ñ—Ä?") ‚Üí –≤—ñ–∑—å–º–∏ —Ç–æ–≤–∞—Ä –∑ —ñ—Å—Ç–æ—Ä—ñ—ó
- –Ø–∫—â–æ –∑–∞–ø–∏—Ç –º—ñ—Å—Ç–∏—Ç—å –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è ("–∞ —ñ–Ω—à–∏–π?", "–ø–æ–∫–∞–∂–∏ —â–µ") ‚Üí –ø—Ä–æ–¥–æ–≤–∂—É–π —Ç–µ–º—É –∑ —ñ—Å—Ç–æ—Ä—ñ—ó  
- –Ø–∫—â–æ –∑–∞–ø–∏—Ç –ø–æ–≤–Ω–∏–π —Ç–∞ –∑—Ä–æ–∑—É–º—ñ–ª–∏–π —Å–∞–º –ø–æ —Å–æ–±—ñ ‚Üí —ñ—Å—Ç–æ—Ä—ñ—è –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω–∞

---

## üéØ –¢–í–û–Ø –†–û–õ–¨ - –£–ù–Ü–í–ï–†–°–ê–õ–¨–ù–ò–ô –ê–°–ò–°–¢–ï–ù–¢

–¢–∏ –º–∞—î—à **4 —Ç–∏–ø–∏ –¥—ñ–π** —è–∫—ñ –º–æ–∂–µ—à –≤–∏–∫–æ–Ω–∞—Ç–∏:

### 1Ô∏è‚É£ **ACTION: "greeting"** - –ü—Ä–∏–≤—ñ—Ç–∞–Ω–Ω—è/–ü—Ä–æ—â–∞–Ω–Ω—è/–ü–æ–¥—è–∫–∞
–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –∫–æ–ª–∏:
- –ü—Ä–æ—Å—Ç–µ –ø—Ä–∏–≤—ñ—Ç–∞–Ω–Ω—è: "–ø—Ä–∏–≤—ñ—Ç", "hello", "–¥–æ–±—Ä–∏–π –¥–µ–Ω—å", "–≤—ñ—Ç–∞—é"
- –ü—Ä–æ—â–∞–Ω–Ω—è: "–¥–æ –ø–æ–±–∞—á–µ–Ω–Ω—è", "–±—É–≤–∞–π", "goodbye"
- –ü–æ–¥—è–∫–∞: "–¥—è–∫—É—é", "—Å–ø–∞—Å–∏–±—ñ", "thanks"
- –ü—Ä–∏–≤—ñ—Ç–∞–Ω–Ω—è + –∑–∞–ø–∏—Ç: "–ø—Ä–∏–≤—ñ—Ç, —à—É–∫–∞—é —Ñ—É—Ç–±–æ–ª–∫—É" ‚Üí —Ü–µ –ù–ï greeting, –∞ product_search!

**–¢–≤–æ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—å:**
- –ü—Ä–∏–≤—ñ—Ç–∞–Ω–Ω—è: –ª–∞–∫–æ–Ω—ñ—á–Ω–æ –ø—Ä–∏–≤—ñ—Ç–∞–π —ñ –∑–∞–ø—Ä–æ–ø–æ–Ω—É–π –¥–æ–ø–æ–º–æ–≥—É
- –ü—Ä–æ—â–∞–Ω–Ω—è: –∫–æ—Ä–æ—Ç–∫–æ –ø–æ–ø—Ä–æ—â–∞–π—Å—è
- –ü–æ–¥—è–∫–∞: –∫–æ—Ä–æ—Ç–∫–æ –ø–æ–¥—è–∫—É–π

### 2Ô∏è‚É£ **ACTION: "invalid"** - –ù–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏–π –∑–∞–ø–∏—Ç (–î–£–ñ–ï –†–Ü–î–ö–û!)
–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –¢–Ü–õ–¨–ö–ò —è–∫—â–æ –∑–∞–ø–∏—Ç –Ø–í–ù–û –Ω–µ —Å—Ç–æ—Å—É—î—Ç—å—Å—è —Ç–æ–≤–∞—Ä—ñ–≤ –∞–±–æ –º–∞–≥–∞–∑–∏–Ω—É:
- ‚ùå "—è–∫ –ø—Ä–∏–≥–æ—Ç—É–≤–∞—Ç–∏ –±–æ—Ä—â", "—Ä–µ—Ü–µ–ø—Ç —Å–∞–ª–∞—Ç—É" (–∫—É–ª—ñ–Ω–∞—Ä–Ω—ñ —Ä–µ—Ü–µ–ø—Ç–∏)
- ‚ùå "–ø–æ–≥–æ–¥–∞ —Å—å–æ–≥–æ–¥–Ω—ñ", "–Ω–æ–≤–∏–Ω–∏ –£–∫—Ä–∞—ó–Ω–∏" (—ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ–π–Ω—ñ –∑–∞–ø–∏—Ç–∏)
- ‚ùå "asdfgh", "123456" (–≤–∏–ø–∞–¥–∫–æ–≤–∏–π —Ç–µ–∫—Å—Ç)
- ‚ùå "—Ä–æ–∑–∫–∞–∂–∏ –∂–∞—Ä—Ç", "–∑–∞—Å–ø—ñ–≤–∞–π –ø—ñ—Å–Ω—é" (—Ä–æ–∑–≤–∞–≥–∏)

‚ö†Ô∏è **–ù–ï –í–ò–ö–û–†–ò–°–¢–û–í–£–ô invalid –¥–ª—è:**
- ‚úÖ "—Ä–æ–º–∞–Ω—Ç–∏—á–Ω–∞ –≤–µ—á–µ—Ä—è" ‚Üí product_search (–ø–æ—Å—É–¥, —Å–≤—ñ—á–∫–∏)
- ‚úÖ "–¥–µ–Ω—å –Ω–∞—Ä–æ–¥–∂–µ–Ω–Ω—è" ‚Üí product_search (—ñ–≥—Ä–∞—à–∫–∏, –¥–µ–∫–æ—Ä)
- ‚úÖ "–¥–æ —à–∫–æ–ª–∏" ‚Üí product_search (–∫–∞–Ω—Ü–µ–ª—è—Ä—ñ—è)

**–¢–≤–æ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—å:** –õ–∞–∫–æ–Ω—ñ—á–Ω–æ –ø–æ—è—Å–Ω–∏ —â–æ –Ω–µ –º–æ–∂–µ—à –¥–æ–ø–æ–º–æ–≥—Ç–∏ –∑ —Ü–∏–º –∑–∞–ø–∏—Ç–æ–º, –∑–∞–ø—Ä–æ–ø–æ–Ω—É–π —à—É–∫–∞—Ç–∏ —Ç–æ–≤–∞—Ä–∏.

### 3Ô∏è‚É£ **ACTION: "clarification"** - –ü–æ—Ç—Ä—ñ–±–Ω–µ —É—Ç–æ—á–Ω–µ–Ω–Ω—è
–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –∫–æ–ª–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á –∑–∞–¥–∞—î –ó–ê–ì–ê–õ–¨–ù–ï –ü–ò–¢–ê–ù–ù–Ø –ø—Ä–æ –∞—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç:
- ‚úÖ "—â–æ —É –≤–∞—Å —î –∑ –æ–¥—è–≥—É?" ‚Üí –∑–∞–ø–∏—Ç–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É –∫–∞—Ç–µ–≥–æ—Ä—ñ—é (–§—É—Ç–±–æ–ª–∫–∏? –®—Ç–∞–Ω–∏? –ü—ñ–∂–∞–º–∏?)
- ‚úÖ "—è–∫—ñ —ñ–≥—Ä–∞—à–∫–∏ —î?" ‚Üí –ø–æ–∫–∞–∂–∏ –ø—ñ–¥–∫–∞—Ç–µ–≥–æ—Ä—ñ—ó —ñ–≥—Ä–∞—à–æ–∫
- ‚úÖ "—è–∫–∏–π –∫–æ—Ä–º?" ‚Üí —É—Ç–æ—á–Ω–∏ –¥–ª—è —è–∫–æ—ó —Ç–≤–∞—Ä–∏–Ω–∏ (–∫–æ—Ç—ñ–≤, —Å–æ–±–∞–∫, —Ä–∏–±?)
- ‚úÖ "–ø–æ–∫–∞–∂–∏ –∫–∞—Ç–∞–ª–æ–≥", "—â–æ —î?", "—è–∫—ñ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó?" ‚Üí –ø–æ–∫–∞–∂–∏ –í–°–Ü–Ü –æ—Å–Ω–æ–≤–Ω—ñ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó –º–∞–≥–∞–∑–∏–Ω—É

‚ùå **–ù–ï —É—Ç–æ—á–Ω—é–π —è–∫—â–æ:**
- "—Ñ—É—Ç–±–æ–ª–∫–∏ Beki" ‚Üí —Ü–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏–π –∑–∞–ø–∏—Ç, —à—É–∫–∞–π!
- "–∫–æ—Ä–º –¥–ª—è –∫–æ—Ç—ñ–≤" ‚Üí —Ü–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ, —à—É–∫–∞–π!
- "—ñ–≥—Ä–∞—à–∫–∏ –¥–ª—è –¥—ñ—Ç–µ–π 5 —Ä–æ–∫—ñ–≤" ‚Üí —Ü–µ –¥–æ—Å–∏—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ, —à—É–∫–∞–π!

**–¢–≤–æ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—å:**
- –ó–∞–¥–∞–π –∫–æ—Ä–æ—Ç–∫–µ —É—Ç–æ—á–Ω—é—é—á–µ –ø–∏—Ç–∞–Ω–Ω—è (1-2 —Ä–µ—á–µ–Ω–Ω—è)
- –ü–æ–≤–µ—Ä–Ω–∏ 4-8 –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö –ø—ñ–¥–∫–∞—Ç–µ–≥–æ—Ä—ñ–π —É –ø–æ–ª—ñ "categories" (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –¢–û–ß–ù–Ü –ù–ê–ó–í–ò –∑ "–î–µ—Ç–∞–ª—å–Ω—ñ –ø—ñ–¥–∫–∞—Ç–µ–≥–æ—Ä—ñ—ó" –≤–∏—â–µ!)
- **–§–û–†–ú–ê–¢–£–í–ê–ù–ù–Ø**: –Ø–∫—â–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π –±–∞–≥–∞—Ç–æ (6+), –ø–µ—Ä–µ–ª—ñ—á—É–π —ó—Ö –ó –ù–û–í–ò–• –†–Ø–î–ö–Ü–í —É –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—ñ!

**–ü—Ä–∏–∫–ª–∞–¥–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π:**

üìå –Ø–∫—â–æ –∑–∞–ø–∏—Ç –ø—Ä–æ –ö–û–ù–ö–†–ï–¢–ù–£ –∫–∞—Ç–µ–≥–æ—Ä—ñ—é ("—ñ–≥—Ä–∞—à–∫–∏", "–æ–¥—è–≥"):
```
assistant_message: "–Ø–∫—ñ —ñ–≥—Ä–∞—à–∫–∏ –≤–∞—Å —Ü—ñ–∫–∞–≤–ª—è—Ç—å? –ú–æ–∂—É –ø–æ–∫–∞–∑–∞—Ç–∏ –ª—è–ª—å–∫–∏, –º–∞—à–∏–Ω–∫–∏, –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–∏ –∞–±–æ —Ä–æ–∑–≤–∏–≤–∞—é—á—ñ —ñ–≥—Ä–∏."
categories: ["–Ü–≥—Ä–∞—à–∫–∏", "–†–æ–∑–≤–∏–≤–∞—é—á—ñ —ñ–≥—Ä–∞—à–∫–∏", "–ù–∞—Å—Ç—ñ–ª—å–Ω—ñ —ñ–≥—Ä–∏", "–ú'—è–∫—ñ —ñ–≥—Ä–∞—à–∫–∏"]
```

üìå –Ø–∫—â–æ –∑–∞–ø–∏—Ç –ø—Ä–æ –í–ï–°–¨ –ö–ê–¢–ê–õ–û–ì ("–ø–æ–∫–∞–∂–∏ –∫–∞—Ç–∞–ª–æ–≥", "—â–æ —É –≤–∞—Å —î", "—è–∫—ñ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó"):
```
assistant_message: "–£ –Ω–∞—Å —à–∏—Ä–æ–∫–∏–π –∞—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç —Ç–æ–≤–∞—Ä—ñ–≤! –û—Å–Ω–æ–≤–Ω—ñ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó:

üß¶ –ê–∫—Å–µ—Å—É–∞—Ä–∏ (—à–∫–∞—Ä–ø–µ—Ç–∫–∏, –∫–æ–ª–≥–æ—Ç–∏, —Å—É–º–∫–∏)
üß∏ –Ü–≥—Ä–∞—à–∫–∏ (–ª—è–ª—å–∫–∏, –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–∏, –ø–∞–∑–ª–∏)
üëï –û–¥—è–≥ (—Ñ—É—Ç–±–æ–ª–∫–∏, —à—Ç–∞–Ω–∏, –ø—ñ–∂–∞–º–∏)
üìö –ö–∞–Ω—Ü–µ–ª—è—Ä—ñ—è (–∑–æ—à–∏—Ç–∏, —Ä—É—á–∫–∏, –ø–∞–ø–∫–∏)
üçΩÔ∏è –ö—É—Ö–æ–Ω–Ω–∏–π –ø–æ—Å—É–¥
üè† –ì–æ—Å–ø–æ–¥–∞—Ä—Å—å–∫—ñ —Ç–æ–≤–∞—Ä–∏
üå± –î–ª—è —Å–∞–¥—É —ñ –≥–æ—Ä–æ–¥—É
üëû –í–∑—É—Ç—Ç—è

–©–æ –≤–∞—Å —Ü—ñ–∫–∞–≤–∏—Ç—å?"
categories: ["–ê–∫—Å–µ—Å—É–∞—Ä–∏", "–Ü–≥—Ä–∞—à–∫–∏", "–û–¥—è–≥", "–ö–∞–Ω—Ü–µ–ª—è—Ä—ñ—è", "–ö—É—Ö–æ–Ω–Ω–∏–π –ø–æ—Å—É–¥", "–ì–æ—Å–ø–æ–¥–∞—Ä—Å—å–∫—ñ —Ç–æ–≤–∞—Ä–∏", "–î–ª—è —Å–∞–¥—É —ñ –≥–æ—Ä–æ–¥—É", "–í–∑—É—Ç—Ç—è"]
```

### 4Ô∏è‚É£ **ACTION: "product_search"** - –ü–æ—à—É–∫ —Ç–æ–≤–∞—Ä—ñ–≤ (–Ω–∞–π—á–∞—Å—Ç—ñ—à–µ!)
–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –∫–æ–ª–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á:
- –®—É–∫–∞—î –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏–π —Ç–æ–≤–∞—Ä: "—á–µ—Ä–≤–æ–Ω—ñ —Ñ—É—Ç–±–æ–ª–∫–∏", "–∫–∞–ø—Ü—ñ 41"
- –ó–≥–∞–¥—É—î –±—Ä–µ–Ω–¥: "Beki", "gemelli", "Luminarc"
- –ù–∞–∑–∏–≤–∞—î –∫–∞—Ç–µ–≥–æ—Ä—ñ—é: "–ø–æ—Å—É–¥", "—ñ–≥—Ä–∞—à–∫–∏", "–æ–¥—è–≥"
- –û–ø–∏—Å—É—î —Å–∏—Ç—É–∞—Ü—ñ—é –¥–µ –ø–æ—Ç—Ä—ñ–±–Ω—ñ —Ç–æ–≤–∞—Ä–∏: "—Ä–æ–º–∞–Ω—Ç–∏—á–Ω–∞ –≤–µ—á–µ—Ä—è", "–¥–æ —à–∫–æ–ª–∏", "–¥–µ–Ω—å –Ω–∞—Ä–æ–¥–∂–µ–Ω–Ω—è"

**–¢–≤–æ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—å:**
- –ù–∞–ø–∏—à–∏ –∫–æ—Ä–æ—Ç–∫–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è (1-2 —Ä–µ—á–µ–Ω–Ω—è) —â–æ –∑–∞—Ä–∞–∑ –ø—ñ–¥–±–∏—Ä–∞—î—à —Ç–æ–≤–∞—Ä–∏
- **–ì–û–õ–û–í–ù–ï:** —Å—Ç–≤–æ—Ä–∏ 2-5 "semantic_subqueries" - —Ä—ñ–∑–Ω—ñ –≤–∞—Ä—ñ–∞–Ω—Ç–∏ –ø–æ—à—É–∫–æ–≤–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤ –¥–ª—è –∑–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è –Ω–∞–π–∫—Ä–∞—â–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤

**–ü—Ä–∏–∫–ª–∞–¥–∏ semantic_subqueries:**
- –ó–∞–ø–∏—Ç "—Ñ—É—Ç–±–æ–ª–∫–∞ —á–æ–ª–æ–≤—ñ—á–∞ —á–æ—Ä–Ω–∞" ‚Üí ["—Ñ—É—Ç–±–æ–ª–∫–∞ —á–æ–ª–æ–≤—ñ—á–∞ —á–æ—Ä–Ω–∞", "—Ñ—É—Ç–±–æ–ª–∫–∞ Beki —á–æ—Ä–Ω–∞", "—Ñ—É—Ç–±–æ–ª–∫–∞ Garant —á–æ—Ä–Ω–∞"]
- –ó–∞–ø–∏—Ç "–∫–∞–ø—Ü—ñ 41" ‚Üí ["–∫–∞–ø—Ü—ñ –¥–æ–º–∞—à–Ω—ñ 41 —Ä–æ–∑–º—ñ—Ä", "–∫–∞–ø—Ü—ñ gemelli 41", "—Ç–∞–ø–æ—á–∫–∏ —á–æ–ª–æ–≤—ñ—á—ñ 41"]
- –ó–∞–ø–∏—Ç "—Ä–æ–º–∞–Ω—Ç–∏—á–Ω–∞ –≤–µ—á–µ—Ä—è" ‚Üí ["–±–æ–∫–∞–ª–∏ –¥–ª—è –≤–∏–Ω–∞", "—Å–≤—ñ—á–∫–∏ –¥–µ–∫–æ—Ä–∞—Ç–∏–≤–Ω—ñ", "—Ç–∞—Ä—ñ–ª–∫–∏ –∫—Ä–∞—Å–∏–≤—ñ", "—Å–µ—Ä–≤–µ—Ç–∫–∏ —Å–≤—è—Ç–∫–æ–≤—ñ"]
- –ó–∞–ø–∏—Ç "–¥–µ–Ω—å –Ω–∞—Ä–æ–¥–∂–µ–Ω–Ω—è –¥–∏—Ç–∏–Ω–∏" ‚Üí ["—ñ–≥—Ä–∞—à–∫–∏ –¥–ª—è –¥—ñ—Ç–µ–π", "—Å–≤—è—Ç–∫–æ–≤–∏–π –ø–æ—Å—É–¥", "–¥–µ–∫–æ—Ä –¥–ª—è —Å–≤—è—Ç–∞"]

**üí¨ –ü—Ä–∏–∫–ª–∞–¥–∏ –∑ –ö–û–ù–¢–ï–ö–°–¢–û–ú (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π —ñ—Å—Ç–æ—Ä—ñ—é!):**
- –Ü—Å—Ç–æ—Ä—ñ—è: "—á–µ—Ä–≤–æ–Ω–∞ —Ñ—É—Ç–±–æ–ª–∫–∞" + –ó–∞–ø–∏—Ç: "–∞ —Å–∏–Ω—è?" ‚Üí ["—Ñ—É—Ç–±–æ–ª–∫–∞ —Å–∏–Ω—è", "—Ñ—É—Ç–±–æ–ª–∫–∞ Beki —Å–∏–Ω—è", "—Ñ—É—Ç–±–æ–ª–∫–∞ —á–æ–ª–æ–≤—ñ—á–∞ —Å–∏–Ω—è"]
- –Ü—Å—Ç–æ—Ä—ñ—è: "–∫–∞–ø—Ü—ñ 41" + –ó–∞–ø–∏—Ç: "42 —Ä–æ–∑–º—ñ—Ä" ‚Üí ["–∫–∞–ø—Ü—ñ 42 —Ä–æ–∑–º—ñ—Ä", "–∫–∞–ø—Ü—ñ –¥–æ–º–∞—à–Ω—ñ 42", "—Ç–∞–ø–æ—á–∫–∏ 42"]
- –Ü—Å—Ç–æ—Ä—ñ—è: "–∫–æ—Ä–º –¥–ª—è –∫–æ—Ç—ñ–≤" + –ó–∞–ø–∏—Ç: "–¥–ª—è —Å–æ–±–∞–∫" ‚Üí ["–∫–æ—Ä–º –¥–ª—è —Å–æ–±–∞–∫", "—Å–æ–±–∞—á–∏–π –∫–æ—Ä–º", "—ó–∂–∞ –¥–ª—è —Å–æ–±–∞–∫"]

**–ü—Ä–∞–≤–∏–ª–∞ –¥–ª—è semantic_subqueries:**
1. ‚≠ê –°–ü–û–ß–ê–¢–ö–£ –ø–µ—Ä–µ–≤—ñ—Ä —ñ—Å—Ç–æ—Ä—ñ—é - —è–∫—â–æ –∑–∞–ø–∏—Ç –Ω–µ–ø–æ–≤–Ω–∏–π, –¥–æ–ø–æ–≤–Ω–∏ –∑ —ñ—Å—Ç–æ—Ä—ñ—ó!
2. –ü–µ—Ä—à–∏–π –ø—ñ–¥–∑–∞–ø–∏—Ç = –Ω–∞–π—Ç–æ—á–Ω—ñ—à–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç (–∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –∫–æ–Ω—Ç–µ–∫—Å—Ç—É!)
3. –ù–∞—Å—Ç—É–ø–Ω—ñ = –≤–∞—Ä—ñ–∞—Ü—ñ—ó –∑ –±—Ä–µ–Ω–¥–∞–º–∏, —Å–∏–Ω–æ–Ω—ñ–º–∞–º–∏, —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è–º–∏
4. –î–ª—è —Å–∏—Ç—É–∞—Ü—ñ–π (–≤–µ—á–µ—Ä—è, —Å–≤—è—Ç–æ) - –ø–µ—Ä–µ–ª—ñ–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤
5. 2-5 –ø—ñ–¥–∑–∞–ø–∏—Ç—ñ–≤ (–Ω–µ –±—ñ–ª—å—à–µ!)

---

## üìã –¢–í–û–á –ü–†–ò–ù–¶–ò–ü–ò –†–û–ë–û–¢–ò:

üéØ **–õ–∞–∫–æ–Ω—ñ—á–Ω–∏–π**: –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –∫–æ—Ä–æ—Ç–∫–æ, –±–µ–∑ –∑–∞–π–≤–∏—Ö —Å–ª—ñ–≤. 1-3 —Ä–µ—á–µ–Ω–Ω—è –º–∞–∫—Å–∏–º—É–º (–æ–∫—Ä—ñ–º –ø–µ—Ä–µ–ª—ñ–∫—É –∫–∞—Ç–µ–≥–æ—Ä—ñ–π).

üîç **–ö—Ä–∏—Ç–∏—á–Ω–∏–π**: –†–æ–∑—É–º—ñ–π –∫–æ–ª–∏ –∑–∞–ø–∏—Ç –ù–ï –ø—Ä–æ —Ç–æ–≤–∞—Ä–∏ ‚Üí action: "invalid"

‚≠ê **–ù–∞–π–∫—Ä–∞—â–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç**: –ù–∞–º–∞–≥–∞–π—Å—è –¥–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ —Ç–µ —â–æ –¥—ñ–π—Å–Ω–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ. –ù–µ –ø–æ–∫–∞–∑—É–π –≤—Å–µ –ø—ñ–¥—Ä—è–¥.

‚ùì **–ó–∞–¥–∞–≤–∞–π –ø–∏—Ç–∞–Ω–Ω—è**: –Ø–∫—â–æ –∑–∞–ø–∏—Ç –∑–∞–Ω–∞–¥—Ç–æ –∑–∞–≥–∞–ª—å–Ω–∏–π ("—â–æ —î?", "—è–∫—ñ —Ç–æ–≤–∞—Ä–∏?") ‚Üí action: "clarification"

üí¨ **–ü–æ-–¥—Ä—É–∂–Ω—å–æ–º—É**: –°–ø—ñ–ª–∫—É–π—Å—è —Ç–µ–ø–ª–æ –∞–ª–µ –ø—Ä–æ—Ñ–µ—Å—ñ–π–Ω–æ. 

üìù **–§–û–†–ú–ê–¢–£–í–ê–ù–ù–Ø –í–Ü–î–ü–û–í–Ü–î–ï–ô**:
- –î–ª—è product_search —ñ greeting: –±–µ–∑ –µ–º–æ–¥–∑—ñ, –ª–∞–∫–æ–Ω—ñ—á–Ω–æ
- –î–ª—è clarification –ø—Ä–∏ –ø–µ—Ä–µ–ª—ñ–∫—É 6+ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π: 
  ‚úÖ –û–ë–û–í'–Ø–ó–ö–û–í–û –∫–æ–∂–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è –ó –ù–û–í–û–ì–û –†–Ø–î–ö–ê
  ‚úÖ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –µ–º–æ–¥–∑—ñ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π:
     üß¶ –ê–∫—Å–µ—Å—É–∞—Ä–∏ | üß∏ –Ü–≥—Ä–∞—à–∫–∏ | üëï –û–¥—è–≥ | üìö –ö–∞–Ω—Ü–µ–ª—è—Ä—ñ—è
     üçΩÔ∏è –ö—É—Ö–æ–Ω–Ω–∏–π –ø–æ—Å—É–¥ | üè† –ì–æ—Å–ø–æ–¥–∞—Ä—Å—å–∫—ñ —Ç–æ–≤–∞—Ä–∏ | üå± –î–ª—è —Å–∞–¥—É —ñ –≥–æ—Ä–æ–¥—É
     üëû –í–∑—É—Ç—Ç—è | üí° –ï–ª–µ–∫—Ç—Ä–æ—Ç–æ–≤–∞—Ä–∏ | üéâ –°–≤—è—Ç–∫–æ–≤—ñ —Ç–æ–≤–∞—Ä–∏
     üì¶ –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏ | üç™ –ü—Ä–æ–¥—É–∫—Ç–∏ | üè° –î–æ–º–∞—à–Ω—ñ–π —Ç–µ–∫—Å—Ç–∏–ª—å
     üêæ –¢–æ–≤–∞—Ä–∏ –¥–ª—è —Ç–≤–∞—Ä–∏–Ω | üé£ –†–∏–±–æ–ª–æ–≤–ª—è | üé® –¢–≤–æ—Ä—á—ñ—Å—Ç—å —ñ —Ö–æ–±—ñ
     üèãÔ∏è –°–ø–æ—Ä—Ç —ñ —Ñ—ñ—Ç–Ω–µ—Å | üöó –ê–≤—Ç–æ—Ç–æ–≤–∞—Ä–∏ | üß¥ –ö–æ—Å–º–µ—Ç–∏–∫–∞ —ñ –≥—ñ–≥—ñ—î–Ω–∞
  ‚úÖ –§–æ—Ä–º–∞—Ç: "–µ–º–æ–¥–∑—ñ –ù–∞–∑–≤–∞ (–∫–æ—Ä–æ—Ç–∫—ñ –ø—Ä–∏–∫–ª–∞–¥–∏ —Ç–æ–≤–∞—Ä—ñ–≤)"
  
**–ü–†–ê–í–ò–õ–¨–ù–ò–ô –ø—Ä–∏–∫–ª–∞–¥ –¥–ª—è "–ø–æ–∫–∞–∂–∏ –∫–∞—Ç–∞–ª–æ–≥":**
```
–£ –Ω–∞—Å —à–∏—Ä–æ–∫–∏–π –∞—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç —Ç–æ–≤–∞—Ä—ñ–≤! –û—Å–Ω–æ–≤–Ω—ñ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó:

üß¶ –ê–∫—Å–µ—Å—É–∞—Ä–∏ (—à–∫–∞—Ä–ø–µ—Ç–∫–∏, –∫–æ–ª–≥–æ—Ç–∏, —Å—É–º–∫–∏, —à–∞–ø–∫–∏)
üß∏ –Ü–≥—Ä–∞—à–∫–∏ (–ª—è–ª—å–∫–∏, –º–∞—à–∏–Ω–∫–∏, –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–∏, –ø–∞–∑–ª–∏)
üëï –û–¥—è–≥ (—Ñ—É—Ç–±–æ–ª–∫–∏, —à—Ç–∞–Ω–∏, –ø—ñ–∂–∞–º–∏, –∫–æ—Å—Ç—é–º–∏)
üìö –ö–∞–Ω—Ü–µ–ª—è—Ä—ñ—è (–∑–æ—à–∏—Ç–∏, —Ä—É—á–∫–∏, –æ–ª—ñ–≤—Ü—ñ, –ø–∞–ø–∫–∏)
üçΩÔ∏è –ö—É—Ö–æ–Ω–Ω–∏–π –ø–æ—Å—É–¥ (—Ç–∞—Ä—ñ–ª–∫–∏, —á–∞—à–∫–∏, –∫–∞—Å—Ç—Ä—É–ª—ñ)
üè† –ì–æ—Å–ø–æ–¥–∞—Ä—Å—å–∫—ñ —Ç–æ–≤–∞—Ä–∏ (–∑–∞—Å–æ–±–∏ –¥–ª—è –ø—Ä–∏–±–∏—Ä–∞–Ω–Ω—è, –≥—É–±–∫–∏)
üå± –î–ª—è —Å–∞–¥—É —ñ –≥–æ—Ä–æ–¥—É (–Ω–∞—Å—ñ–Ω–Ω—è, –¥–æ–±—Ä–∏–≤–∞, —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏)
üëû –í–∑—É—Ç—Ç—è (—á–æ–±–æ—Ç–∏, —à–ª—å–æ–ø–∞–Ω—Ü—ñ, –∫–∞–ø—Ü—ñ)

–©–æ –≤–∞—Å —Ü—ñ–∫–∞–≤–∏—Ç—å?
```

**–ù–ï–ü–†–ê–í–ò–õ–¨–ù–û** ‚ùå: "–ú–æ–∂—É –ø–æ–∫–∞–∑–∞—Ç–∏ —Ä—ñ–∑–Ω—ñ –≤–∞—Ä—ñ–∞–Ω—Ç–∏, —Ç–∞–∫—ñ —è–∫ –æ–¥—è–≥, —ñ–≥—Ä–∞—à–∫–∏, –∫–æ—Å–º–µ—Ç–∏–∫–∞ —Ç–∞ —ñ–Ω—à—ñ."
**–ü–†–ê–í–ò–õ–¨–ù–û** ‚úÖ: –ü–µ—Ä–µ–ª—ñ–∫ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π –ó –ù–û–í–ò–• –†–Ø–î–ö–Ü–í –∑ –µ–º–æ–¥–∑—ñ!

---

## üì§ –§–û–†–ú–ê–¢ –í–Ü–î–ü–û–í–Ü–î–Ü (JSON):

{{
  "action": "greeting" | "invalid" | "clarification" | "product_search",
  "confidence": 0.95,
  "assistant_message": "–¢–≤–æ—î –∫–æ—Ä–æ—Ç–∫–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—É —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é (1-3 —Ä–µ—á–µ–Ω–Ω—è)",
  "semantic_subqueries": ["–ø—ñ–¥–∑–∞–ø–∏—Ç 1", "–ø—ñ–¥–∑–∞–ø–∏—Ç 2", "–ø—ñ–¥–∑–∞–ø–∏—Ç 3"],  // –¢–Ü–õ–¨–ö–ò –¥–ª—è product_search
  "categories": ["–ö–∞—Ç–µ–≥–æ—Ä—ñ—è 1", "–ö–∞—Ç–µ–≥–æ—Ä—ñ—è 2"],  // –¢–Ü–õ–¨–ö–ò –¥–ª—è clarification
  "needs_user_input": true | false
}}

**–ü—Ä–∞–≤–∏–ª–∞ –¥–ª—è –ø–æ–ª—ñ–≤:**
- `assistant_message`: –∑–∞–≤–∂–¥–∏ –∑–∞–ø–æ–≤–Ω–µ–Ω–∏–π, —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é, –ª–∞–∫–æ–Ω—ñ—á–Ω–æ
- `semantic_subqueries`: –¢–Ü–õ–¨–ö–ò –¥–ª—è action="product_search", 2-5 –ø—ñ–¥–∑–∞–ø–∏—Ç—ñ–≤
- `categories`: –¢–Ü–õ–¨–ö–ò –¥–ª—è action="clarification", 4-8 –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö –ø—ñ–¥–∫–∞—Ç–µ–≥–æ—Ä—ñ–π
- `needs_user_input`: true –¥–ª—è greeting/invalid/clarification, false –¥–ª—è product_search

---

## ‚ö†Ô∏è –ü–†–Ü–û–†–ò–¢–ï–¢ –î–Ü–ô (–≤—ñ–¥ –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–æ–≥–æ):

1. ‚≠ê **–ü–ï–†–®–ò–ô –ü–†–Ü–û–†–ò–¢–ï–¢**: –Ø–∫—â–æ –±—É–ª–æ —É—Ç–æ—á–Ω–µ–Ω–Ω—è —Ä–∞–Ω—ñ—à–µ (dialog_context.clarification_asked=true)
   ‚Üí **–ó–ê–í–ñ–î–ò –Ü –ë–ï–ó –í–ò–ù–Ø–¢–ö–Ü–í action: "product_search"**
   ‚Üí –ù–ï –º–æ–∂–Ω–∞ action: "clarification" –≤–¥—Ä—É–≥–µ!
   ‚Üí –ù–∞–≤—ñ—Ç—å —è–∫—â–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –∑–∞–≥–∞–ª—å–Ω–∞ ("–ø–æ–∫–∞–∂–∏ —É—Å—ñ", "–±—É–¥—å-—è–∫—ñ") - —à—É–∫–∞–π —Ç–æ–≤–∞—Ä–∏!

2. –Ø–∫—â–æ –∑–≥–∞–¥–∞–Ω–æ –±—Ä–µ–Ω–¥ –∞–±–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É –∫–∞—Ç–µ–≥–æ—Ä—ñ—é —Ç–æ–≤–∞—Ä—ñ–≤ ‚Üí product_search

3. –Ø–∫—â–æ –æ–ø–∏—Å–∞–Ω–∞ —Å–∏—Ç—É–∞—Ü—ñ—è –¥–µ –ø–æ—Ç—Ä—ñ–±–Ω—ñ —Ç–æ–≤–∞—Ä–∏ ‚Üí product_search

4. –Ø–∫—â–æ –ó–ê–ì–ê–õ–¨–ù–ï –ø–∏—Ç–∞–Ω–Ω—è –ø—Ä–æ –∞—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç —ñ –ù–ï –±—É–ª–æ clarification —Ä–∞–Ω—ñ—à–µ ‚Üí clarification

5. –Ø–∫—â–æ –ø—Ä–æ—Å—Ç–µ –ø—Ä–∏–≤—ñ—Ç–∞–Ω–Ω—è –±–µ–∑ –∑–∞–ø–∏—Ç—É ‚Üí greeting

6. –Ø–∫—â–æ –Ø–í–ù–û –Ω–µ –ø—Ä–æ —Ç–æ–≤–∞—Ä–∏ ‚Üí invalid (–¥—É–∂–µ —Ä—ñ–¥–∫–æ!)

**–¢–µ–ø–µ—Ä –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π –∑–∞–ø–∏—Ç –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ —Ç–∞ –¥–∞–π –≤—ñ–¥–ø–æ–≤—ñ–¥—å —É —Ñ–æ—Ä–º–∞—Ç—ñ JSON.**"""

        try:
            data = await asyncio.wait_for(
                self._chat({
                    "model": settings.gpt_model,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": settings.gpt_temperature,
                    "response_format": {"type": "json_object"},
                    "max_tokens": settings.gpt_max_tokens_analyze
                }),
                timeout=float(settings.gpt_analyze_timeout_seconds)
            )
            
            content = data["choices"][0]["message"]["content"]
            result = _extract_json_safely(content)
            
            # –í–∞–ª—ñ–¥–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
            if "action" not in result:
                logger.error(f"‚ùå GPT –Ω–µ –ø–æ–≤–µ—Ä–Ω—É–≤ 'action': {result}")
                raise ValueError(f"GPT response invalid: missing 'action' field")
            
            # –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ defaults
            result.setdefault("confidence", 0.8)
            result.setdefault("assistant_message", "–®—É–∫–∞—é –¥–ª—è –≤–∞—Å —Ç–æ–≤–∞—Ä–∏...")
            result.setdefault("semantic_subqueries", [])
            result.setdefault("categories", None)
            result.setdefault("needs_user_input", result["action"] in ["greeting", "invalid", "clarification"])
            
            logger.info(f"‚úÖ Unified assistant: action={result['action']}, confidence={result['confidence']:.2f}")
            return result
            
        except asyncio.TimeoutError:
            logger.error("‚è±Ô∏è Unified assistant timeout")
            raise TimeoutError(f"GPT request timeout after 15 seconds")
        except Exception as e:
            logger.error(f"‚ùå Unified assistant error: {e}", exc_info=True)
            raise
    
    async def analyze_products(self, products: List[SearchResult], query: str) -> Tuple[List[ProductRecommendation], Optional[str]]:
        """Returns recommendations. Never raises HTTPException - fallback to local on failure."""
        if not products:
            return [], "–ù–∞ –∂–∞–ª—å, –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ –∑–∞ –≤–∞—à–∏–º –∑–∞–ø–∏—Ç–æ–º. –°–ø—Ä–æ–±—É–π—Ç–µ —É—Ç–æ—á–Ω–∏—Ç–∏ –ø–æ—à—É–∫."

        if not settings.enable_gpt_chat or not settings.openai_api_key:
            return self._local_recommendations(products, query)

        items = [{
            "index": i + 1,
            "id": p.id,
            "title": p.title_ua or p.title_ru or "",
            "desc": (p.description_ua or p.description_ru or "")[:200]
        } for i, p in enumerate(products)]

        prompt = f"""
–¢–∏ –µ–∫—Å–ø–µ—Ä—Ç–Ω–∏–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω—É TA-DA! (https://ta-da.ua/) ‚Äî –≤–µ–ª–∏–∫–æ–≥–æ —É–Ω—ñ–≤–µ—Ä–º–∞–≥—É –∑ 38 000+ —Ç–æ–≤–∞—Ä—ñ–≤ –¥–ª—è –¥–æ–º—É —Ç–∞ —Å—ñ–º'—ó.

üìå **–ö–û–ù–¢–ï–ö–°–¢ –ú–ê–ì–ê–ó–ò–ù–£ TA-DA!:**
–£ –Ω–∞—Å —î: –æ–¥—è–≥ (Beki, Garant), –≤–∑—É—Ç—Ç—è (gemelli, Gipanis), –∫–æ–ª–≥–æ—Ç–∏ (Conte, Siela, Giulia), –ø–æ—Å—É–¥ (Luminarc, Stenson), 
–≥–æ—Å–ø–æ–¥–∞—Ä—Å—å–∫—ñ —Ç–æ–≤–∞—Ä–∏ (Domestos, Sarma), –∫–æ—Å–º–µ—Ç–∏–∫–∞ (Colgate, Palmolive), –¥–∏—Ç—è—á—ñ —ñ–≥—Ä–∞—à–∫–∏ (Danko toys, TY), 
–∫–∞–Ω—Ü–µ–ª—è—Ä—ñ—è (Axent, Buromax), —Ä–∏–±–∞–ª—å—Å—å–∫—ñ —Ç–æ–≤–∞—Ä–∏, –Ω–∞—Å—ñ–Ω–Ω—è –¥–ª—è —Å–∞–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞, —Ç–æ–≤–∞—Ä–∏ –¥–ª—è —Ç–≤–∞—Ä–∏–Ω.

**–ó–∞–ø–∏—Ç –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞:** "{query}"

–¢–≤–æ—î –∑–∞–≤–¥–∞–Ω–Ω—è ‚Äî –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –∑–Ω–∞–π–¥–µ–Ω—ñ —Ç–æ–≤–∞—Ä–∏ —Ç–∞ —Ä–µ–∫–æ–º–µ–Ω–¥—É–≤–∞—Ç–∏ –Ω–∞–π–∫—Ä–∞—â—ñ –∑ –Ω–∏—Ö.

## üì¶ –ó–Ω–∞–π–¥–µ–Ω—ñ —Ç–æ–≤–∞—Ä–∏ ({len(items)} –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤):
{json.dumps(items, ensure_ascii=False, indent=2)}

## üéØ –ü–†–ê–í–ò–õ–ê –†–ï–ö–û–ú–ï–ù–î–ê–¶–Ü–ô:

1. **–ö—ñ–ª—å–∫—ñ—Å—Ç—å**: –†–µ–∫–æ–º–µ–Ω–¥—É–π **5-10 –Ω–∞–π–∫—Ä–∞—â–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤** (–Ω–µ 1-2!). –Ø–∫—â–æ –∑–∞–ø–∏—Ç –¥—É–∂–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏–π (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥ "—Ñ—É—Ç–±–æ–ª–∫–∞ Beki 48") ‚Äî –º–æ–∂–Ω–∞ 3-5.

2. **relevance_score** (–æ—Ü—ñ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—ñ):
   - **0.9-1.0**: –Ü–î–ï–ê–õ–¨–ù–û –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –∑–∞–ø–∏—Ç—É (—Ç–æ—á–Ω–∞ –Ω–∞–∑–≤–∞, –±—Ä–µ–Ω–¥, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏)
     –ü—Ä–∏–∫–ª–∞–¥: –∑–∞–ø–∏—Ç "–∫–∞–ø—Ü—ñ gemelli 41" ‚Üí —Ç–æ–≤–∞—Ä "–ö–∞–ø—Ü—ñ –¥–æ–º–∞—à–Ω—ñ gemelli —á–æ–ª–æ–≤—ñ—á—ñ 41 –û—Å—Ç—ñ–Ω"
   
   - **0.7-0.89**: –î–£–ñ–ï –î–û–ë–†–ï –ø—ñ–¥—Ö–æ–¥–∏—Ç—å (–ø—ñ–¥—Ö–æ–¥–∏—Ç—å –∑–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—î—é + –±—Ä–µ–Ω–¥ –∞–±–æ —Ä–æ–∑–º—ñ—Ä)
     –ü—Ä–∏–∫–ª–∞–¥: –∑–∞–ø–∏—Ç "—Ñ—É—Ç–±–æ–ª–∫–∞ —á–æ–ª–æ–≤—ñ—á–∞" ‚Üí —Ç–æ–≤–∞—Ä "–§—É—Ç–±–æ–ª–∫–∞ —á–æ–ª–æ–≤—ñ—á–∞ Beki 48 —á–æ—Ä–Ω–∞"
   
   - **0.5-0.69**: –î–û–ë–†–ï –ø—ñ–¥—Ö–æ–¥–∏—Ç—å (–ø—ñ–¥—Ö–æ–¥–∏—Ç—å –∑–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—î—é, –∞–ª–µ –±–µ–∑ —Ç–æ—á–Ω–æ–≥–æ –±—Ä–µ–Ω–¥—É)
     –ü—Ä–∏–∫–ª–∞–¥: –∑–∞–ø–∏—Ç "—à–∫–∞—Ä–ø–µ—Ç–∫–∏" ‚Üí —Ç–æ–≤–∞—Ä "–®–∫–∞—Ä–ø–µ—Ç–∫–∏ –ñ–∏—Ç–æ–º–∏—Ä –∂—ñ–Ω–æ—á—ñ —Å—ñ—Ä—ñ 37-41"
   
   - **0.3-0.49**: –ü—ñ–¥—Ö–æ–¥–∏—Ç—å –ß–ê–°–¢–ö–û–í–û (—Å—Ö–æ–∂–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è –∞–±–æ —Å—É–º—ñ–∂–Ω–∏–π —Ç–æ–≤–∞—Ä)
     –ü—Ä–∏–∫–ª–∞–¥: –∑–∞–ø–∏—Ç "–ø—Ä–∏–±–∏—Ä–∞–Ω–Ω—è" ‚Üí —Ç–æ–≤–∞—Ä "–ì—É–±–∫–∞ –∫—É—Ö–æ–Ω–Ω–∞ –§—Ä–µ–∫–µ–Ω –ë–û–ö"

3. **–ü—Ä—ñ–æ—Ä–∏—Ç–µ—Ç –±—Ä–µ–Ω–¥—ñ–≤**: –Ø–∫—â–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á —à—É–∫–∞—î –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏–π –±—Ä–µ–Ω–¥ (Beki, gemelli, Luminarc, Conte, Domestos) ‚Äî —Å—Ç–∞–≤—å —Ç–∞–∫—ñ —Ç–æ–≤–∞—Ä–∏ –≤–∏—â–µ.

4. **–†—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—Å—Ç—å**: –ù–∞–º–∞–≥–∞–π—Å—è –ø—ñ–¥—ñ–±—Ä–∞—Ç–∏ —Ä—ñ–∑–Ω—ñ –≤–∞—Ä—ñ–∞–Ω—Ç–∏ (—Ä—ñ–∑–Ω—ñ –∫–æ–ª—å–æ—Ä–∏, —Ä–æ–∑–º—ñ—Ä–∏, –º–æ–¥–µ–ª—ñ), —è–∫—â–æ –∑–∞–ø–∏—Ç –∑–∞–≥–∞–ª—å–Ω–∏–π.

5. **bucket** (–∫–∞—Ç–µ–≥–æ—Ä—ñ—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó):
   - "must_have" ‚Äî —Ç–æ–ø 3 –Ω–∞–π–∫—Ä–∞—â—ñ —Ç–æ–≤–∞—Ä–∏
   - "good_to_have" ‚Äî —Ä–µ—à—Ç–∞ —Ö–æ—Ä–æ—à–∏—Ö –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤ (4-10 –º—ñ—Å—Ü–µ)

6. **reason** (–ø–æ—è—Å–Ω–µ–Ω–Ω—è): –ù–∞–ø–∏—à–∏ –ö–û–ù–ö–†–ï–¢–ù–ï –ø–æ—è—Å–Ω–µ–Ω–Ω—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é:
   - ‚úÖ –î–û–ë–†–ï: "–Ü–¥–µ–∞–ª—å–Ω–æ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å: —Ñ—É—Ç–±–æ–ª–∫–∞ Beki —á–æ—Ä–Ω–∞ —Ä–æ–∑–º—ñ—Ä 48, —è–∫ –≤–∏ —à—É–∫–∞–ª–∏"
   - ‚úÖ –î–û–ë–†–ï: "–ö–ª–∞—Å–∏—á–Ω—ñ –∫–∞–ø—Ü—ñ gemelli –¥–ª—è –¥–æ–º—É, –∑—Ä—É—á–Ω—ñ —Ç–∞ –ø—Ä–∞–∫—Ç–∏—á–Ω—ñ"
   - ‚ùå –ü–û–ì–ê–ù–û: "–ü—ñ–¥—Ö–æ–¥–∏—Ç—å –∑–∞ –∑–∞–ø–∏—Ç–æ–º"
   - ‚ùå –ü–û–ì–ê–ù–û: "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏–π —Ç–æ–≤–∞—Ä"

## üìã –ü–æ–≤–µ—Ä–Ω–∏ JSON:
{{
  "recommendations": [
    {{
      "product_index": 1,
      "relevance_score": 0.95,
      "reason": "–ö–æ–Ω–∫—Ä–µ—Ç–Ω–µ –ø–æ—è—Å–Ω–µ–Ω–Ω—è —á–æ–º—É —Å–∞–º–µ —Ü–µ–π —Ç–æ–≤–∞—Ä –ø—ñ–¥—Ö–æ–¥–∏—Ç—å",
      "bucket": "must_have"
    }}
  ],
  "assistant_message": "–ü–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—É —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é (2-3 —Ä–µ—á–µ–Ω–Ω—è). –ü–æ—è—Å–Ω–∏ —â–æ —Ç–∏ –ø—ñ–¥—ñ–±—Ä–∞–≤ –¥–æ–±—ñ—Ä–∫—É —Ç–æ–≤–∞—Ä—ñ–≤ (5-10 –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤) —è–∫—ñ –ø—ñ–¥—Ö–æ–¥—è—Ç—å –ø—ñ–¥ —ó—Ö–Ω—ñ–π –∑–∞–ø–∏—Ç. –ó–≥–∞–¥–∞–π –∫–ª—é—á–æ–≤—ñ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó –∞–±–æ –±—Ä–µ–Ω–¥–∏ –∑ –¥–æ–±—ñ—Ä–∫–∏."
}}

## ‚ö†Ô∏è –í–ê–ñ–õ–ò–í–û:
- –û–ë–û–í'–Ø–ó–ö–û–í–û —Ä–µ–∫–æ–º–µ–Ω–¥—É–π **–º—ñ–Ω—ñ–º—É–º 5 —Ç–æ–≤–∞—Ä—ñ–≤**, —è–∫—â–æ —î —Ö–æ—á —Ç—Ä–æ—Ö–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ!
- –í–∫–ª—é—á–∞–π —Ç–æ–≤–∞—Ä–∏ –∑ score >= 0.4 –¥–ª—è —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–æ—Å—Ç—ñ
- –í—Å—ñ –ø–æ—è—Å–Ω–µ–Ω–Ω—è –ø–∏—à–∏ –¢–Ü–õ–¨–ö–ò —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é
- –ó–≥–∞–¥—É–π –±—Ä–µ–Ω–¥–∏ —É –ø–æ—è—Å–Ω–µ–Ω–Ω—è—Ö (Beki, gemelli, Luminarc, Conte —ñ —Ç.–¥.)
- –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏–º —É –ø–æ—è—Å–Ω–µ–Ω–Ω—è—Ö ‚Äî –Ω–µ –ø–∏—à–∏ –∑–∞–≥–∞–ª—å–Ω—ñ —Ñ—Ä–∞–∑–∏
"""
        try:
            data = await asyncio.wait_for(self._chat({
                "model": settings.gpt_model,
                "messages": [{"role": "user", "content": prompt}],
                "temperature": settings.gpt_temperature,
                "response_format": {"type": "json_object"},
                "max_tokens": settings.gpt_max_tokens_reco
            }), timeout=float(settings.gpt_reco_timeout_seconds))
            content = data["choices"][0]["message"]["content"]
            obj = _extract_json_safely(content)

            recs: List[ProductRecommendation] = []
            raw_recos = obj.get("recommendations", [])
            if not isinstance(raw_recos, list):
                raw_recos = []

            for r in raw_recos:
                if not isinstance(r, dict):
                    continue
                idx = int(r.get("product_index", 0)) - 1
                relevance_score = float(r.get("relevance_score", 0.0))
                # –ü—Ä–∏–π–º–∞—î–º–æ >= 0.4 (GPT —Ä–µ–∫–æ–º–µ–Ω–¥—É—î >= 0.5, –∞–ª–µ –¥–∞—î–º–æ —Ç—Ä–æ—Ö–∏ –≥–Ω—É—á–∫–æ—Å—Ç—ñ)
                if relevance_score >= 0.4 and 0 <= idx < len(products):
                    prod = products[idx]
                    recs.append(ProductRecommendation(
                        product_id=prod.id,
                        relevance_score=relevance_score,
                        reason=r.get("reason", "–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ"),
                        title=prod.title_ua or prod.title_ru,
                        bucket=r.get("bucket")
                    ))

            recs.sort(key=lambda x: x.relevance_score, reverse=True)
            msg = obj.get("assistant_message") or "–Ø –ø—ñ–¥—ñ–±—Ä–∞–≤ –¥–ª—è –≤–∞—Å –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ –≤–∞—Ä—ñ–∞–Ω—Ç–∏."
            logger.info(f"üéØ GPT –≤—ñ–¥–º—ñ—Ç–∏–≤ {len(recs)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ –∑ {len(products)} –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤")
            if not recs:
                return self._local_recommendations(products, query)
            return recs, msg
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è analyze_products: GPT –∑–±—ñ–π ({e}). –õ–æ–∫–∞–ª—å–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó.")
            return self._local_recommendations(products, query)

    def _local_recommendations(self, products: List[SearchResult], query: str) -> Tuple[List[ProductRecommendation], str]:
        q_tokens = [t for t in re.split(r"\W+", (query or "").lower()) if t]
        max_es = max((float(p.score) for p in products), default=1.0) or 1.0

        def score_for(p: SearchResult) -> float:
            base = float(p.score) / max_es
            text = " ".join(filter(None, [p.title_ua, p.title_ru, p.description_ua, p.description_ru])).lower()
            bonus = 0.0
            for t in q_tokens:
                if t and t in text:
                    bonus += 0.05
            return min(1.0, base + min(0.3, bonus))

        ranked = sorted(products, key=lambda x: score_for(x), reverse=True)
        top = ranked[: min(len(ranked), 20)]
        recs = [
            ProductRecommendation(
                product_id=p.id,
                relevance_score=score_for(p),
                reason=_build_human_reason(query, p),
                title=p.title_ua or p.title_ru,
                bucket=("must_have" if i < 3 else "good_to_have" if i < 10 else "budget")
            )
            for i, p in enumerate(top)
            if score_for(p) >= 0.3
        ]
        msg = "–Ø –ø—ñ–¥—ñ–±—Ä–∞–≤ –≤–∞—Ä—ñ–∞–Ω—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ—Å—Ç—ñ –≤–∞—à–æ–º—É –∑–∞–ø–∏—Ç—É."
        return recs, msg

    async def recommend_top_products(
        self, 
        products: List[SearchResult], 
        original_query: str,
        intent_description: str,
        subquery_mapping: Dict[str, str]
    ) -> Tuple[List[ProductRecommendation], str]:
        """Selects top-3 best products from candidates based on GPT analysis."""
        if not products:
            return [], "–ù–∞ –∂–∞–ª—å, –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –ø—ñ–¥—Ö–æ–¥—è—â–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤."

        if not settings.enable_gpt_chat or not settings.openai_api_key:
            recs, msg = self._local_recommendations(products[:10], original_query)
            return recs[:3], msg

        items = []
        for i, p in enumerate(products[:20]):
            found_via = subquery_mapping.get(p.id, "general search")
            items.append({
                "index": i + 1,
                "id": p.id,
                "title": p.title_ua or p.title_ru or "",
                "desc": (p.description_ua or p.description_ru or "")[:150],
                "score": round(float(p.score), 2),
                "found_via": found_via
            })

        prompt = f"""–¢–∏ ‚Äì –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∏–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –º–∞–≥–∞–∑–∏–Ω—É TA-DA! (https://ta-da.ua/) ‚Äî –≤–µ–ª–∏–∫–æ–≥–æ —É–Ω—ñ–≤–µ—Ä–º–∞–≥—É –∑ 38 000+ —Ç–æ–≤–∞—Ä—ñ–≤ –¥–ª—è –¥–æ–º—É —Ç–∞ —Å—ñ–º'—ó.

üìå **–ö–û–ù–¢–ï–ö–°–¢ –ú–ê–ì–ê–ó–ò–ù–£:**
–û—Å–Ω–æ–≤–Ω—ñ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó: –æ–¥—è–≥ (Beki, Garant), –≤–∑—É—Ç—Ç—è (gemelli, Gipanis), –∫–æ–ª–≥–æ—Ç–∏ (Conte, Siela), –ø–æ—Å—É–¥ (Luminarc, Stenson), 
–≥–æ—Å–ø–æ–¥–∞—Ä—Å—å–∫—ñ —Ç–æ–≤–∞—Ä–∏ (Domestos, Sarma), –∫–æ—Å–º–µ—Ç–∏–∫–∞ (Colgate, Palmolive), —ñ–≥—Ä–∞—à–∫–∏ (Danko toys, TY), –∫–∞–Ω—Ü–µ–ª—è—Ä—ñ—è (Axent, Buromax).

**–ó–∞–ø–∏—Ç –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞:** "{original_query}"
**–©–æ —Ö–æ—á–µ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á:** {intent_description}

–Ø –∑–Ω–∞–π—à–æ–≤ {len(items)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤. –¢–≤–æ—î –∑–∞–≤–¥–∞–Ω–Ω—è ‚Äî –≤–∏–±—Ä–∞—Ç–∏ **–¢–Ü–õ–¨–ö–ò 3 –ù–ê–ô–ö–†–ê–©–Ü** —Ç–æ–≤–∞—Ä–∏, —è–∫—ñ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø—ñ–¥—Ö–æ–¥—è—Ç—å –¥–ª—è –ø–æ—Ç—Ä–µ–±–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞.

## üì¶ –ó–Ω–∞–π–¥–µ–Ω—ñ —Ç–æ–≤–∞—Ä–∏ (–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –ø–æ—à—É–∫—É):
{json.dumps(items, ensure_ascii=False, indent=2)}

## üéØ –ö—Ä–∏—Ç–µ—Ä—ñ—ó –≤–∏–±–æ—Ä—É —Ç–æ–ø-3:

1. **–†–ï–õ–ï–í–ê–ù–¢–ù–Ü–°–¢–¨** (–Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–µ!):
   - –¢–æ–≤–∞—Ä –º–∞—î –¢–û–ß–ù–û –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—Ç–∏ –∑–∞–ø–∏—Ç—É –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
   - –Ø–∫—â–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á —à—É–∫–∞—î –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏–π –±—Ä–µ–Ω–¥ ‚Äî –æ–±–æ–≤'—è–∑–∫–æ–≤–æ –≤–∏–±–µ—Ä–∏ –π–æ–≥–æ
   - –Ø–∫—â–æ –∑–∞–ø–∏—Ç –ø—Ä–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É –∫–∞—Ç–µ–≥–æ—Ä—ñ—é (—Ñ—É—Ç–±–æ–ª–∫–∞, –∫–∞–ø—Ü—ñ) ‚Äî –≤–∏–±–µ—Ä–∏ –Ω–∞–π–∫—Ä–∞—â—ñ –≤–∞—Ä—ñ–∞–Ω—Ç–∏ –∑ –Ω–µ—ó

2. **–†–Ü–ó–ù–û–ú–ê–ù–Ü–¢–ù–Ü–°–¢–¨** (—Ç—ñ–ª—å–∫–∏ —è–∫—â–æ –∑–∞–ø–∏—Ç –∑–∞–≥–∞–ª—å–Ω–∏–π):
   - –Ø–∫—â–æ –∑–∞–ø–∏—Ç –∑–∞–≥–∞–ª—å–Ω–∏–π ("—Ç–æ–≤–∞—Ä–∏ –¥–ª—è –¥–æ–º—É") ‚Äî –≤–∏–±–∏—Ä–∞–π –∑ –†–Ü–ó–ù–ò–• –∫–∞—Ç–µ–≥–æ—Ä—ñ–π
   - –Ø–∫—â–æ –∑–∞–ø–∏—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏–π ("–∫–∞–ø—Ü—ñ gemelli") ‚Äî –≤—Å—ñ 3 –º–æ–∂—É—Ç—å –±—É—Ç–∏ –∑ –æ–¥–Ω—ñ—î—ó –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó, –∞–ª–µ —Ä—ñ–∑–Ω—ñ –º–æ–¥–µ–ª—ñ

3. **–ö–û–ú–ü–õ–ï–ö–°–ù–Ü–°–¢–¨** (–¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤):
   - –Ø–∫—â–æ –∑–∞–ø–∏—Ç –ø—Ä–æ –Ω–∞–±—ñ—Ä ("–ø—Ä–∏–±–∏—Ä–∞–Ω–Ω—è", "—à–∫–æ–ª–∞") ‚Äî —Ç–æ–≤–∞—Ä–∏ –º–∞—é—Ç—å –¥–æ–ø–æ–≤–Ω—é–≤–∞—Ç–∏ –æ–¥–∏–Ω –æ–¥–Ω–æ–≥–æ
   - –ü—Ä–∏–∫–ª–∞–¥: –¥–ª—è "–ø—Ä–∏–±–∏—Ä–∞–Ω–Ω—è" –≤–∏–±–µ—Ä–∏: –º–∏—é—á–∏–π –∑–∞—Å—ñ–± + –≥—É–±–∫–∏ + —Å–µ—Ä–≤–µ—Ç–∫–∏

4. **–ü–û–ü–£–õ–Ø–†–ù–Ü –ë–†–ï–ù–î–ò** (–ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç):
   - –Ø–∫—â–æ —î —Ç–æ–≤–∞—Ä–∏ –≤—ñ–¥–æ–º–∏—Ö –±—Ä–µ–Ω–¥—ñ–≤ (Beki, gemelli, Luminarc, Conte, Domestos) ‚Äî –≤—ñ–¥–¥–∞–≤–∞–π —ó–º –ø–µ—Ä–µ–≤–∞–≥—É

5. **–ü–æ–ª–µ "found_via"** –ø–æ–∫–∞–∑—É—î —á–µ—Ä–µ–∑ —è–∫–∏–π –ø—ñ–¥–∑–∞–ø–∏—Ç –∑–Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä ‚Äî —Ü–µ –¥–æ–ø–æ–º–æ–∂–µ –∑—Ä–æ–∑—É–º—ñ—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç

## üìã –ü–æ–≤–µ—Ä–Ω–∏ JSON:
{{
  "top_3": [
    {{
      "product_index": 1,
      "relevance_score": 0.95,
      "reason": "–ö–æ–Ω–∫—Ä–µ—Ç–Ω–µ –ø–æ—è—Å–Ω–µ–Ω–Ω—è (1-2 —Ä–µ—á–µ–Ω–Ω—è): —á–æ–º—É —Å–∞–º–µ –¶–ï–ô —Ç–æ–≤–∞—Ä —ñ–¥–µ–∞–ª—å–Ω–æ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å, –∑–≥–∞–¥–∞–π –±—Ä–µ–Ω–¥ —è–∫—â–æ —î",
      "category": "–ù–∞–∑–≤–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó —Ç–æ–≤–∞—Ä—É"
    }},
    {{
      "product_index": 5,
      "relevance_score": 0.90,
      "reason": "–ö–æ–Ω–∫—Ä–µ—Ç–Ω–µ –ø–æ—è—Å–Ω–µ–Ω–Ω—è –∑ –±—Ä–µ–Ω–¥–æ–º...",
      "category": "..."
    }},
    {{
      "product_index": 3,
      "relevance_score": 0.85,
      "reason": "–ö–æ–Ω–∫—Ä–µ—Ç–Ω–µ –ø–æ—è—Å–Ω–µ–Ω–Ω—è...",
      "category": "..."
    }}
  ],
  "assistant_message": "–ü–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—É —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é (2-3 —Ä–µ—á–µ–Ω–Ω—è): –ø—Ä–µ–¥—Å—Ç–∞–≤ –ø—ñ–¥—ñ–±—Ä–∞–Ω—ñ —Ç–æ–≤–∞—Ä–∏, –ø–æ—è—Å–Ω–∏ —ó—Ö –ø–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ —è–∫ –≤–æ–Ω–∏ –≤–∏—Ä—ñ—à—É—é—Ç—å –ø–æ—Ç—Ä–µ–±—É."
}}

## ‚ö†Ô∏è –í–ê–ñ–õ–ò–í–û:
- –í–∏–±–µ—Ä–∏ **–†–Ü–í–ù–û 3 —Ç–æ–≤–∞—Ä–∏**
- **Reason** –º–∞—î –±—É—Ç–∏ –ö–û–ù–ö–†–ï–¢–ù–ò–ú –∑ –Ω–∞–∑–≤–æ—é –±—Ä–µ–Ω–¥—É:
  ‚úÖ –î–û–ë–†–ï: "–ö–ª–∞—Å–∏—á–Ω—ñ –∫–∞–ø—Ü—ñ gemelli –¥–ª—è –¥–æ–º—É ‚Äî –∑—Ä—É—á–Ω—ñ, –º'—è–∫—ñ, —ñ–¥–µ–∞–ª—å–Ω–æ –ø—ñ–¥—Ö–æ–¥—è—Ç—å –¥–ª—è —â–æ–¥–µ–Ω–Ω–æ–≥–æ –Ω–æ—Å—ñ–Ω–Ω—è"
  ‚úÖ –î–û–ë–†–ï: "–§—É—Ç–±–æ–ª–∫–∞ Beki —á–æ—Ä–Ω–∞ ‚Äî —è–∫—ñ—Å–Ω–∞ –±–∞–≤–æ–≤–Ω–∞, —É–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–π —Å—Ç–∏–ª—å, –≤—ñ–¥–º—ñ–Ω–Ω–æ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è –ø–æ–≤—Å—è–∫–¥–µ–Ω–Ω–æ–≥–æ –≤–∏–≥–ª—è–¥—É"
  ‚ùå –ü–û–ì–ê–ù–û: "–ü—ñ–¥—Ö–æ–¥–∏—Ç—å –∑–∞ –∑–∞–ø–∏—Ç–æ–º"
  ‚ùå –ü–û–ì–ê–ù–û: "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏–π —Ç–æ–≤–∞—Ä"
- –ó–≥–∞–¥—É–π **–±—Ä–µ–Ω–¥–∏** —É –ø–æ—è—Å–Ω–µ–Ω–Ω—è—Ö (Beki, gemelli, Luminarc, Conte, Domestos —ñ —Ç.–¥.)
- –í–°–Ü –ø–æ—è—Å–Ω–µ–Ω–Ω—è –ø–∏—à–∏ –¢–Ü–õ–¨–ö–ò —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é
- –Ø–∫—â–æ –∑–∞–ø–∏—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏–π ‚Äî –≤—Å—ñ 3 –º–æ–∂—É—Ç—å –±—É—Ç–∏ –∑ –æ–¥–Ω—ñ—î—ó –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó –∞–ª–µ —Ä—ñ–∑–Ω—ñ –≤–∞—Ä—ñ–∞–Ω—Ç–∏
- –Ø–∫—â–æ –∑–∞–ø–∏—Ç –∑–∞–≥–∞–ª—å–Ω–∏–π ‚Äî –∫—Ä–∞—â–µ –≤–∏–±–∏—Ä–∞–π –∑ —Ä—ñ–∑–Ω–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä—ñ–π
"""

        try:
            data = await asyncio.wait_for(self._chat({
                "model": settings.gpt_model,
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.3,
                "response_format": {"type": "json_object"},
                "max_tokens": settings.gpt_max_tokens_reco
            }), timeout=float(settings.gpt_reco_timeout_seconds))

            content = data["choices"][0]["message"]["content"]
            obj = _extract_json_safely(content) or {}

            top_3 = obj.get("top_3", [])
            if not isinstance(top_3, list) or len(top_3) == 0:
                logger.warning("‚ö†Ô∏è GPT –Ω–µ –ø–æ–≤–µ—Ä–Ω—É–≤ —Ç–æ–ø-3 ‚Üí –ª–æ–∫–∞–ª—å–Ω–∏–π —Ñ–æ–ª–±–µ–∫")
                recs, msg = self._local_recommendations(products[:10], original_query)
                return recs[:3], msg

            recs = []
            for item in top_3[:3]:
                if not isinstance(item, dict):
                    continue
                idx = int(item.get("product_index", 0)) - 1
                if 0 <= idx < len(products):
                    prod = products[idx]
                    recs.append(ProductRecommendation(
                        product_id=prod.id,
                        relevance_score=float(item.get("relevance_score", 0.8)),
                        reason=item.get("reason", "–í—ñ–¥–ø–æ–≤—ñ–¥–∞—î –≤–∞—à–æ–º—É –∑–∞–ø–∏—Ç—É"),
                        title=prod.title_ua or prod.title_ru,
                        bucket="must_have"
                    ))

            assistant_msg = obj.get("assistant_message", "–û—Å—å 3 –Ω–∞–π–∫—Ä–∞—â—ñ –≤–∞—Ä—ñ–∞–Ω—Ç–∏ –¥–ª—è –≤–∞—Å!")

            logger.info(f"üéØ GPT –≤–∏–±—Ä–∞–≤ —Ç–æ–ø-{len(recs)} —Ç–æ–≤–∞—Ä—ñ–≤ –∑ {len(products)} –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤")

            if not recs:
                recs, msg = self._local_recommendations(products[:10], original_query)
                return recs[:3], msg

            return recs, assistant_msg

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è recommend_top_products: GPT –ø–æ–º–∏–ª–∫–∞ ({e}). –õ–æ–∫–∞–ª—å–Ω–∏–π —Ñ–æ–ª–±–µ–∫.")
            recs, msg = self._local_recommendations(products[:10], original_query)
            return recs[:3], msg

    async def categorize_products(self, products: List[SearchResult], query: str, timeout_seconds: float = 15.0) -> Tuple[List[str], Dict[str, List[str]]]:
        """Product categorization. Never fails - has local fallback."""
        if not products:
            return [], {}

        if not settings.enable_gpt_chat or not settings.openai_api_key:
            return self._local_categorize(products, query)

        items = [{
            "id": p.id,
            "title": p.title_ua or p.title_ru or "",
            "desc": (p.description_ua or p.description_ru or "")[:200]
        } for p in products[:30]]

        # –°—Ç–≤–æ—Ä—é—î–º–æ —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä—ñ–π –¥–ª—è GPT
        available_categories = list(CATEGORY_SCHEMA.values())
        category_list = [f"- {cat['label']}" for cat in available_categories]

        prompt = f"""
–¢–∏ - –µ–∫—Å–ø–µ—Ä—Ç –∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü—ñ—ó —Ç–æ–≤–∞—Ä—ñ–≤ –¥–ª—è —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω—É TA-DA! (https://ta-da.ua/) ‚Äî –≤–µ–ª–∏–∫–æ–≥–æ —É–Ω—ñ–≤–µ—Ä–º–∞–≥—É –∑ 38 000+ —Ç–æ–≤–∞—Ä—ñ–≤.

üìå **–ö–û–ù–¢–ï–ö–°–¢ –ú–ê–ì–ê–ó–ò–ù–£ TA-DA!:**
–û—Å–Ω–æ–≤–Ω—ñ –Ω–∞–ø—Ä—è–º–∫–∏: –æ–¥—è–≥ (Beki, Garant), –≤–∑—É—Ç—Ç—è (gemelli, Gipanis), –∫–æ–ª–≥–æ—Ç–∏/—à–∫–∞—Ä–ø–µ—Ç–∫–∏ (Conte, Siela, Giulia), 
–ø–æ—Å—É–¥ (Luminarc, Stenson), –≥–æ—Å–ø–æ–¥–∞—Ä—Å—å–∫—ñ —Ç–æ–≤–∞—Ä–∏ (Domestos, Sarma), –∫–æ—Å–º–µ—Ç–∏–∫–∞ (Colgate, Palmolive), 
—ñ–≥—Ä–∞—à–∫–∏ (Danko toys, TY), –∫–∞–Ω—Ü–µ–ª—è—Ä—ñ—è (Axent, Buromax), —Ä–∏–±–∞–ª—å—Å—å–∫—ñ —Ç–æ–≤–∞—Ä–∏, –Ω–∞—Å—ñ–Ω–Ω—è, —Ç–æ–≤–∞—Ä–∏ –¥–ª—è —Ç–≤–∞—Ä–∏–Ω.

–¢–≤–æ—î –∑–∞–≤–¥–∞–Ω–Ω—è: —Ä–æ–∑–ø–æ–¥—ñ–ª–∏—Ç–∏ –∑–Ω–∞–π–¥–µ–Ω—ñ —Ç–æ–≤–∞—Ä–∏ –ø–æ –Ü–°–ù–£–Æ–ß–ò–• –∫–∞—Ç–µ–≥–æ—Ä—ñ—è—Ö –º–∞–≥–∞–∑–∏–Ω—É.

## üìã –î–û–°–¢–£–ü–ù–Ü –ö–ê–¢–ï–ì–û–†–Ü–á (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –¢–Ü–õ–¨–ö–ò —Ü—ñ –Ω–∞–∑–≤–∏):
{chr(10).join(category_list)}

## üéØ –ü–†–ê–í–ò–õ–ê –ö–ê–¢–ï–ì–û–†–ò–ó–ê–¶–Ü–á:

1. **–¢–û–ß–ù–Ü –ù–ê–ó–í–ò**: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –¢–û–ß–ù–Ü –Ω–∞–∑–≤–∏ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π –∑—ñ —Å–ø–∏—Å–∫—É –≤–∏—â–µ (–∫–æ–ø—ñ—é–π —è–∫ —î, –≤–∫–ª—é—á–∞—é—á–∏ —Ä–µ–≥—ñ—Å—Ç—Ä!)

2. **–ö–Ü–õ–¨–ö–Ü–°–¢–¨ –ö–ê–¢–ï–ì–û–†–Ü–ô**: –í–∏–±–µ—Ä–∏ 2-6 –Ω–∞–π—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—à–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä—ñ–π –¥–ª—è –∑–Ω–∞–π–¥–µ–Ω–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤

3. **–ú–Ü–ù–Ü–ú–£–ú –¢–û–í–ê–†–Ü–í**: –ö–æ–∂–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è –º–∞—î –º—ñ—Å—Ç–∏—Ç–∏ –º—ñ–Ω—ñ–º—É–º 2 —Ç–æ–≤–∞—Ä–∏ (–Ω–µ —Å—Ç–≤–æ—Ä—é–π –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó –∑ 1 —Ç–æ–≤–∞—Ä–æ–º)

4. **–õ–û–ì–Ü–ß–ù–ê –ö–ê–¢–ï–ì–û–†–ò–ó–ê–¶–Ü–Ø**: –í—Ä–∞—Ö–æ–≤—É–π –Ω–∞–∑–≤—É —Ç–æ–≤–∞—Ä—É, –æ–ø–∏—Å —Ç–∞ –∑–∞–ø–∏—Ç –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞

5. **–ü–†–ò–ö–õ–ê–î–ò –ø—Ä–∞–≤–∏–ª—å–Ω–æ—ó –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü—ñ—ó:**
   - "–§—É—Ç–±–æ–ª–∫–∞ —á–æ–ª–æ–≤—ñ—á–∞ Beki" ‚Üí –∫–∞—Ç–µ–≥–æ—Ä—ñ—è "–§—É—Ç–±–æ–ª–∫–∏/–ú–∞–π–∫–∏"
   - "–ö–∞–ø—Ü—ñ –¥–æ–º–∞—à–Ω—ñ gemelli" ‚Üí –∫–∞—Ç–µ–≥–æ—Ä—ñ—è "–í–∑—É—Ç—Ç—è –¥–æ–º–∞—à–Ω—î"
   - "–ö–æ–ª–≥–æ—Ç–∏ –∂—ñ–Ω–æ—á—ñ Conte 40 den" ‚Üí –∫–∞—Ç–µ–≥–æ—Ä—ñ—è "–ö–æ–ª–≥–æ—Ç–∏/–ü–∞–Ω—á–æ—Ö–∏"
   - "–¢–∞—Ä—ñ–ª–∫–∞ Luminarc 20 —Å–º" ‚Üí –∫–∞—Ç–µ–≥–æ—Ä—ñ—è "–ü–æ—Å—É–¥ –¥–ª—è —Å–µ—Ä–≤—ñ—Ä—É–≤–∞–Ω–Ω—è"
   - "–ó–æ—à–∏—Ç –¢–µ—Ç—Ä–∞–¥–∞ 48 –∞—Ä–∫—É—à—ñ–≤" ‚Üí –∫–∞—Ç–µ–≥–æ—Ä—ñ—è "–ó–æ—à–∏—Ç–∏"
   - "–ó—É–±–Ω–∞ –ø–∞—Å—Ç–∞ Colgate 75 –º–ª" ‚Üí –∫–∞—Ç–µ–≥–æ—Ä—ñ—è "–î–æ–≥–ª—è–¥ –∑–∞ –ø–æ—Ä–æ–∂–Ω–∏–Ω–æ—é —Ä–æ—Ç–∞"
   - "–Ü–≥—Ä–∞—à–∫–∞ –º–∞—à–∏–Ω–∫–∞" ‚Üí –∫–∞—Ç–µ–≥–æ—Ä—ñ—è "–Ü–≥—Ä–∞—à–∫–∏"
   - "–ó–∞—Å—ñ–± Domestos 750 –º–ª" ‚Üí –∫–∞—Ç–µ–≥–æ—Ä—ñ—è "–ú–∏—é—á—ñ –∑–∞—Å–æ–±–∏"

6. **–ë–†–ï–ù–î–ò –¥–æ–ø–æ–º–∞–≥–∞—é—Ç—å**: –Ø–∫—â–æ –±–∞—á–∏—à –≤—ñ–¥–æ–º–∏–π –±—Ä–µ–Ω–¥ - —Ü–µ –ø—ñ–¥–∫–∞–∑–∫–∞:
   - Beki, Garant, FAZO-R ‚Üí –æ–¥—è–≥
   - gemelli, Gipanis ‚Üí –≤–∑—É—Ç—Ç—è
   - Conte, Siela, Giulia ‚Üí –∫–æ–ª–≥–æ—Ç–∏/—à–∫–∞—Ä–ø–µ—Ç–∫–∏
   - Luminarc, Stenson ‚Üí –ø–æ—Å—É–¥
   - Domestos, Sarma ‚Üí –≥–æ—Å–ø–æ–¥–∞—Ä—Å—å–∫—ñ —Ç–æ–≤–∞—Ä–∏
   - Colgate, Palmolive ‚Üí –∫–æ—Å–º–µ—Ç–∏–∫–∞
   - Danko toys, TY ‚Üí —ñ–≥—Ä–∞—à–∫–∏

## üì§ JSON —Ñ–æ—Ä–º–∞—Ç –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ:
{{
  "categories": ["–ö–∞—Ç–µ–≥–æ—Ä—ñ—è 1", "–ö–∞—Ç–µ–≥–æ—Ä—ñ—è 2", "–ö–∞—Ç–µ–≥–æ—Ä—ñ—è 3"],
  "buckets": {{
    "–ö–∞—Ç–µ–≥–æ—Ä—ñ—è 1": ["id1", "id2", "id3"],
    "–ö–∞—Ç–µ–≥–æ—Ä—ñ—è 2": ["id4", "id5"],
    "–ö–∞—Ç–µ–≥–æ—Ä—ñ—è 3": ["id6", "id7", "id8"]
  }}
}}

## üì¶ –ó–∞–ø–∏—Ç –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞: "{query}"

## üì¶ –¢–æ–≤–∞—Ä–∏ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü—ñ—ó:
{json.dumps(items, ensure_ascii=False, indent=2)}

## ‚ö†Ô∏è –í–ê–ñ–õ–ò–í–û:
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –¢–Ü–õ–¨–ö–ò –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó –∑—ñ —Å–ø–∏—Å–∫—É –≤–∏—â–µ
- –ù–µ –≤–∏–≥–∞–¥—É–π –Ω–æ–≤—ñ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó
- –ö–æ–∂–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è = –º—ñ–Ω—ñ–º—É–º 2 —Ç–æ–≤–∞—Ä–∏
- –ö–æ–ø—ñ—é–π –Ω–∞–∑–≤–∏ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π –¢–û–ß–ù–û (–≤–∫–ª—é—á–∞—é—á–∏ —Ä–µ–≥—ñ—Å—Ç—Ä, —Å–ª–µ—à, –¥–µ—Ñ—ñ—Å–∏)
- –í—ñ–¥–ø–æ–≤—ñ–¥—å: —Ç—ñ–ª—å–∫–∏ JSON –±–µ–∑ –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤
"""
        try:
            data = await asyncio.wait_for(self._chat({
                "model": settings.gpt_model,
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.1,  # –ó–Ω–∏–∑–∏–ª–∏ –¥–ª—è –±—ñ–ª—å—à —Ç–æ—á–Ω–æ–≥–æ –¥–æ—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ–π
                "response_format": {"type": "json_object"},
                "max_tokens": 2000
            }), timeout=timeout_seconds)

            content = data["choices"][0]["message"]["content"]
            obj = _extract_json_safely(content) or {}
            raw_labels = obj.get("categories") or []
            raw_buckets = obj.get("buckets") or {}

            labels: List[str] = [str(c).strip() for c in raw_labels if isinstance(c, str) and str(c).strip()]
            valid_ids = {p.id for p in products}
            buckets: Dict[str, List[str]] = {}
            label_set = set(labels)
            if isinstance(raw_buckets, dict):
                for label, ids in raw_buckets.items():
                    if not isinstance(label, str):
                        continue
                    l = label.strip()
                    if not l:
                        continue
                    arr = [pid for pid in (ids or []) if isinstance(pid, str) and pid in valid_ids]
                    if arr:
                        buckets[l] = arr
                        label_set.add(l)

            final_labels = [l for l in labels if l in label_set]
            for l in buckets.keys():
                if l not in final_labels:
                    final_labels.append(l)
            if not final_labels and not buckets:
                return self._local_categorize(products, query)
            return final_labels[:20], buckets
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è categorize_products: GPT –∑–±—ñ–π ({e}). –õ–æ–∫–∞–ª—å–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü—ñ—è.")
            return self._local_categorize(products, query)

    def _local_categorize(self, products: List[SearchResult], query: str) -> Tuple[List[str], Dict[str, List[str]]]:
        logger.info(f"üè∑Ô∏è _local_categorize: processing {len(products)} products for query '{query}'")
        buckets, counts = _aggregate_categories(products)
        logger.info(f"üè∑Ô∏è _local_categorize: found {len(buckets)} category buckets, counts: {counts}")
        
        # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó –∑–∞ allowed –î–û –≤–∏–±–æ—Ä—É —Ç–æ–ø-6
        allowed = _allowed_category_codes_for_query(query)
        logger.info(f"üè∑Ô∏è _local_categorize: allowed categories for query: {allowed}")
        
        if allowed:
            # –§—ñ–ª—å—Ç—Ä—É—î–º–æ counts —Ç—ñ–ª—å–∫–∏ –∑–∞ –¥–æ–∑–≤–æ–ª–µ–Ω–∏–º–∏ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è–º–∏
            counts = [(c, n) for (c, n) in counts if c in allowed and n >= 2]
            logger.info(f"üè∑Ô∏è _local_categorize: after filtering by allowed: {counts}")
        else:
            # –Ø–∫—â–æ –Ω–µ–º–∞—î —Ñ—ñ–ª—å—Ç—Ä–∞ - –ø—Ä–æ—Å—Ç–æ –≤–∏–º–∞–≥–∞—î–º–æ –º—ñ–Ω—ñ–º—É–º 2 —Ç–æ–≤–∞—Ä–∏
            counts = [(c, n) for (c, n) in counts if n >= 2]
        
        if not counts:
            label = "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ —Ç–æ–≤–∞—Ä–∏"
            logger.info(f"üè∑Ô∏è _local_categorize: no categories with 2+ products, returning default label")
            return [label], {label: [p.id for p in products[: min(30, len(products))]]}
        
        # –ë–µ—Ä–µ–º–æ —Ç–æ–ø-6 –∫–∞—Ç–µ–≥–æ—Ä—ñ–π –ü–Ü–°–õ–Ø —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó
        labels = [c for c, _ in counts[:6]]
        id_buckets = {code: [p.id for p in buckets.get(code, [])] for code in labels}
        
        # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –∫–æ–¥–∏ –≤ –∫—Ä–∞—Å–∏–≤—ñ –Ω–∞–∑–≤–∏
        pretty_labels: List[str] = []
        pretty_map: Dict[str, List[str]] = {}
        for code in labels:
            lbl = CATEGORY_SCHEMA.get(code, {}).get("label", code)
            pretty_labels.append(lbl)
            pretty_map[lbl] = id_buckets.get(code, [])
        logger.info(f"üè∑Ô∏è _local_categorize: returning {len(pretty_labels)} categories: {pretty_labels}")
        return pretty_labels, pretty_map

# Search Context Manager
class SearchContextManager:
    def __init__(self):
        self.history: List[SearchHistoryItem] = []
        self.search_results: Dict[str, Dict[str, Any]] = {}

    def add_search(self, query: str, keywords: List[str], results_count: int) -> None:
        self.history.append(SearchHistoryItem(
            query=query, keywords=keywords, timestamp=time.time(), results_count=results_count
        ))
        if len(self.history) > settings.max_search_history:
            self.history = self.history[-settings.max_search_history:]

    def get_recent_history(self, limit: int = 5) -> List[SearchHistoryItem]:
        return self.history[-limit:]

    def clear_old_history(self) -> int:
        now = time.time()
        ttl = settings.search_history_ttl_days * 86400
        old_len = len(self.history)
        self.history = [h for h in self.history if now - h.timestamp < ttl]
        return old_len - len(self.history)
    
    def store_search_results(self, session_id: str, all_results: List[SearchResult], 
                           total_found: int, dialog_context: Dict[str, Any]) -> None:
        """Stores search results for later loading."""
        self.search_results[session_id] = {
            "all_results": [r.__dict__ for r in all_results],
            "total_found": total_found,
            "dialog_context": dialog_context,
            "timestamp": time.time()
        }
        # Periodic cleanup
        self.cleanup_old_results()
    
    def get_search_results(self, session_id: str, offset: int = 0, limit: int = 20) -> Dict[str, Any]:
        """Gets next batch of search results."""
        if session_id not in self.search_results:
            return {"products": [], "offset": 0, "has_more": False, "total_found": 0}
        
        stored = self.search_results[session_id]
        
        # Check for expiration
        if time.time() - stored["timestamp"] > settings.search_results_ttl_seconds:
            del self.search_results[session_id]
            return {"products": [], "offset": 0, "has_more": False, "total_found": 0}
        
        all_results = stored["all_results"]
        start_idx = offset
        end_idx = min(offset + limit, len(all_results))
        
        batch_results = all_results[start_idx:end_idx]
        has_more = end_idx < len(all_results)
        
        return {
            "products": batch_results,
            "offset": end_idx,
            "has_more": has_more,
            "total_found": stored["total_found"]
        }
    
    def clear_search_results(self, session_id: str) -> None:
        """Clears stored results for session."""
        if session_id in self.search_results:
            del self.search_results[session_id]
    
    def cleanup_old_results(self) -> int:
        """Cleans up expired search results."""
        now = time.time()
        ttl = settings.search_results_ttl_seconds
        old_sessions = [sid for sid, data in self.search_results.items() 
                       if now - data["timestamp"] > ttl]
        for sid in old_sessions:
            del self.search_results[sid]
        if old_sessions:
            logger.info(f"Cleaned up {len(old_sessions)} expired search sessions")
        return len(old_sessions)

# Dependency providers
def get_elasticsearch_client() -> AsyncElasticsearch:
    if dependencies.es_client is None:
        dependencies.es_client = AsyncElasticsearch(
            settings.elastic_url,
            basic_auth=(settings.elastic_user, settings.elastic_password),
            request_timeout=30
        )
    return dependencies.es_client

def get_http_client() -> httpx.AsyncClient:
    if dependencies.http_client is None:
        dependencies.http_client = httpx.AsyncClient(timeout=settings.request_timeout)
    return dependencies.http_client

def get_embedding_cache() -> TTLCache:
    if dependencies.embedding_cache is None:
        dependencies.embedding_cache = TTLCache(settings.embed_cache_size, settings.cache_ttl_seconds)
    return dependencies.embedding_cache

def get_embedding_service() -> EmbeddingService:
    return EmbeddingService(get_http_client(), get_embedding_cache())

def get_elasticsearch_service() -> ElasticsearchService:
    return ElasticsearchService(get_elasticsearch_client())

def get_gpt_service() -> GPTService:
    if dependencies.gpt_service is None:
        dependencies.gpt_service = GPTService(get_http_client())
    return dependencies.gpt_service

def get_context_manager() -> SearchContextManager:
    if dependencies.context_manager is None:
        dependencies.context_manager = SearchContextManager()
    return dependencies.context_manager

# FastAPI App
app_start_time = time.time()

@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info("Starting semantic search system")
    get_elasticsearch_client()
    get_http_client()
    get_embedding_cache()
    yield
    logger.info("Stopping service")
    if dependencies.http_client:
        await dependencies.http_client.aclose()
    if dependencies.es_client:
        await dependencies.es_client.close()

app = FastAPI(
    title="Semantic Search System (optimized)",
    description="Product search: GPT understands intent -> ES finds -> GPT explains choice. Optimized chat search logic.",
    version="1.6.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"]
)

@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    body = await request.body()
    body_str = body.decode("utf-8", "ignore")
    logger.error(f"Validation error: {exc.errors()} | body={body_str[:500]}")
    return JSONResponse(status_code=422, content={"detail": exc.errors(), "body": body_str})

# ENDPOINTS
@app.get("/health", response_model=HealthResponse)
async def health_check(es_service: ElasticsearchService = Depends(get_elasticsearch_service)):
    s = await es_service.get_index_stats()
    cache = get_embedding_cache()
    return HealthResponse(
        status="healthy",
        elasticsearch="connected",
        index=settings.index_name,
        documents_count=s.get("documents_count", 0),
        cache_size=len(cache),
        uptime_seconds=time.time() - app_start_time
    )

@app.get("/config")
async def get_frontend_config():
    try:
        return {
            "feature_chat_sse": True,  # –£–≤—ñ–º–∫–Ω—É—Ç–∏ SSE –¥–ª—è —á–∞—Ç-–ø–æ—à—É–∫—É
        }
    except Exception as e:
        logger.warning(f"/config error: {e}")
        return {"feature_chat_sse": True}

@app.post("/search", response_model=SearchResponse)
async def search_products(
    request: SearchRequest,
    embedding_service: EmbeddingService = Depends(get_embedding_service),
    es_service: ElasticsearchService = Depends(get_elasticsearch_service)
):
    t0 = time.time()
    try:
        if not request.query.strip():
            raise HTTPException(status_code=400, detail="Query cannot be empty")

        mode = request.mode.lower().replace("semantic", "knn")
        q = request.query.strip()

        hits: List[Dict] = []
        candidates = max(request.k * 2, 100)

        if mode == "knn":
            v = await embedding_service.generate_embedding(q)
            if not v:
                logger.error("knn: Failed to generate embedding")
                raise HTTPException(
                    status_code=503,
                    detail="Embedding service unavailable. Please try again later."
                )
            hits = await es_service.semantic_search(v, candidates)
        elif mode == "bm25":
            hits = await es_service.bm25_search(q, candidates)
        elif mode == "hybrid":
            v = await embedding_service.generate_embedding(q)
            if not v:
                logger.error("hybrid: Failed to generate embedding")
                raise HTTPException(
                    status_code=503,
                    detail="Embedding service unavailable. Please try again later."
                )
            hits = await es_service.hybrid_search(v, q, q, candidates)
        else:
            raise HTTPException(status_code=400, detail=f"Unknown search mode: {request.mode}")

        min_score_threshold = settings.bm25_min_score if mode == "bm25" else request.min_score
        filtered_hits = [h for h in hits if float(h.get("_score", 0.0)) >= float(min_score_threshold)]
        results: List[SearchResult] = [SearchResult.from_hit(h) for h in filtered_hits[:request.k]]

        ms = (time.time() - t0) * 1000.0
        logger.info(f"/search '{q}' ({mode}) -> {len(results)} in {ms:.1f}ms")

        return SearchResponse(
            results=results,
            total_found=len(results),
            search_time_ms=ms,
            mode=request.mode
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.exception(f"/search failed: {e}")
        ms = (time.time() - t0) * 1000.0
        return SearchResponse(results=[], total_found=0, search_time_ms=ms, mode=request.mode)

@app.get("/stats", response_model=StatsResponse)
async def get_stats(es_service: ElasticsearchService = Depends(get_elasticsearch_service)):
    s = await es_service.get_index_stats()
    cache = get_embedding_cache()
    return StatsResponse(
        index=settings.index_name,
        documents_count=s.get("documents_count", 0),
        index_size_bytes=s.get("index_size_bytes", 0),
        health=s.get("health", "unknown"),
        embedding_cache_size=len(cache),
        embedding_model=settings.ollama_model_name,
        uptime_seconds=time.time() - app_start_time
    )

@app.post("/api/ta-da/find.gcode")
async def ta_da_find_gcode(req: TadaFindRequest, http_client: httpx.AsyncClient = Depends(get_http_client)):
    """Server-side proxy to TA-DA API: hides token and unifies headers."""
    headers = {
        "Authorization": f"Bearer {settings.ta_da_api_token}" if settings.ta_da_api_token else "",
        "User-Language": (req.user_language or settings.ta_da_default_language),
        "Content-Type": "application/json"
    }
    payload = {"shop_id": req.shop_id or settings.ta_da_default_shop_id, "good_code": req.good_code}
    base = settings.ta_da_api_base_url.rstrip("/")
    url = f"{base}/find.gcode"
    try:
        r = await http_client.post(url, headers=headers, json=payload, timeout=20.0)
        if r.status_code != 200:
            return {"error": "API unavailable", "price": 0, "rating": 0}
        data = r.json()
        if not isinstance(data, dict):
            return {"error": "bad_response", "price": 0, "rating": 0}
        return data
    except Exception as e:
        logger.warning(f"TA-DA proxy error: {e}")
        return {"error": "API unavailable", "price": 0, "rating": 0}

@app.post("/cache/clear")
async def clear_cache(cache: TTLCache = Depends(get_embedding_cache)):
    try:
        await cache.clear()
        return {"message": "Cache cleared"}
    except Exception as e:
        logger.exception(f"/cache/clear failed: {e}")
        return JSONResponse(status_code=200, content={"message": "error suppressed", "error": str(e)})

@app.get("/cache/stats")
async def get_cache_stats(cache: TTLCache = Depends(get_embedding_cache)):
    try:
        expired = await cache.cleanup_expired()
        return {
            "size": len(cache),
            "capacity": cache.capacity,
            "ttl_seconds": cache.ttl_seconds,
            "expired_cleaned_now": expired
        }
    except Exception as e:
        logger.exception(f"/cache/stats failed: {e}")
        return {"size": len(cache), "capacity": cache.capacity, "ttl_seconds": cache.ttl_seconds, "error": str(e)}

@app.post("/chat/search", response_model=ChatSearchResponse)
async def chat_search(
    request: ChatSearchRequest,
    gpt_service: GPTService = Depends(get_gpt_service),
    embedding_service: EmbeddingService = Depends(get_embedding_service),
    es_service: ElasticsearchService = Depends(get_elasticsearch_service),
    context_manager: SearchContextManager = Depends(get_context_manager)
):
    """üéØ –ù–û–í–ò–ô –°–ü–†–û–©–ï–ù–ò–ô chat search –∑ —É–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–º GPT –∞—Å–∏—Å—Ç–µ–Ω—Ç–æ–º."""
    t0 = time.time()
    try:
        query = request.query.strip()
        if not query:
            raise HTTPException(400, "Query cannot be empty")
        
        logger.info(f"üí¨ Chat search: '{query}'")
        
        # –ë–∞–∑–æ–≤–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è –∑–∞–ø–∏—Ç—É (–∑–∞–ª–∏—à–∞—î–º–æ –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ)
        is_valid, validation_error = _validate_query_basic(query)
        if not is_valid:
            logger.warning(f"‚ö†Ô∏è –ë–∞–∑–æ–≤–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞: {validation_error}")
            ms = (time.time() - t0) * 1000.0
            return ChatSearchResponse(
                query_analysis=QueryAnalysis(
                    original_query=query,
                    expanded_query=query,
                    keywords=[],
                    context_used=False,
                    intent="invalid"
                ),
                results=[],
                recommendations=[],
                search_time_ms=ms,
                context_used=False,
                assistant_message=validation_error,
                dialog_state="validation_error",
                dialog_context=None,
                needs_user_input=True
            )
        
        # üéØ –ù–û–í–ò–ô –ü–Ü–î–•–Ü–î: –û–¥–∏–Ω –≤–∏–∫–ª–∏–∫ unified_chat_assistant –∑–∞–º—ñ—Å—Ç—å 3-4 —Ä—ñ–∑–Ω–∏—Ö —Ñ—É–Ω–∫—Ü—ñ–π!
        assistant_response = await gpt_service.unified_chat_assistant(
            query=query,
            search_history=request.search_history or [],
            dialog_context=request.dialog_context
        )
        
        action = assistant_response["action"]
        logger.info(f"ü§ñ Assistant action: {action} (confidence: {assistant_response['confidence']:.2f})")
        
        # –û–±—Ä–æ–±–∫–∞ —Ä—ñ–∑–Ω–∏—Ö —Ç–∏–ø—ñ–≤ –¥—ñ–π
        if action == "greeting":
            # –ü—Ä–∏–≤—ñ—Ç–∞–Ω–Ω—è/–ø—Ä–æ—â–∞–Ω–Ω—è/–ø–æ–¥—è–∫–∞
            ms = (time.time() - t0) * 1000.0
            return ChatSearchResponse(
                query_analysis=QueryAnalysis(
                    original_query=query,
                    expanded_query=query,
                    keywords=[],
                    context_used=False,
                    intent="greeting"
                ),
                results=[],
                recommendations=[],
                search_time_ms=ms,
                context_used=False,
                assistant_message=assistant_response["assistant_message"],
                dialog_state="greeting",
                dialog_context=None,
                needs_user_input=assistant_response["needs_user_input"]
            )
        
        elif action == "invalid":
            # –ù–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏–π –∑–∞–ø–∏—Ç
            ms = (time.time() - t0) * 1000.0
            return ChatSearchResponse(
                query_analysis=QueryAnalysis(
                    original_query=query,
                    expanded_query=query,
                    keywords=[],
                    context_used=False,
                    intent="invalid"
                ),
                results=[],
                recommendations=[],
                search_time_ms=ms,
                context_used=False,
                assistant_message=assistant_response["assistant_message"],
                dialog_state="invalid_query",
                dialog_context=None,
                needs_user_input=assistant_response["needs_user_input"]
            )
        
        elif action == "clarification":
            # –ü–æ—Ç—Ä–µ–±—É—î —É—Ç–æ—á–Ω–µ–Ω–Ω—è - –ø–æ–∫–∞–∑—É—î–º–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó –ë–ï–ó —Ç–æ–≤–∞—Ä—ñ–≤
            ms = (time.time() - t0) * 1000.0
            categories = assistant_response.get("categories", [])
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ action buttons –∑ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è–º–∏
            actions = None
            if categories:
                actions = [
                    {
                        "type": "button",
                        "action": "search_category",
                        "value": cat,
                        "label": cat
                    }
                    for cat in categories[:8]
                ]
            
            return ChatSearchResponse(
                query_analysis=QueryAnalysis(
                    original_query=query,
                    expanded_query=query,
                    keywords=[],
                    context_used=False,
                    intent="clarification"
                ),
                results=[],
                recommendations=[],
                search_time_ms=ms,
                context_used=False,
                assistant_message=assistant_response["assistant_message"],
                dialog_state="clarification",
                dialog_context={
                    "clarification_asked": True,
                    "categories_suggested": categories
                },
                needs_user_input=assistant_response["needs_user_input"],
                actions=actions
            )
        
        # action == "product_search" - —à—É–∫–∞—î–º–æ —Ç–æ–≤–∞—Ä–∏
        # üöÄ –û–ü–¢–ò–ú–Ü–ó–ê–¶–Ü–Ø: –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ semantic_subqueries –∑ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ GPT (–Ω–µ —Ç—Ä–µ–±–∞ —Ä–æ–±–∏—Ç–∏ —â–µ –æ–¥–∏–Ω –≤–∏–∫–ª–∏–∫!)
        semantic_subqueries = assistant_response.get("semantic_subqueries", [query])
        if not semantic_subqueries:
            semantic_subqueries = [query]
        
        logger.info(f"üîç Semantic subqueries ({len(semantic_subqueries)}): {semantic_subqueries}")
        
        # Generate embeddings for subqueries (with concurrency limit)
        embeddings = await embedding_service.generate_embeddings_parallel(
            semantic_subqueries,
            max_concurrent=settings.embedding_max_concurrent
        )
        valid_queries = [(sq, emb) for sq, emb in zip(semantic_subqueries, embeddings) if emb is not None]
        
        if not valid_queries:
            # –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –∂–æ–¥–Ω–æ–≥–æ –µ–º–±–µ–¥—ñ–Ω–≥—É - –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ –ø–æ–º–∏–ª–∫—É
            logger.error(f"‚ùå Failed to generate embeddings for query: '{query}'")
            raise HTTPException(
                status_code=503,
                detail="Embedding service unavailable. Please try again later."
            )
        
        # Parallel semantic search - –∑–º–µ–Ω—à—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –Ω–∞ –ø—ñ–¥–∑–∞–ø–∏—Ç –¥–ª—è –∫—Ä–∞—â–æ—ó —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—ñ
        k_per_subquery = min(settings.chat_search_max_k_per_subquery, max(10, 50 // len(valid_queries)))
        logger.info(f"üîç –ü–æ—à—É–∫: {len(valid_queries)} –ø—ñ–¥–∑–∞–ø–∏—Ç—ñ–≤ √ó {k_per_subquery} —Ç–æ–≤–∞—Ä—ñ–≤")
        
        search_results = await es_service.multi_semantic_search(valid_queries, k_per_subquery)
        
        # –û–±'—î–¥–Ω—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑—ñ –∑–≤–∞–∂–µ–Ω–∏–º–∏ —Å–∫–æ—Ä–∞–º–∏
        all_hits_dict = {}
        for idx, (subquery, hits) in enumerate(search_results.items()):
            # –ü–µ—Ä—à–∏–π –ø—ñ–¥–∑–∞–ø–∏—Ç –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∏–π - –¥–∞—î–º–æ –π–æ–º—É –±—ñ–ª—å—à—É –≤–∞–≥—É
            weight = 1.0 if idx == 0 else settings.chat_search_subquery_weight_decay ** idx
            
            for hit in hits:
                product_id = hit["_id"]
                base_score = float(hit.get("_score", 0.0))
                weighted_score = base_score * weight
                
                if product_id not in all_hits_dict:
                    all_hits_dict[product_id] = hit.copy()
                    all_hits_dict[product_id]["_score"] = weighted_score
                    all_hits_dict[product_id]["_subquery_match"] = subquery
                else:
                    # –Ø–∫—â–æ —Ç–æ–≤–∞—Ä –∑–Ω–∞–π–¥–µ–Ω–∏–π –≤ –∫—ñ–ª—å–∫–æ—Ö –ø—ñ–¥–∑–∞–ø–∏—Ç–∞—Ö - –ø—ñ–¥—Å–∏–ª—é—î–º–æ –π–æ–≥–æ —Å–∫–æ—Ä
                    current_score = float(all_hits_dict[product_id].get("_score", 0.0))
                    all_hits_dict[product_id]["_score"] = max(current_score, weighted_score) + 0.1
        
        all_hits = sorted(all_hits_dict.values(), key=lambda x: float(x.get("_score", 0.0)), reverse=True)
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω–∏–π threshold –¥–ª—è –∫—Ä–∞—â–æ—ó —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—ñ
        max_score = max([float(h.get("_score", 0.0)) for h in all_hits], default=0.0)
        dynamic_threshold = settings.chat_search_score_threshold_ratio * max_score if max_score > 0 else 0.0
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω–∏–π –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π –ø–æ—Ä—ñ–≥ –∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        if len(all_hits) < 10:
            # –Ø–∫—â–æ –º–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ - –∑–Ω–∏–∂—É—î–º–æ –ø–æ—Ä—ñ–≥ –Ω–∞ 30%
            adaptive_min = settings.chat_search_min_score_absolute * 0.7
            logger.info(f"üîΩ –ú–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ ({len(all_hits)}), –∑–Ω–∏–∂—É—é –ø–æ—Ä—ñ–≥ –¥–æ {adaptive_min:.2f}")
        elif len(all_hits) < 30:
            # –°–µ—Ä–µ–¥–Ω—è –∫—ñ–ª—å–∫—ñ—Å—Ç—å - –∑–Ω–∏–∂—É—î–º–æ –Ω–∞ 15%
            adaptive_min = settings.chat_search_min_score_absolute * 0.85
            logger.info(f"üìâ –°–µ—Ä–µ–¥–Ω—è –∫—ñ–ª—å–∫—ñ—Å—Ç—å ({len(all_hits)}), –∞–¥–∞–ø—Ç—É—é –ø–æ—Ä—ñ–≥ –¥–æ {adaptive_min:.2f}")
        else:
            # –ë–∞–≥–∞—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π –ø–æ—Ä—ñ–≥
            adaptive_min = settings.chat_search_min_score_absolute
        
        min_score_threshold = max(adaptive_min, dynamic_threshold) if max_score > 0 else 0.0
        
        logger.info(f"üìä –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è: max_score={max_score:.2f}, dynamic={dynamic_threshold:.2f}, adaptive_min={adaptive_min:.2f}, final_threshold={min_score_threshold:.2f}")
        candidate_results = [SearchResult.from_hit(h) for h in all_hits if float(h.get("_score", 0.0)) >= min_score_threshold]
        logger.info(f"‚úÖ –ü—ñ—Å–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ—ó —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó: {len(candidate_results)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ –∑ {len(all_hits)} –∑–Ω–∞–π–¥–µ–Ω–∏—Ö")
        
        # Categorize products
        labels: List[str] = []
        id_buckets: Dict[str, List[str]] = {}
        try:
            labels, id_buckets = await gpt_service.categorize_products(candidate_results[:30], query, timeout_seconds=15.0)
            logger.info(f"üè∑Ô∏è POST /chat/search: Categorization succeeded: {len(labels)} categories")
            logger.info(f"üè∑Ô∏è POST /chat/search: Category labels: {labels}")
        except Exception as e:
            logger.error(f"‚ùå POST /chat/search: Categorization failed: {e}", exc_info=True)
            labels, id_buckets = [], {}
        
        # Get recommendations (–∑–º–µ–Ω—à–µ–Ω–æ –¥–æ 20 –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ GPT)
        recommendations, assistant_message = await gpt_service.analyze_products(candidate_results[:20], query)
        
        # Final results
        sorted_candidates = sorted(candidate_results, key=lambda r: r.score, reverse=True)
        candidate_map = {r.id: r for r in sorted_candidates}
        reco_ids = [rec.product_id for rec in recommendations if rec.product_id in candidate_map]
        ordered_from_reco = [candidate_map[rid] for rid in reco_ids]
        remaining = [r for r in sorted_candidates if r.id not in set(reco_ids)]
        
        max_display = min(request.k, settings.max_chat_display_items)
        final_results = (ordered_from_reco + remaining)[:max_display]
        
        # –î–æ–¥–∞—î–º–æ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω—É –∫–∞—Ç–µ–≥–æ—Ä—ñ—é "–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ" –∑ GPT —Ç–æ–≤–∞—Ä–∞–º–∏
        if reco_ids:
            id_buckets["‚≠ê –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ –¥–ª—è –≤–∞—Å"] = reco_ids  # –í–°–Ü —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó GPT
            if "‚≠ê –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ –¥–ª—è –≤–∞—Å" not in labels:
                labels.insert(0, "‚≠ê –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ –¥–ª—è –≤–∞—Å")  # –î–æ–¥–∞—î–º–æ –Ω–∞ –ø–µ—Ä—à–µ –º—ñ—Å—Ü–µ
        
        # Create category action buttons
        actions = None
        if labels:
            actions = []
            for label in labels[:10]:
                button = {
                    "type": "button",
                    "action": "select_category",
                    "value": label,
                    "label": label
                }
                # –ü–æ–∑–Ω–∞—á–∞—î–º–æ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω—É –∫–∞—Ç–µ–≥–æ—Ä—ñ—é —è–∫ –æ—Å–æ–±–ª–∏–≤—É
                if label == "‚≠ê –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ –¥–ª—è –≤–∞—Å":
                    button["special"] = "recommended"
                actions.append(button)
            logger.info(f"‚úÖ POST /chat/search: Created {len(actions)} category action buttons")
        else:
            logger.warning(f"‚ö†Ô∏è POST /chat/search: No category labels found, actions will be None")
        
        # Store for lazy loading
        context_manager.store_search_results(
            session_id=request.session_id,
            all_results=ordered_from_reco + remaining,
            total_found=len(candidate_results),
            dialog_context=None
        )
        
        ms = (time.time() - t0) * 1000.0
        logger.info(f"Chat search completed: {len(final_results)} products, {ms:.1f}ms")
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ QueryAnalysis –∑ –¥–∞–Ω–∏—Ö assistant_response
        keywords = [w for w in query.split() if len(w) > 2][:5]
        query_analysis = QueryAnalysis(
            original_query=query,
            expanded_query=query,
            keywords=keywords,
            context_used=bool(request.search_history),
            intent="product_search",
            semantic_subqueries=semantic_subqueries
        )
        
        context_manager.add_search(query=query, keywords=keywords, results_count=len(final_results))
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ dialog_context –∑ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è–º–∏ –¥–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó –Ω–∞ –∫–ª—ñ—î–Ω—Ç—ñ
        dialog_ctx = {
            "original_query": query,
            "available_categories": labels,
            "category_buckets": id_buckets,
            "current_filter": None
        }
        
        # –õ–æ–≥—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –ø–æ—à—É–∫—É –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
        try:
            top_products_for_log = [
                {
                    "id": p.id,
                    "name": p.title_ua or p.title_ru or p.id,
                    "score": round(float(p.score), 4),
                    "recommended": p.id in set(reco_ids)
                }
                for p in final_results[:20]  # –¢–æ–ø-20 —Ç–æ–≤–∞—Ä—ñ–≤
            ]
            
            search_logger.log_search_query(
                session_id=request.session_id,
                query=query,
                subqueries=semantic_subqueries,
                total_products_found=len(all_hits),
                products_after_filtering=len(candidate_results),
                max_score=max_score,
                threshold=min_score_threshold,
                adaptive_min=adaptive_min,
                dynamic_threshold=dynamic_threshold,
                top_products=top_products_for_log,
                search_time_ms=ms,
                intent="product_search",
                additional_info={
                    "categories": labels,
                    "recommendations_count": len(recommendations),
                    "total_display": len(final_results),
                    "k_per_subquery": k_per_subquery,
                    "assistant_confidence": assistant_response['confidence']
                }
            )
        except Exception as log_error:
            logger.warning(f"‚ö†Ô∏è Failed to log search query: {log_error}")
        
        return ChatSearchResponse(
            query_analysis=query_analysis,
            results=final_results,
            recommendations=recommendations,
            search_time_ms=ms,
            context_used=query_analysis.context_used,
            assistant_message=assistant_message or assistant_response["assistant_message"] or "–û—Å—å –ø—ñ–¥—ñ–±—Ä–∞–Ω—ñ —Ç–æ–≤–∞—Ä–∏ –∑–∞ –≤–∞—à–∏–º –∑–∞–ø–∏—Ç–æ–º.",
            dialog_state="final_results",
            dialog_context=dialog_ctx,
            needs_user_input=False,
            actions=actions,
            stage_timings_ms={"total": ms}
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.exception(f"Chat search failed: {e}")
        ms = (time.time() - t0) * 1000.0
        return ChatSearchResponse(
            query_analysis=QueryAnalysis(
                original_query=request.query,
                expanded_query=request.query,
                keywords=[],
                context_used=False,
                intent="search"
            ),
            results=[],
            recommendations=[],
            search_time_ms=ms,
            context_used=False,
            assistant_message="–í–∏–±–∞—á—Ç–µ, –≤–∏–Ω–∏–∫–ª–∞ –ø–æ–º–∏–ª–∫–∞. –ë—É–¥—å –ª–∞—Å–∫–∞, —Å–ø—Ä–æ–±—É–π—Ç–µ —â–µ —Ä–∞–∑.",
            dialog_state="error",
            dialog_context=None,
            needs_user_input=False
        )

@app.post("/chat/search/load-more")
async def load_more_products(
    request: LoadMoreRequest,
    context_manager: SearchContextManager = Depends(get_context_manager)
):
    """Load next batch of products without repeating search."""
    try:
        logger.info(f"Load more: session={request.session_id}, offset={request.offset}, limit={request.limit}")
        
        result = context_manager.get_search_results(
            session_id=request.session_id,
            offset=request.offset,
            limit=request.limit
        )
        
        if not result["products"]:
            logger.warning(f"No saved results for session {request.session_id}")
            return {
                "products": [],
                "offset": request.offset,
                "has_more": False,
                "total_found": 0,
                "error": "No saved search results"
            }
        
        logger.info(f"Load more: returned {len(result['products'])} products, has_more={result['has_more']}")
        
        return {
            "products": result["products"],
            "offset": result["offset"],
            "has_more": result["has_more"],
            "total_found": result["total_found"]
        }
        
    except Exception as e:
        logger.exception(f"/chat/search/load-more failed: {e}")
        return {
            "products": [],
            "offset": request.offset,
            "has_more": False,
            "total_found": 0,
            "error": str(e)
        }

@app.get("/api/image-proxy")
async def image_proxy(
    url: str,
    http_client: httpx.AsyncClient = Depends(get_http_client)
):
    """Proxy for product images with caching."""
    try:
        # Check that URL is actually from ta-da.net.ua
        if "ta-da.net.ua" not in url:
            raise HTTPException(400, "Invalid image URL")
        
        logger.info(f"Image proxy: {url}")
        
        response = await http_client.get(url, timeout=10.0)
        response.raise_for_status()
        
        return Response(
            content=response.content,
            media_type=response.headers.get("content-type", "image/png"),
            headers={
                "Cache-Control": "public, max-age=86400",  # Cache for 24 hours
                "Access-Control-Allow-Origin": "*"
            }
        )
    except Exception as e:
        logger.warning(f"Image proxy error for {url}: {e}")
        raise HTTPException(404, "Image not found")

@app.get("/chat/search/sse")
async def chat_search_sse(
    request: Request,
    query: str,
    session_id: str,
    k: int = 50,
    selected_category: Optional[str] = None,
    dialog_context_b64: Optional[str] = None,
    search_history_b64: Optional[str] = None,
    gpt_service: GPTService = Depends(get_gpt_service),
    embedding_service: EmbeddingService = Depends(get_embedding_service),
    es_service: ElasticsearchService = Depends(get_elasticsearch_service)
):
    """SSE stream for chat search."""
    async def event_generator():
        def sse_event(event: str, data: Dict[str, Any]) -> str:
            return f"event: {event}\n" + "data: " + json.dumps(data, ensure_ascii=False) + "\n\n"

        t0 = time.time()
        try:
            query_stripped = query.strip()
            
            dialog_context: Optional[Dict[str, Any]] = None
            if dialog_context_b64:
                try:
                    import base64
                    decoded = base64.urlsafe_b64decode(dialog_context_b64.encode("utf-8")).decode("utf-8")
                    dialog_context = json.loads(decoded)
                except Exception:
                    dialog_context = None
            
            # –ë–∞–∑–æ–≤–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è –∑–∞–ø–∏—Ç—É
            is_valid, validation_error = _validate_query_basic(query_stripped)
            if not is_valid:
                logger.warning(f"‚ö†Ô∏è SSE: –ë–∞–∑–æ–≤–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞: {validation_error}")
                yield sse_event("assistant_start", {"length": len(validation_error)})
                for char in validation_error:
                    yield sse_event("assistant_delta", {"text": char})
                    await asyncio.sleep(0.02)
                yield sse_event("assistant_end", {})
                
                # –í—ñ–¥–ø—Ä–∞–≤–ª—è—î–º–æ –ø–æ—Ä–æ–∂–Ω—ñ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                ms = (time.time() - t0) * 1000.0
                empty_qa = QueryAnalysis(
                    original_query=query_stripped,
                    expanded_query=query_stripped,
                    keywords=[],
                    context_used=False,
                    intent="invalid"
                )
                payload = ChatSearchResponse(
                    query_analysis=empty_qa,
                    results=[],
                    recommendations=[],
                    search_time_ms=ms,
                    context_used=False,
                    assistant_message=validation_error,
                    dialog_state="validation_error",
                    dialog_context=None,
                    needs_user_input=True
                ).model_dump()
                yield sse_event("final", payload)
                return
            
            # üéØ –ù–û–í–ò–ô –ü–Ü–î–•–Ü–î SSE: –û–¥–∏–Ω –≤–∏–∫–ª–∏–∫ unified_chat_assistant
            # –ü–æ–∫–∞–∑—É—î–º–æ —Å—Ç–∞—Ç—É—Å "–î—É–º–∞—é..."
            thinking_message = "–î—É–º–∞—é..."
            yield sse_event("status", {"message": thinking_message, "type": "thinking"})
            
            # –í–∏—Ç—è–≥—É—î–º–æ search_history –∑ –æ–∫—Ä–µ–º–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ search_history_b64
            search_history = []
            if search_history_b64:
                try:
                    import base64
                    history_json = base64.b64decode(search_history_b64).decode('utf-8')
                    history_items = json.loads(history_json)
                    logger.info(f"üìú SSE: –û—Ç—Ä–∏–º–∞–Ω–æ history_items: {history_items}")
                    if isinstance(history_items, list):
                        # –ü–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ –∑ —Ñ–æ—Ä–º–∞—Ç—É {query, timestamp} –≤ SearchHistoryItem
                        for item in history_items:
                            if isinstance(item, dict) and "query" in item:
                                try:
                                    search_history.append(
                                        SearchHistoryItem(
                                            query=item.get("query", ""),
                                            timestamp=item.get("timestamp", "")
                                        )
                                    )
                                except Exception as e:
                                    logger.warning(f"‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—å —Ä–æ–∑–ø–∞—Ä—Å–∏—Ç–∏ history item {item}: {e}")
                        logger.info(f"üìú SSE: –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ —ñ—Å—Ç–æ—Ä—ñ—é: {len(search_history)} –∑–∞–ø–∏—Ç—ñ–≤")
                        if search_history:
                            logger.info(f"üìú SSE: –û—Å—Ç–∞–Ω–Ω—ñ –∑–∞–ø–∏—Ç–∏: {[h.query for h in search_history]}")
                except Exception as e:
                    logger.error(f"‚ùå SSE: –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ search_history_b64: {e}", exc_info=True)
                    search_history = []
            
            # –í–∏–∫–ª–∏–∫–∞—î–º–æ unified_chat_assistant –ë–ï–ó fallback - —Ö–∞–π –ø–∞–¥–∞—î —è–∫—â–æ —î –ø–æ–º–∏–ª–∫–∞!
            logger.info(f"üîç SSE: –í–∏–∫–ª–∏–∫–∞—î–º–æ unified_chat_assistant –∑ —ñ—Å—Ç–æ—Ä—ñ—î—é: {len(search_history)} –∑–∞–ø–∏—Ç—ñ–≤")
            assistant_response = await gpt_service.unified_chat_assistant(
                query=query_stripped,
                search_history=search_history,
                dialog_context=dialog_context
            )
            logger.info(f"‚úÖ SSE: unified_chat_assistant –≤—ñ–¥–ø–æ–≤—ñ–≤ —É—Å–ø—ñ—à–Ω–æ")
            
            action = assistant_response["action"]
            response_text = assistant_response["assistant_message"]
            logger.info(f"ü§ñ SSE Assistant: {action} (confidence: {assistant_response['confidence']:.2f})")
            
            # –û–±—Ä–æ–±–∫–∞ greeting
            if action == "greeting":
                # –í–∏–≤–æ–¥–∏–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å —Å–∏–º–≤–æ–ª –∑–∞ —Å–∏–º–≤–æ–ª–æ–º
                yield sse_event("assistant_start", {"length": len(response_text)})
                for char in response_text:
                    yield sse_event("assistant_delta", {"text": char})
                    await asyncio.sleep(0.015)
                yield sse_event("assistant_end", {})
                
                # –í—ñ–¥–ø—Ä–∞–≤–ª—è—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                ms = (time.time() - t0) * 1000.0
                qa = QueryAnalysis(
                    original_query=query_stripped,
                    expanded_query=query_stripped,
                    keywords=[],
                    context_used=False,
                    intent="greeting"
                )
                payload = ChatSearchResponse(
                    query_analysis=qa,
                    results=[],
                    recommendations=[],
                    search_time_ms=ms,
                    context_used=False,
                    assistant_message=response_text,
                    dialog_state="greeting",
                    dialog_context=None,
                    needs_user_input=assistant_response["needs_user_input"]
                ).model_dump()
                yield sse_event("final", payload)
                return
            
            # –û–±—Ä–æ–±–∫–∞ invalid –∑–∞–ø–∏—Ç—ñ–≤
            if action == "invalid":
                logger.warning(f"‚ö†Ô∏è SSE: Invalid query: {query_stripped}")
                
                yield sse_event("assistant_start", {"length": len(response_text)})
                for char in response_text:
                    yield sse_event("assistant_delta", {"text": char})
                    await asyncio.sleep(0.02)
                yield sse_event("assistant_end", {})
                
                ms = (time.time() - t0) * 1000.0
                empty_qa = QueryAnalysis(
                    original_query=query_stripped,
                    expanded_query=query_stripped,
                    keywords=[],
                    context_used=False,
                    intent="invalid"
                )
                payload = ChatSearchResponse(
                    query_analysis=empty_qa,
                    results=[],
                    recommendations=[],
                    search_time_ms=ms,
                    context_used=False,
                    assistant_message=response_text,
                    dialog_state="invalid_query",
                    dialog_context=None,
                    needs_user_input=True
                ).model_dump()
                yield sse_event("final", payload)
                return
            
            # –û–±—Ä–æ–±–∫–∞ clarification –∑–∞–ø–∏—Ç—ñ–≤ (–ë–ï–ó –ø–æ—à—É–∫—É —Ç–æ–≤–∞—Ä—ñ–≤)
            if action == "clarification":
                logger.info(f"üí¨ SSE: Clarification needed for: {query_stripped}")
                categories = assistant_response.get("categories", [])
                
                yield sse_event("assistant_start", {"length": len(response_text)})
                for char in response_text:
                    yield sse_event("assistant_delta", {"text": char})
                    await asyncio.sleep(0.02)
                yield sse_event("assistant_end", {})
                
                ms = (time.time() - t0) * 1000.0
                empty_qa = QueryAnalysis(
                    original_query=query_stripped,
                    expanded_query=query_stripped,
                    keywords=[],
                    context_used=False,
                    intent="clarification"
                )
                
                # –°—Ç–≤–æ—Ä—é—î–º–æ action buttons
                actions = None
                if categories:
                    actions = [
                        {
                            "type": "button",
                            "action": "search_category",
                            "value": cat,
                            "label": cat
                        }
                        for cat in categories[:8]
                    ]
                
                payload = ChatSearchResponse(
                    query_analysis=empty_qa,
                    results=[],  # –ù–ï –ø–æ–∫–∞–∑—É—î–º–æ —Ç–æ–≤–∞—Ä–∏
                    recommendations=[],
                    search_time_ms=ms,
                    context_used=False,
                    assistant_message=response_text,
                    dialog_state="clarification",
                    dialog_context={
                        "clarification_asked": True,
                        "categories_suggested": categories
                    },
                    needs_user_input=True,
                    actions=actions
                ).model_dump()
                yield sse_event("final", payload)
                return

            # action == "product_search" - –ø—Ä–æ–¥–æ–≤–∂—É—î–º–æ –∑–≤–∏—á–∞–π–Ω–∏–π –ø–æ—à—É–∫
            # üöÄ –û–ü–¢–ò–ú–Ü–ó–ê–¶–Ü–Ø: –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ semantic_subqueries –∑ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ GPT
            semantic_subqueries = assistant_response.get("semantic_subqueries", [query_stripped])
            if not semantic_subqueries:
                semantic_subqueries = [query_stripped]
            
            logger.info(f"üîç SSE: {len(semantic_subqueries)} subqueries: {semantic_subqueries}")
            
            # –ü–æ–∫–∞–∑—É—î–º–æ —Å—Ç–∞—Ç—É—Å "–®—É–∫–∞—é —Ç–æ–≤–∞—Ä–∏..."
            searching_message = "–®—É–∫–∞—é —Ç–æ–≤–∞—Ä–∏..."
            yield sse_event("status", {"message": searching_message, "type": "searching"})
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ QueryAnalysis
            keywords = [w for w in query_stripped.split() if len(w) > 2][:5]
            qa = QueryAnalysis(
                original_query=query_stripped,
                expanded_query=query_stripped,
                keywords=keywords,
                context_used=False,
                intent="product_search",
                semantic_subqueries=semantic_subqueries
            )
            
            yield sse_event("analysis", {"query_analysis": qa.model_dump()})

            # –ì–µ–Ω–µ—Ä—É—î–º–æ embeddings –¥–ª—è –≤—Å—ñ—Ö –ø—ñ–¥–∑–∞–ø–∏—Ç—ñ–≤ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ (–∑ –æ–±–º–µ–∂–µ–Ω–Ω—è–º)
            t_embed_start = time.time()
            embeddings = await embedding_service.generate_embeddings_parallel(
                semantic_subqueries,
                max_concurrent=settings.embedding_max_concurrent
            )
            embedding_time = (time.time() - t_embed_start) * 1000.0
            
            valid_queries = [(sq, emb) for sq, emb in zip(semantic_subqueries, embeddings) if emb is not None]
            
            if not valid_queries:
                # –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –∂–æ–¥–Ω–æ–≥–æ –µ–º–±–µ–¥—ñ–Ω–≥—É - –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ –ø–æ–º–∏–ª–∫—É —á–µ—Ä–µ–∑ SSE
                logger.error(f"‚ùå SSE: Failed to generate embeddings for query: '{query}'")
                yield sse_event("error", {
                    "error": "Embedding service unavailable",
                    "message": "–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –µ–º–±–µ–¥—ñ–Ω–≥–∏. –°–ø—Ä–æ–±—É–π—Ç–µ —â–µ —Ä–∞–∑ –ø—ñ–∑–Ω—ñ—à–µ."
                })
                return
            
            logger.info(f"SSE: Generated {len(valid_queries)}/{len(qa.semantic_subqueries)} embeddings for parallel search")
            
            # –õ–æ–≥—É—î–º–æ –ø—ñ–¥–∑–∞–ø–∏—Ç–∏ –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
            for i, (sq, _) in enumerate(valid_queries):
                logger.info(f"üîç SSE –ø—ñ–¥–∑–∞–ø–∏—Ç #{i+1}: '{sq}'")
            
            # –ü–∞—Ä–∞–ª–µ–ª—å–Ω–∏–π –ø–æ—à—É–∫ –ø–æ –≤—Å—ñ—Ö –ø—ñ–¥–∑–∞–ø–∏—Ç–∞—Ö - –∞–¥–∞–ø—Ç–∏–≤–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å
            k_per_subquery = min(settings.chat_search_max_k_per_subquery, max(10, 50 // len(valid_queries)))
            logger.info(f"üîç SSE –ø–æ—à—É–∫: {len(valid_queries)} –ø—ñ–¥–∑–∞–ø–∏—Ç—ñ–≤ √ó {k_per_subquery} —Ç–æ–≤–∞—Ä—ñ–≤")
            
            search_results = await es_service.multi_semantic_search(valid_queries, k_per_subquery)
            
            # –û–±'—î–¥–Ω—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑—ñ –∑–≤–∞–∂–µ–Ω–∏–º–∏ —Å–∫–æ—Ä–∞–º–∏
            all_hits_dict: Dict[str, Dict] = {}
            subquery_mapping: Dict[str, str] = {}
            
            for idx, (subquery, hits) in enumerate(search_results.items()):
                # –ü–µ—Ä—à–∏–π –ø—ñ–¥–∑–∞–ø–∏—Ç –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∏–π - –¥–∞—î–º–æ –π–æ–º—É –±—ñ–ª—å—à—É –≤–∞–≥—É
                weight = 1.0 if idx == 0 else settings.chat_search_subquery_weight_decay ** idx
                logger.info(f"üì¶ SSE –ø—ñ–¥–∑–∞–ø–∏—Ç '{subquery[:30]}...': –∑–Ω–∞–π–¥–µ–Ω–æ {len(hits)} —Ç–æ–≤–∞—Ä—ñ–≤, –≤–∞–≥–∞={weight:.2f}")
                
                for hit in hits:
                    product_id = hit["_id"]
                    base_score = float(hit.get("_score", 0.0))
                    weighted_score = base_score * weight
                    
                    if product_id not in all_hits_dict:
                        all_hits_dict[product_id] = hit.copy()
                        all_hits_dict[product_id]["_score"] = weighted_score
                        subquery_mapping[product_id] = subquery
                    else:
                        # –Ø–∫—â–æ —Ç–æ–≤–∞—Ä –∑–Ω–∞–π–¥–µ–Ω–∏–π –≤ –∫—ñ–ª—å–∫–æ—Ö –ø—ñ–¥–∑–∞–ø–∏—Ç–∞—Ö - –ø—ñ–¥—Å–∏–ª—é—î–º–æ –π–æ–≥–æ —Å–∫–æ—Ä
                        current_score = float(all_hits_dict[product_id].get("_score", 0.0))
                        all_hits_dict[product_id]["_score"] = max(current_score, weighted_score) + 0.1
            
            all_hits = sorted(all_hits_dict.values(), key=lambda x: float(x.get("_score", 0.0)), reverse=True)
            logger.info(f"SSE: Merged {len(all_hits)} unique products from {len(search_results)} subqueries")
            
            # –ê–¥–∞–ø—Ç–∏–≤–Ω–∏–π threshold –¥–ª—è –∫—Ä–∞—â–æ—ó —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—ñ (SSE)
            max_score = max([float(h.get("_score", 0.0)) for h in all_hits], default=0.0)
            dynamic_threshold = settings.chat_search_score_threshold_ratio * max_score if max_score > 0 else 0.0
            
            # –ê–¥–∞–ø—Ç–∏–≤–Ω–∏–π –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π –ø–æ—Ä—ñ–≥ –∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
            if len(all_hits) < 10:
                # –Ø–∫—â–æ –º–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ - –∑–Ω–∏–∂—É—î–º–æ –ø–æ—Ä—ñ–≥ –Ω–∞ 30%
                adaptive_min = settings.chat_search_min_score_absolute * 0.7
                logger.info(f"üîΩ SSE: –ú–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ ({len(all_hits)}), –∑–Ω–∏–∂—É—é –ø–æ—Ä—ñ–≥ –¥–æ {adaptive_min:.2f}")
            elif len(all_hits) < 30:
                # –°–µ—Ä–µ–¥–Ω—è –∫—ñ–ª—å–∫—ñ—Å—Ç—å - –∑–Ω–∏–∂—É—î–º–æ –Ω–∞ 15%
                adaptive_min = settings.chat_search_min_score_absolute * 0.85
                logger.info(f"üìâ SSE: –°–µ—Ä–µ–¥–Ω—è –∫—ñ–ª—å–∫—ñ—Å—Ç—å ({len(all_hits)}), –∞–¥–∞–ø—Ç—É—é –ø–æ—Ä—ñ–≥ –¥–æ {adaptive_min:.2f}")
            else:
                # –ë–∞–≥–∞—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π –ø–æ—Ä—ñ–≥
                adaptive_min = settings.chat_search_min_score_absolute
            
            min_score_threshold = max(adaptive_min, dynamic_threshold) if max_score > 0 else 0.0
            
            logger.info(f"üìä SSE —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è: max_score={max_score:.2f}, dynamic={dynamic_threshold:.2f}, adaptive_min={adaptive_min:.2f}, final={min_score_threshold:.2f}")
            candidate_results = [SearchResult.from_hit(h) for h in all_hits if float(h.get("_score", 0.0)) >= min_score_threshold]
            
            logger.info(f"‚úÖ SSE: –ü—ñ—Å–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ—ó —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó {len(candidate_results)} —Ç–æ–≤–∞—Ä—ñ–≤ –∑ {len(all_hits)} –∑–Ω–∞–π–¥–µ–Ω–∏—Ö")
            yield sse_event("candidates", {"count": len(candidate_results)})

            labels: List[str] = []
            id_buckets: Dict[str, List[str]] = {}
            try:
                labels, id_buckets = await gpt_service.categorize_products(candidate_results[:30], query, timeout_seconds=15.0)
                logger.info(f"üè∑Ô∏è Categorization succeeded: {len(labels)} categories, {len(id_buckets)} buckets")
                logger.info(f"üè∑Ô∏è Category labels: {labels}")
            except Exception as e:
                logger.error(f"‚ùå Categorization failed: {e}", exc_info=True)
                labels, id_buckets = [], {}

            recommendations: List[ProductRecommendation] = []
            assistant_message: str = ""
            try:
                # –ó–º–µ–Ω—à–µ–Ω–æ –¥–æ 20 –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ GPT
                recommendations, assistant_message = await gpt_service.analyze_products(candidate_results[:20], query)
            except Exception:
                recommendations, assistant_message = [], "–û—Å—å –ø—ñ–¥—ñ–±—Ä–∞–Ω—ñ —Ç–æ–≤–∞—Ä–∏ –∑–∞ –≤–∞—à–∏–º –∑–∞–ø–∏—Ç–æ–º."
            
            # –î–æ–¥–∞—î–º–æ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω—É –∫–∞—Ç–µ–≥–æ—Ä—ñ—é "–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ" –∑ GPT —Ç–æ–≤–∞—Ä–∞–º–∏
            reco_ids_list = [rec.product_id for rec in recommendations]
            if reco_ids_list:
                id_buckets["‚≠ê –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ –¥–ª—è –≤–∞—Å"] = reco_ids_list  # –í–°–Ü —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó GPT
                if "‚≠ê –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ –¥–ª—è –≤–∞—Å" not in labels:
                    labels.insert(0, "‚≠ê –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ –¥–ª—è –≤–∞—Å")
            
            yield sse_event("categories", {"labels": labels})
            yield sse_event("recommendations", {
                "count": len(recommendations),
                "assistant_message": assistant_message
            })

            # –í—ñ–¥–ø—Ä–∞–≤–ª—è—î–º–æ —Ç–µ–∫—Å—Ç –∞—Å–∏—Å—Ç–µ–Ω—Ç–∞ –ø–æ—Å–∏–º–≤–æ–ª—å–Ω–æ –¥–ª—è –µ—Ñ–µ–∫—Ç—É –¥—Ä—É–∫—É
            try:
                typed = assistant_message or "–Ø –ø—ñ–¥—ñ–±—Ä–∞–≤ –¥–ª—è –≤–∞—Å –¥–æ–±—ñ—Ä–∫—É —Ç–æ–≤–∞—Ä—ñ–≤."
                yield sse_event("assistant_start", {"length": len(typed)})
                
                # –í—ñ–¥–ø—Ä–∞–≤–ª—è—î–º–æ –ø–æ 1 —Å–∏–º–≤–æ–ª—É –¥–ª—è –ø–ª–∞–≤–Ω–æ—ó –∞–Ω—ñ–º–∞—Ü—ñ—ó
                for char in typed:
                    yield sse_event("assistant_delta", {"text": char})
                    await asyncio.sleep(0.02)  # 20–º—Å –º—ñ–∂ —Å–∏–º–≤–æ–ª–∞–º–∏
                
                yield sse_event("assistant_end", {})
            except Exception:
                pass

            sorted_candidates = sorted(candidate_results, key=lambda r: float(getattr(r, 'score', 0.0)), reverse=True)
            candidate_map = {r.id: r for r in sorted_candidates}
            reco_ids_in_order: List[str] = [rec.product_id for rec in recommendations if rec.product_id in candidate_map] if recommendations else []
            ordered_from_reco: List[SearchResult] = [candidate_map[rid] for rid in reco_ids_in_order]
            remaining: List[SearchResult] = [r for r in sorted_candidates if r.id not in set(reco_ids_in_order)]
            max_display_items = min(k, settings.max_chat_display_items)
            final_results = (ordered_from_reco + remaining)[:max_display_items]

            actions = None
            if labels:
                actions = []
                for label in labels[:10]:
                    button = {
                        "type": "button",
                        "action": "select_category",
                        "value": label,
                        "label": label
                    }
                    # –ü–æ–∑–Ω–∞—á–∞—î–º–æ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω—É –∫–∞—Ç–µ–≥–æ—Ä—ñ—é —è–∫ –æ—Å–æ–±–ª–∏–≤—É
                    if label == "‚≠ê –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ –¥–ª—è –≤–∞—Å":
                        button["special"] = "recommended"
                    actions.append(button)
                logger.info(f"‚úÖ Created {len(actions)} category action buttons")
            else:
                logger.warning(f"‚ö†Ô∏è No category labels found, actions will be None")

            ms = (time.time() - t0) * 1000.0
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ dialog_context –∑ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è–º–∏ –¥–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó –Ω–∞ –∫–ª—ñ—î–Ω—Ç—ñ
            dialog_ctx = {
                "original_query": query,
                "available_categories": labels,
                "category_buckets": id_buckets,
                "current_filter": None
            }
            
            # –õ–æ–≥—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ SSE –ø–æ—à—É–∫—É –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
            try:
                top_products_for_log = [
                    {
                        "id": p.id,
                        "name": p.title_ua or p.title_ru or p.id,
                        "score": round(float(p.score), 4),
                        "recommended": p.id in set(reco_ids_in_order)
                    }
                    for p in final_results[:20]  # –¢–æ–ø-20 —Ç–æ–≤–∞—Ä—ñ–≤
                ]
                
                search_logger.log_search_query(
                    session_id=session_id,
                    query=query_stripped,
                    subqueries=qa.semantic_subqueries,
                    total_products_found=len(all_hits),
                    products_after_filtering=len(candidate_results),
                    max_score=max_score,
                    threshold=min_score_threshold,
                    adaptive_min=adaptive_min,
                    dynamic_threshold=dynamic_threshold,
                    top_products=top_products_for_log,
                    search_time_ms=ms,
                    intent="product_search",
                    additional_info={
                        "assistant_confidence": assistant_response['confidence'],
                        "categories": labels,
                        "recommendations_count": len(recommendations),
                        "total_display": len(final_results),
                        "k_per_subquery": k_per_subquery,
                        "method": "SSE"
                    }
                )
            except Exception as log_error:
                logger.warning(f"‚ö†Ô∏è SSE: Failed to log search query: {log_error}")
            
            payload = ChatSearchResponse(
                query_analysis=qa,
                results=final_results,
                recommendations=recommendations,
                search_time_ms=ms,
                context_used=qa.context_used,
                assistant_message=assistant_message,
                dialog_state="final_results",
                dialog_context=dialog_ctx,
                needs_user_input=False,
                actions=actions,
                stage_timings_ms={"total": ms, "embedding_generation": embedding_time}
            ).model_dump()
            yield sse_event("final", payload)
        except Exception as e:
            logger.error(f"SSE chat search error: {e}", exc_info=True)
            yield sse_event("error", {"message": f"–í–∏–±–∞—á—Ç–µ, –≤–∏–Ω–∏–∫–ª–∞ –ø–æ–º–∏–ª–∫–∞: {str(e)}"})

    return StreamingResponse(event_generator(), media_type="text/event-stream")


# ========================================
# API –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –ª–æ–≥–∞–º–∏ –ø–æ—à—É–∫—É
# ========================================

@app.get("/search-logs/sessions")
async def get_search_log_sessions():
    """
    –ü–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ –≤—Å—ñ—Ö —Å–µ—Å—ñ–π –∑ –ª–æ–≥–∞–º–∏.
    """
    try:
        sessions = search_logger.get_all_sessions()
        return {
            "total": len(sessions),
            "sessions": sessions
        }
    except Exception as e:
        logger.error(f"Failed to get search log sessions: {e}")
        raise HTTPException(500, f"Failed to get sessions: {str(e)}")


@app.get("/search-logs/session/{session_id}")
async def get_session_logs(session_id: str):
    """
    –ü–æ–≤–µ—Ä—Ç–∞—î –≤—Å—ñ –ª–æ–≥–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—ó —Å–µ—Å—ñ—ó.
    """
    try:
        logs = search_logger.get_session_logs(session_id)
        if not logs:
            raise HTTPException(404, f"Session '{session_id}' not found")
        
        return {
            "session_id": session_id,
            "total_queries": len(logs),
            "queries": logs
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get session logs: {e}")
        raise HTTPException(500, f"Failed to get session logs: {str(e)}")


@app.get("/search-logs/report/{session_id}")
async def get_session_report(session_id: str):
    """
    –ì–µ–Ω–µ—Ä—É—î –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω–∏–π –∑–≤—ñ—Ç –ø–æ —Å–µ—Å—ñ—ó.
    """
    try:
        report = search_logger.generate_session_report(session_id)
        if "error" in report:
            raise HTTPException(404, report["error"])
        
        return report
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to generate session report: {e}")
        raise HTTPException(500, f"Failed to generate report: {str(e)}")


@app.get("/search-logs/export")
async def export_all_sessions():
    """
    –ï–∫—Å–ø–æ—Ä—Ç—É—î –∑–≤—ñ—Ç–∏ –ø–æ –≤—Å—ñ—Ö —Å–µ—Å—ñ—è—Ö –≤ JSON —Ñ–∞–π–ª.
    –ü–æ–≤–µ—Ä—Ç–∞—î —à–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É.
    """
    try:
        output_path = search_logger.export_all_sessions_report()
        return {
            "success": True,
            "file_path": output_path,
            "message": f"Report exported to {output_path}"
        }
    except Exception as e:
        logger.error(f"Failed to export sessions: {e}")
        raise HTTPException(500, f"Failed to export: {str(e)}")


@app.get("/search-logs/stats")
async def get_search_logs_stats():
    """
    –ü–æ–≤–µ—Ä—Ç–∞—î –∑–∞–≥–∞–ª—å–Ω—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—Å—ñ—Ö –ª–æ–≥–∞—Ö.
    """
    try:
        sessions = search_logger.get_all_sessions()
        
        total_queries = 0
        all_scores = []
        all_search_times = []
        
        for session_id in sessions:
            logs = search_logger.get_session_logs(session_id)
            total_queries += len(logs)
            
            for log in logs:
                all_search_times.append(log["search_stats"]["search_time_ms"])
                all_scores.extend([p["score"] for p in log["top_products"]])
        
        return {
            "total_sessions": len(sessions),
            "total_queries": total_queries,
            "average_queries_per_session": round(total_queries / len(sessions), 2) if sessions else 0,
            "search_time": {
                "min": round(min(all_search_times), 2) if all_search_times else 0,
                "max": round(max(all_search_times), 2) if all_search_times else 0,
                "avg": round(sum(all_search_times) / len(all_search_times), 2) if all_search_times else 0
            },
            "scores": {
                "min": round(min(all_scores), 4) if all_scores else 0,
                "max": round(max(all_scores), 4) if all_scores else 0,
                "avg": round(sum(all_scores) / len(all_scores), 4) if all_scores else 0
            }
        }
    except Exception as e:
        logger.error(f"Failed to get search logs stats: {e}")
        raise HTTPException(500, f"Failed to get stats: {str(e)}")


# RUN
if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)